2023-10-12 02:50:49,613 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmps85b9ni4
2023-10-12 02:50:49,613 [instantiator.py:76 in _write] INFO - Writing /tmp/tmps85b9ni4/_remote_module_non_scriptable.py
2023-10-12 02:50:50,013 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-12 02:50:50,079 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-12 02:50:51,614 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-12 02:50:51,951 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-12 02:50:51,951 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-12 02:50:51,951 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-12 02:50:51,951 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-12 02:50:52,794 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-12 02:50:52,954 [model.py:159 in is_on_disk] INFO - [], []
2023-10-12 02:50:52,999 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-12 02:50:53,148 [model.py:159 in is_on_disk] INFO - [], []
2023-10-12 02:50:53,152 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/Salesforce.codegen-350M-mono'
2023-10-12 02:50:53,158 [model.py:138 in get_policy_weight_map] DEBUG - transformer.wte, [0. 0. 1.], size_todo: 304283648
2023-10-12 02:50:53,158 [model.py:138 in get_policy_weight_map] DEBUG - transformer.drop, [0. 0. 1.], size_todo: 304283648
2023-10-12 02:50:53,159 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.0, [0.         0.06454052 0.93545948], size_todo: 291693568
2023-10-12 02:50:53,160 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.1, [0.         0.10814092 0.89185908], size_todo: 279103488
2023-10-12 02:50:53,161 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.2, [0.         0.13956973 0.86043027], size_todo: 266513408
2023-10-12 02:50:53,161 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.3, [0.         0.16329946 0.83670054], size_todo: 253923328
2023-10-12 02:50:53,162 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.4, [0.         0.18185045 0.81814955], size_todo: 241333248
2023-10-12 02:50:53,163 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.5, [0.         0.19675122 0.80324878], size_todo: 228743168
2023-10-12 02:50:53,164 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.6, [0.         0.20898262 0.79101738], size_todo: 216153088
2023-10-12 02:50:53,164 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.7, [0.       0.219203 0.780797], size_todo: 203563008
2023-10-12 02:50:53,165 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.8, [0.         0.22787062 0.77212938], size_todo: 190972928
2023-10-12 02:50:53,166 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.9, [0.         0.23531438 0.76468562], size_todo: 178382848
2023-10-12 02:50:53,166 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.10, [0.        0.2417764 0.7582236], size_todo: 165792768
2023-10-12 02:50:53,167 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.11, [0.         0.24743886 0.75256114], size_todo: 153202688
2023-10-12 02:50:53,168 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.12, [0.         0.25244154 0.74755846], size_todo: 140612608
2023-10-12 02:50:53,169 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.13, [0.         0.25689339 0.74310661], size_todo: 128022528
2023-10-12 02:50:53,169 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.14, [0.         0.26088064 0.73911936], size_todo: 115432448
2023-10-12 02:50:53,170 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.15, [0.         0.26447241 0.73552759], size_todo: 102842368
2023-10-12 02:50:53,171 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.16, [0.         0.26772477 0.73227523], size_todo: 90252288
2023-10-12 02:50:53,171 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.17, [0.         0.27068364 0.72931636], size_todo: 77662208
2023-10-12 02:50:53,172 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.18, [0.         0.27338705 0.72661295], size_todo: 65072128
2023-10-12 02:50:53,173 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.19, [0.         0.27586671 0.72413329], size_todo: 52482048
2023-10-12 02:50:53,173 [model.py:138 in get_policy_weight_map] DEBUG - transformer.ln_f, [0.         0.27586822 0.72413178], size_todo: 52480000
2023-10-12 02:50:53,174 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.23528213 0.76471787], size_todo: 0
2023-10-12 02:50:53,174 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-12 02:50:53,175 [model.py:148 in get_policy_weight_map] INFO - CausalLM Salesforce/codegen-350M-mono is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.16 GiB (23.53%), Disk Mem 0.51 Gib (76.47%)
2023-10-12 02:50:53,176 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-12 02:50:53,192 [forward.py:48 in to_test_forward] DEBUG - transformer.wte to test forward
2023-10-12 02:50:53,192 [forward.py:48 in to_test_forward] DEBUG - transformer.drop to test forward
2023-10-12 02:50:53,192 [forward.py:48 in to_test_forward] DEBUG - transformer.h.0 to test forward
2023-10-12 02:50:53,192 [forward.py:48 in to_test_forward] DEBUG - transformer.h.1 to test forward
2023-10-12 02:50:53,192 [forward.py:48 in to_test_forward] DEBUG - transformer.h.2 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.3 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.4 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.5 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.6 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.7 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.8 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.9 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.10 to test forward
2023-10-12 02:50:53,193 [forward.py:48 in to_test_forward] DEBUG - transformer.h.11 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.12 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.13 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.14 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.15 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.16 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.17 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.18 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.h.19 to test forward
2023-10-12 02:50:53,194 [forward.py:48 in to_test_forward] DEBUG - transformer.ln_f to test forward
2023-10-12 02:50:53,195 [forward.py:48 in to_test_forward] DEBUG - lm_head to test forward
2023-10-12 02:50:53,234 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-12 02:50:53,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:53,315 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:53,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:53,315 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:53,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:53,326 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:53,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:53,334 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:53,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:53,343 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:53,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:53,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:53,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:53,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:53,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:53,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:53,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:53,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:53,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:53,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:53,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:53,390 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:53,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:53,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:53,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:53,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:53,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:53,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:53,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:53,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:53,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:53,428 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:53,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:53,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:53,437 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:53,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:53,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:53,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:53,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:53,457 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:53,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:53,464 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:53,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:53,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:53,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:53,475 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:53,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:53,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:53,502 [test.py:40 in test_hf_gen] INFO - 0,
2023-10-12 02:50:53,503 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:50:53,513 [forward.py:28 in reset_forward] DEBUG - transformer.wte from test to old.
2023-10-12 02:50:53,513 [forward.py:28 in reset_forward] DEBUG - transformer.drop from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.0 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.1 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.2 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.3 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.4 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.5 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.6 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.7 from test to old.
2023-10-12 02:50:53,514 [forward.py:28 in reset_forward] DEBUG - transformer.h.8 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.9 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.10 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.11 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.12 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.13 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.14 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.15 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.16 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.17 from test to old.
2023-10-12 02:50:53,515 [forward.py:28 in reset_forward] DEBUG - transformer.h.18 from test to old.
2023-10-12 02:50:53,516 [forward.py:28 in reset_forward] DEBUG - transformer.h.19 from test to old.
2023-10-12 02:50:53,516 [forward.py:28 in reset_forward] DEBUG - transformer.ln_f from test to old.
2023-10-12 02:50:53,516 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-12 02:50:53,516 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.wte to flexgen forward
2023-10-12 02:50:53,516 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.drop to flexgen forward
2023-10-12 02:50:53,516 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.0 to flexgen forward
2023-10-12 02:50:53,516 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.1 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.2 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.3 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.4 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.5 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.6 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.7 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.8 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.9 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.10 to flexgen forward
2023-10-12 02:50:53,517 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.11 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.12 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.13 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.14 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.15 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.16 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.17 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.18 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.19 to flexgen forward
2023-10-12 02:50:53,518 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.ln_f to flexgen forward
2023-10-12 02:50:53,519 [forward.py:125 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-12 02:50:53,563 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-12 02:50:53,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:53,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:53,629 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 8])",)
2023-10-12 02:50:53,629 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:53,630 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,631 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,632 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,632 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,633 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])
2023-10-12 02:50:53,633 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:53,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:53,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:53,635 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])",)
2023-10-12 02:50:53,636 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:53,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,638 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,639 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:53,639 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])
2023-10-12 02:50:53,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:53,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:53,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:53,643 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,643 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,652 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,659 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,668 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,676 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,676 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,676 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:53,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:53,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:53,682 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,682 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,690 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,698 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,712 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,713 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,713 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:53,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:53,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:53,718 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,718 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,727 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,734 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,748 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,748 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,748 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:53,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:53,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:53,754 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,755 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,763 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,771 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,785 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,785 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:53,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:53,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:53,791 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,791 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,799 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,806 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,813 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,822 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,822 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,822 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:53,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:53,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:53,828 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,828 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,849 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,873 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,874 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:53,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:53,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:53,879 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,879 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,887 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,895 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,903 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,910 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,911 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,911 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:53,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:53,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:53,916 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,917 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,925 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,932 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,938 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,945 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,946 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:53,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:53,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:53,952 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,952 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,962 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,969 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:53,983 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:53,983 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:53,985 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:53,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:53,989 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:53,989 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:53,997 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,004 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,020 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,020 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,020 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:54,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:54,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:54,026 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,026 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,034 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,041 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,048 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,054 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,055 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:54,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:54,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:54,060 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,061 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,068 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,076 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,082 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,089 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,089 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,089 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:54,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:54,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:54,095 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,095 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,103 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,110 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,118 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,125 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,125 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:54,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:54,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:54,131 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,131 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,147 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,153 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,161 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,161 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,162 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:54,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:54,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:54,167 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,168 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,199 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,199 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:54,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:54,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:54,205 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,205 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,213 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,220 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,226 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,233 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,233 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:54,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:54,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:54,239 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,239 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,255 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,263 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,270 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:54,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:54,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:54,276 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,276 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,283 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,290 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,296 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,304 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,305 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:54,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:54,309 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:54,311 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,312 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,320 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,327 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,341 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,341 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:54,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:54,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:54,346 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,346 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,354 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,361 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,368 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:50:54,374 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:50:54,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:54,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:54,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:54,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])",)
2023-10-12 02:50:54,378 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:54,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:54,381 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:54,382 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:54,383 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:50:54,384 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])
2023-10-12 02:50:54,384 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:54,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:54,386 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:54,387 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 8, 1024])",)
2023-10-12 02:50:54,387 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:54,404 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:50:54,426 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:50:54,447 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:50:54,461 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:50:54,468 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 8, 51200])
2023-10-12 02:50:54,468 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:54,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:54,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:54,475 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:50:54,475 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:54,477 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,477 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,478 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,479 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:54,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:54,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:54,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:54,481 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:54,482 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:54,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,483 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,484 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,484 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:54,484 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:54,484 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:54,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:54,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:54,488 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,512 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,512 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:54,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:54,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:54,518 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,518 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,535 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,540 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,540 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,540 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:54,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:54,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:54,546 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,546 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,558 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,563 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,569 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,569 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:54,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:54,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:54,574 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,574 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,580 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,586 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,591 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,596 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,596 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:54,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:54,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:54,601 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,601 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,607 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,613 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,619 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,624 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,624 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,624 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:54,626 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:54,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:54,629 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,630 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,646 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,652 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:54,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:54,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:54,657 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,657 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,664 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,669 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,679 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,680 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:54,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:54,683 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:54,685 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,685 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,715 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,750 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,758 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,758 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,758 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:54,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:54,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:54,764 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,765 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,795 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,802 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,802 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,802 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:54,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:54,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:54,807 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,807 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,815 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,821 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,828 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,834 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,834 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,835 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:54,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:54,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:54,840 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,840 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,847 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,854 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,866 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,867 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:54,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:54,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:54,872 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,872 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,886 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,892 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,907 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,907 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:54,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:54,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:54,914 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,914 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,930 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,966 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,973 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:54,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:54,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:54,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:54,979 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:54,980 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:54,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:54,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,000 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,006 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,007 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:55,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:55,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:55,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:55,013 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,014 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,047 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,047 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:55,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:55,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:55,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:55,053 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,053 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,069 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,077 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,084 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,084 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:55,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:55,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:55,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:55,092 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,092 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,103 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,108 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,119 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:55,119 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:55,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:55,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:55,124 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,124 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,137 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,143 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,148 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,148 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:55,148 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:55,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:55,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:55,154 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,154 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,161 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,166 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,171 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,177 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:55,177 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:55,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:55,180 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:55,181 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,181 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,196 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,209 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:50:55,210 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:50:55,210 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:55,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:55,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:55,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:55,213 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:55,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,225 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:55,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:55,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:55,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:55,227 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:55,228 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:55,247 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:55,263 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:55,277 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:55,289 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:55,291 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:50:55,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:55,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:55,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:55,299 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:50:55,299 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:55,302 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,305 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,306 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:55,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:55,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:55,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:55,310 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:55,311 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:55,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,313 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,314 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:55,314 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:55,314 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:55,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:55,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:55,318 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,318 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,332 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,337 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,343 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,344 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,344 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:55,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:55,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:55,350 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,350 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,357 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,363 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,368 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,374 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:55,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:55,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:55,380 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,380 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,387 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,397 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,402 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,420 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,420 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,421 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:55,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:55,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:55,426 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,427 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,434 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,445 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,451 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,451 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,451 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:55,453 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:55,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:55,457 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,457 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,464 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,469 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,475 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,481 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,481 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:55,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:55,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:55,487 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,487 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,512 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,523 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,523 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,524 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:55,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:55,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:55,529 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,529 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,536 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,541 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,547 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,553 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:55,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:55,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:55,558 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,559 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,566 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,571 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,576 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,582 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,582 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:55,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:55,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:55,587 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,588 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,594 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,600 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,605 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,611 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,611 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,611 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:55,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:55,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:55,617 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,617 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,624 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,630 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,635 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,640 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,641 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,641 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:55,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:55,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:55,647 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,647 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,653 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,659 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,666 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,671 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,671 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,671 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:55,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:55,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:55,677 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,677 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,700 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,948 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,966 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,966 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:55,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:55,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:55,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:55,975 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:55,976 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:55,984 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:55,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,004 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,005 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,005 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:56,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:56,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:56,012 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,012 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,023 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,029 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,035 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,044 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,045 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:56,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:56,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:56,050 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,050 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,067 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,072 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,073 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:56,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:56,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:56,078 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,078 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,084 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,090 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,106 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:56,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:56,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:56,113 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,113 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,121 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,144 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,150 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,150 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,150 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:56,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:56,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:56,156 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,156 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,163 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,169 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,174 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,180 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,180 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,180 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:56,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:56,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:56,186 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,186 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,192 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,198 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,203 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,208 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,209 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,209 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:56,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:56,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:56,213 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,220 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,225 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:50:56,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:50:56,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:56,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:56,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:56,239 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:56,239 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:56,240 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,241 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,242 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,243 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,243 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:56,243 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:56,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:56,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:56,245 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:56,245 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:56,258 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:56,272 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:56,284 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:56,296 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:56,334 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:50:56,336 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:56,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:56,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:56,357 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:50:56,357 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:56,361 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,365 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,366 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,366 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:56,366 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:56,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:56,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:56,369 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:56,369 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:56,370 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,370 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,372 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:56,372 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:56,372 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:56,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:56,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:56,377 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,377 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,385 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,392 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,406 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,406 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:56,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:56,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:56,411 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,411 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,419 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,426 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,433 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,439 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,439 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,439 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:56,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:56,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:56,445 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,445 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,455 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,467 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,473 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,473 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:56,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:56,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:56,479 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,479 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,486 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,491 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,496 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,502 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,502 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:56,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:56,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:56,508 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,508 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,531 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,536 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,536 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,536 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:56,538 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:56,540 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:56,542 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,542 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,549 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,578 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,584 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,591 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,591 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,591 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:56,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:56,595 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:56,597 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,597 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,610 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,621 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,622 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,622 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:56,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:56,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:56,627 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,627 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,634 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,646 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,653 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,654 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,654 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:56,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:56,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:56,662 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,662 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,670 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,676 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,683 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,689 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,689 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:56,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:56,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:56,696 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,696 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,703 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,717 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,723 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,723 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,724 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:56,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:56,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:56,729 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,730 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,743 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,750 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,756 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,756 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:56,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:56,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:56,763 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,763 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,770 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,776 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,782 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,788 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,789 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,789 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:56,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:56,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:56,795 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,795 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,802 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,814 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,820 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,820 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:56,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:56,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:56,826 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,827 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,853 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,859 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,873 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:56,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:56,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:56,879 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,887 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,892 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,899 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,905 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,905 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,905 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:56,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:56,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:56,911 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,911 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,918 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,924 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,931 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,937 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,938 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,938 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:56,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:56,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:56,944 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,944 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,951 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,963 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:56,971 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:56,971 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:56,973 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:56,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:56,978 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:56,978 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:56,986 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,018 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,018 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:57,018 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:57,020 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:57,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:57,024 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,025 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,031 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,037 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,044 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,063 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,063 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:57,063 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:57,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:57,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:57,067 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,067 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,075 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,081 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,088 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:50:57,094 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:50:57,094 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:57,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:57,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:57,097 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:57,097 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,098 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,099 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,101 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:57,102 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:57,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:57,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:57,104 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:57,104 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,115 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,124 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,134 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,144 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,145 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:50:57,145 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:57,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:57,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:57,152 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:50:57,152 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,155 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,155 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,156 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,157 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,157 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:57,157 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:57,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:57,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:57,160 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:57,160 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,161 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,161 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,162 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,163 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,163 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:57,163 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:57,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:57,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:57,167 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,168 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,176 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,188 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,195 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,195 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:57,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:57,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:57,201 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,208 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,220 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,228 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,228 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:57,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:57,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:57,234 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,234 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,241 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,247 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,253 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,259 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,260 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,260 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:57,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:57,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:57,266 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,266 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,273 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,279 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,285 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,292 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,292 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,292 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:57,294 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:57,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:57,298 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,299 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,305 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,333 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,352 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,359 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,359 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:57,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:57,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:57,365 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,365 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,373 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,385 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,391 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,391 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,392 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:57,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:57,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:57,397 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,397 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,404 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,410 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,416 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,422 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,423 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,423 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:57,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:57,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:57,429 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,429 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,436 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,442 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,454 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,455 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,455 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:57,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:57,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:57,461 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,461 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,474 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,480 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,486 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,487 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,487 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:57,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:57,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:57,493 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,493 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,500 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,507 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,513 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,519 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,520 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,520 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:57,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:57,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:57,525 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,526 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,538 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,545 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,567 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,568 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,568 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:57,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:57,571 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:57,574 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,574 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,600 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,600 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,600 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:57,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:57,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:57,606 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,606 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,613 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,619 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,631 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,631 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:57,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:57,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:57,638 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,638 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,645 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,652 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,658 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,665 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,665 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,665 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:57,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:57,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:57,671 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,671 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,678 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,684 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,690 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,696 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,696 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,696 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:57,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:57,700 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:57,702 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,702 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,715 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,721 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,727 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,728 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,728 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:57,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:57,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:57,734 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,734 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,747 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,753 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,758 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,759 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,759 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:57,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:57,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:57,765 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,765 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,772 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,784 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,790 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,790 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:57,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:57,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:57,796 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,796 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,803 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,809 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,815 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,820 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,821 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:57,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:57,825 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:57,825 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,825 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,854 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:50:57,873 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:50:57,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:57,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:57,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:57,876 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:57,876 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,877 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,877 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,878 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,879 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:57,879 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:57,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:57,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:57,882 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:57,882 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,892 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,902 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,911 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,920 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:57,922 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:50:57,922 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:57,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:57,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:57,929 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:50:57,929 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,931 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,933 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,935 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,935 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:57,935 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:57,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:57,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:57,938 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:57,938 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:57,939 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,939 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,940 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,941 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:57,941 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:57,941 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:57,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:57,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:57,945 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,945 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,952 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:57,958 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:57,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:57,972 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:57,973 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:57,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:57,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:57,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:57,978 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:57,979 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:57,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:57,992 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:57,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,004 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,004 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,004 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:58,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:58,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:58,010 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,010 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,023 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,029 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,035 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,035 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,035 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:58,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:58,038 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:58,041 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,041 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,048 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,090 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,107 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:58,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:58,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:58,114 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,114 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,135 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,141 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,142 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,142 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:58,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:58,146 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:58,149 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,149 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,156 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,162 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,168 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,175 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:58,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:58,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:58,181 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,181 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,188 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,194 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,201 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,207 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,208 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:58,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:58,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:58,213 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,214 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,221 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,227 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,239 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,239 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:58,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:58,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:58,252 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,253 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,260 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,266 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,278 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,278 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:58,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:58,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:58,284 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,284 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,296 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,308 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,309 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:58,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:58,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:58,314 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,315 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,321 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,327 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,370 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,381 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,381 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:58,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:58,384 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:58,386 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,387 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,410 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,420 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,421 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,421 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:58,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:58,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:58,426 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,427 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,446 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,452 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,452 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,452 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:58,454 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:58,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:58,457 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,458 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,465 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,471 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,476 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,482 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:58,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:58,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:58,488 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,507 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,513 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,514 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,514 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:58,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:58,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:58,519 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,520 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,526 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,538 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,544 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,545 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,545 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:58,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:58,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:58,550 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,550 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,580 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,585 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,585 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,585 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:58,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:58,588 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:58,590 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,591 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,597 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,602 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,613 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,613 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,613 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:58,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:58,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:58,618 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,619 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,626 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,631 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,642 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,642 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:58,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:58,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:58,646 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,647 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,653 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,659 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,665 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,670 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:50:58,670 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:50:58,670 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:58,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:58,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:58,673 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:58,673 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:58,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,675 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,676 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,676 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:58,677 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:58,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:58,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:58,679 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:58,679 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:58,690 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:58,700 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:58,709 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:58,719 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:58,720 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:50:58,721 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:58,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:58,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:58,728 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:50:58,728 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:58,728 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,729 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,730 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,731 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:58,731 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:58,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:58,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:58,733 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:58,733 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:58,734 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,736 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:58,736 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:58,736 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:58,736 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:58,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:58,740 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,740 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,747 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,752 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,764 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,764 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:58,764 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:58,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:58,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:58,769 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,770 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,777 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,782 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,788 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,794 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,794 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:58,794 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:58,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:58,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:58,799 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,799 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,806 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,812 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,817 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,834 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,834 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:58,835 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:58,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:58,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:58,840 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,840 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,861 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,879 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:58,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:58,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:58,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:58,885 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,886 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,896 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,902 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,915 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:58,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:58,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:58,918 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:58,920 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,920 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,928 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,941 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,947 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,947 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:58,948 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:58,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:58,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:58,953 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,954 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,960 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,966 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,974 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,980 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:58,980 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:58,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:58,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:58,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:58,986 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:58,986 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:58,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,000 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,007 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,014 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,014 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,014 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:59,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:59,018 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:59,020 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,020 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,046 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,046 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:59,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:59,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:59,051 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,051 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,058 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,090 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,097 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,103 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,104 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,104 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:59,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:59,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:59,110 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,110 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,117 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,123 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,133 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,139 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:59,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:59,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:59,145 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,146 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,153 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,172 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,172 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,172 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:59,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:59,176 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:59,178 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,178 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,185 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,198 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,204 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,204 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:59,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:59,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:59,210 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,210 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,237 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,237 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,237 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:50:59,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:50:59,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:59,243 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,243 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,250 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,263 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,270 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:50:59,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:50:59,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:59,276 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,276 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,283 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,295 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,300 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,301 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,301 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:50:59,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:50:59,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:59,306 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,306 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,313 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,319 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,325 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,331 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,331 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,332 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:50:59,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:50:59,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:59,337 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,338 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,362 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,368 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,380 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,380 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:50:59,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:50:59,384 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:59,386 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,386 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,393 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,411 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,411 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,412 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:50:59,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:50:59,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:59,416 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,416 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,423 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,430 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,436 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,442 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:50:59,442 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:50:59,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:50:59,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:50:59,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:59,445 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:59,445 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:59,446 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,447 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,449 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,449 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:59,449 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:50:59,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:50:59,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:59,451 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:59,451 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:59,462 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:59,472 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:59,481 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:59,491 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:50:59,493 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:50:59,493 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:50:59,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:50:59,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:59,500 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:50:59,500 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:59,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,503 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,505 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:59,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:50:59,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:50:59,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:59,507 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:50:59,507 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:50:59,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,510 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:50:59,510 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:50:59,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:50:59,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:50:59,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:59,514 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,515 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,528 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,535 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,541 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,541 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,541 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:50:59,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:50:59,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:59,547 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,547 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,554 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,594 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,600 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,600 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,600 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:50:59,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:50:59,604 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:59,606 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,606 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,613 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,619 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,631 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,637 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,637 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:50:59,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:50:59,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:59,643 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,644 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,657 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,664 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,670 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,670 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,670 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:50:59,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:50:59,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:59,676 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,676 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,683 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,696 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,702 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,702 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,702 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:50:59,704 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:50:59,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:59,708 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,708 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,715 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,721 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,727 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,733 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,733 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,733 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:50:59,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:50:59,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:59,739 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,739 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,746 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,752 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,766 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,766 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:50:59,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:50:59,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:59,772 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,772 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,779 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,791 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,797 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,797 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,798 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:50:59,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:50:59,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:59,803 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,803 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,810 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,816 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,823 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,829 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,830 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,830 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:50:59,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:50:59,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:59,836 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,836 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,887 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,893 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,893 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:50:59,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:50:59,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:59,899 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,899 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,920 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,926 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,926 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:50:59,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:50:59,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:59,932 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,933 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,940 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,946 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,959 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,960 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,960 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:50:59,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:50:59,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:59,966 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:50:59,966 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:50:59,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:50:59,994 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:50:59,995 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:50:59,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:50:59,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:00,000 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,001 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,018 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,024 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,030 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,031 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:51:00,031 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:00,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:00,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:00,037 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,037 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,051 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,080 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,081 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:51:00,081 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:00,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:00,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:00,086 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,087 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,113 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:51:00,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:00,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:00,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:00,119 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,119 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,146 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:51:00,146 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:00,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:00,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:00,151 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,152 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,170 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,176 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,183 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:51:00,183 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:00,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:00,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:00,189 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,189 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,196 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,203 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,208 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,216 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,216 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:51:00,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:00,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:00,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:00,221 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,221 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,253 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,261 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,266 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:51:00,267 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:51:00,267 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:00,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:00,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:00,269 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:00,270 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:00,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,271 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,273 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,274 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,274 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:00,274 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:00,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:00,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:00,277 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:00,277 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:00,288 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:00,298 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:00,308 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:00,317 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:00,320 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:00,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:00,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:00,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:00,331 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:00,331 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:00,332 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,335 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:00,336 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:00,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:00,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:00,338 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:00,338 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:00,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,341 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,342 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:00,342 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:00,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:00,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:00,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:00,347 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,348 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,370 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,376 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,376 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,376 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:00,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:00,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:00,382 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,383 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,389 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,395 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,401 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,408 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,408 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,408 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:00,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:00,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:00,414 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,414 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,421 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,427 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,433 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,438 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,439 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,439 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:00,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:00,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:00,444 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,445 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,451 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,457 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,463 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,470 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:00,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:00,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:00,478 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,478 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,488 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,505 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:00,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:00,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:00,511 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,511 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,530 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,536 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,536 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,536 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:00,538 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:00,540 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:00,542 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,542 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,550 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,589 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,589 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,590 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:00,591 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:00,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:00,595 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,595 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,603 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,610 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,617 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,624 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,624 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,624 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:00,626 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:00,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:00,630 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,630 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,643 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,656 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,656 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:00,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:00,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:00,661 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,662 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,669 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,680 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,686 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:00,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:00,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:00,692 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,692 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,699 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,705 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,711 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,717 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,717 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,718 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:00,719 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:00,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:00,723 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,723 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,730 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,747 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,748 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,748 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:00,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:00,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:00,753 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,753 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,767 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,779 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,779 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:00,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:00,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:00,785 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,785 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,792 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,798 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,810 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,811 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,811 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:00,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:00,814 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:00,816 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,816 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,823 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,829 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,878 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,879 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,879 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:00,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:00,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:00,885 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,885 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,894 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,901 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,910 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,917 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,917 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,917 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:00,919 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:00,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:00,923 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,924 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,932 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,938 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,951 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,951 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,951 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:00,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:00,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:00,957 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,957 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,970 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,982 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:00,982 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:00,983 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:00,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:00,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:00,988 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:00,989 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:00,996 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,002 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,008 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,015 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,015 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:01,015 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:01,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:01,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:01,020 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,020 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,047 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:51:01,048 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:51:01,048 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:01,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:01,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:01,051 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:01,051 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:01,052 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,054 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,060 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,060 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:01,061 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:01,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:01,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:01,063 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:01,063 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:01,076 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:01,086 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:01,095 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:01,105 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:01,106 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:01,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:01,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:01,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:01,113 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:01,113 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:01,114 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,115 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,115 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,116 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,116 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:01,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:01,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:01,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:01,119 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:01,119 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:01,120 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,120 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,121 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,122 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:01,122 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:01,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:01,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:01,126 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,126 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,133 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,146 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,153 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,153 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:01,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:01,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:01,158 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,158 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,172 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,185 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,185 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:01,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:01,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:01,190 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,191 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,198 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,208 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,221 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,221 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,221 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:01,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:01,225 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:01,227 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,227 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,241 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,247 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,254 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,254 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,254 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:01,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:01,257 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:01,260 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,260 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,284 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,291 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:01,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:01,294 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:01,296 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,297 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,310 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,316 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,344 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,344 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,344 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:01,346 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:01,348 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:01,350 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,351 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,370 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,376 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,377 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,377 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:01,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:01,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:01,382 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,383 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,390 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,396 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,402 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,409 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,409 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,409 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:01,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:01,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:01,414 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,414 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,423 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,429 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,439 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,467 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,467 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,468 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:01,469 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:01,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:01,473 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,473 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,483 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,510 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,510 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:01,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:01,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:01,516 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,516 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,526 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,535 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,543 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,551 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,551 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,551 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:01,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:01,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:01,557 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,557 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,603 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,684 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,692 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,692 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:01,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:01,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:01,697 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,698 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,713 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,726 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,726 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:01,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:01,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:01,731 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,731 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,738 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,750 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,756 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,756 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:01,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:01,759 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:01,761 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,761 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,775 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,781 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,792 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,792 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:01,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:01,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:01,798 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,798 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,806 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,813 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,819 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,832 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,832 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,833 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:01,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:01,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:01,838 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,839 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,857 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,863 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,869 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,875 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,875 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,875 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:01,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:01,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:01,880 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,881 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,900 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,906 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,906 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:01,908 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:01,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:01,912 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,912 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,926 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,932 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,937 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,938 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,938 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:01,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:01,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:01,942 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:01,942 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:01,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,955 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,961 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:51:01,967 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:51:01,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:01,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:01,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:01,969 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:01,969 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:01,970 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,972 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:01,973 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:01,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:01,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:01,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:01,975 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:01,976 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:01,986 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:01,996 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:02,006 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:02,015 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:02,017 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:02,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:02,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:02,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:02,025 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:02,025 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:02,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,028 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,029 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,029 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,029 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:02,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:02,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:02,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:02,032 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:02,032 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:02,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,034 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,034 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,035 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,035 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:02,035 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:02,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:02,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:02,039 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,040 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,048 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,054 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,078 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,079 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,079 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:02,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:02,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:02,085 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,085 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,112 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,113 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:02,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:02,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:02,120 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,120 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,133 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,146 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,146 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:02,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:02,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:02,152 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,152 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,166 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,172 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,178 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:02,180 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:02,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:02,183 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,183 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,196 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,202 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,207 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,208 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:02,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:02,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:02,213 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,219 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,225 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:02,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:02,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:02,242 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,242 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,250 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,255 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,261 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,267 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,267 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,267 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:02,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:02,271 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:02,273 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,273 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,281 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,292 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,298 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,298 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:02,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:02,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:02,303 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,304 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,317 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,323 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,340 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,340 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,340 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:02,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:02,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:02,346 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,346 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,355 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,361 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,367 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,374 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:02,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:02,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:02,380 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,380 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,387 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,393 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,405 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:02,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:02,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:02,410 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,411 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,418 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,425 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,431 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,437 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:02,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:02,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:02,442 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,450 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,456 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,467 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,467 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,467 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:02,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:02,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:02,472 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,472 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,486 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,492 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,497 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,497 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,497 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:02,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:02,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:02,503 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,503 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,511 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,517 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,530 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,530 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,530 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:02,531 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:02,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:02,535 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,535 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,542 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,548 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,554 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,560 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,560 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,560 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:02,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:02,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:02,565 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,566 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,599 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,605 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,605 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:02,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:02,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:02,611 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,611 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,618 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,623 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,637 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,637 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:02,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:02,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:02,642 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,643 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,650 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,662 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,668 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,669 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:02,671 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:02,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:02,673 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,673 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,681 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,687 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,693 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,699 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:51:02,699 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:51:02,699 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:02,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:02,702 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:02,702 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:02,702 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:02,703 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,704 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,705 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,706 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:02,706 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:02,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:02,708 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:02,708 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:02,708 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:02,719 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:02,728 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:02,738 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:02,748 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:02,749 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:02,750 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:02,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:02,756 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:02,756 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:02,756 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:02,757 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,760 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:02,761 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:02,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:02,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:02,763 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:02,763 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:02,764 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,766 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:02,766 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:02,766 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:02,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:02,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:02,771 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,771 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,784 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,797 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,797 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:02,797 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:02,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:02,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:02,803 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,803 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,810 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,816 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,822 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,828 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,828 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:02,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:02,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:02,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:02,834 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,834 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,870 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,877 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,884 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,890 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:02,890 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:02,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:02,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:02,896 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,896 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,903 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,909 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,916 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,922 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:02,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:02,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:02,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:02,928 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,928 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,935 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,941 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,947 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,954 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:02,954 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:02,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:02,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:02,959 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,959 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,979 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:02,986 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:02,986 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:02,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:02,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:02,991 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:02,992 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:02,999 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,017 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:03,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:03,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:03,023 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,023 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,030 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,037 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,043 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,049 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,049 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,049 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:03,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:03,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:03,054 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,055 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,068 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,074 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,108 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,108 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,108 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:03,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:03,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:03,114 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,114 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,121 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,127 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,134 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,140 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,141 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,141 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:03,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:03,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:03,147 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,147 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,162 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,169 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,176 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,184 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,184 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:03,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:03,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:03,191 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,191 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,199 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,215 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,222 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,222 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:03,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:03,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:03,228 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,228 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,242 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,249 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,256 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,256 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,256 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:03,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:03,260 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:03,262 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,262 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,276 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,283 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,290 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,290 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,290 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:03,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:03,294 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:03,296 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,296 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,309 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,315 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,321 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,322 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,322 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:03,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:03,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:03,327 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,327 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,340 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,347 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,353 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,353 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,354 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:03,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:03,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:03,359 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,359 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,396 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,403 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,409 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,415 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,415 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,415 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:03,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:03,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:03,420 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,421 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,428 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,434 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,446 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,447 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,447 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:03,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:03,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:03,452 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,452 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,459 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,466 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,472 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,478 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,478 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,478 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:03,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:03,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:03,483 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,483 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,490 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,496 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:51:03,510 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:51:03,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:03,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:03,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:03,513 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:03,513 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:03,514 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,514 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,516 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,516 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:03,516 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:03,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:03,518 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:03,518 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:03,519 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:03,529 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:03,538 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:03,547 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:03,556 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:03,558 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:03,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:03,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:03,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:03,564 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:03,565 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:03,565 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,569 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,569 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:03,569 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:03,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:03,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:03,572 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:03,572 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:03,573 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,573 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:03,575 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:03,575 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:03,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:03,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:03,579 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,579 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,602 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,614 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,621 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,621 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:03,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:03,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:03,627 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,627 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,634 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,640 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,647 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,653 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,653 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,653 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:03,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:03,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:03,658 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,659 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,666 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,680 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,687 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,687 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:03,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:03,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:03,692 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,692 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,699 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,705 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,711 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,717 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,718 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,718 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:03,719 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:03,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:03,724 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,724 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,743 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,749 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,749 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,749 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:03,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:03,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:03,755 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,755 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,762 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,774 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,780 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,781 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,781 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:03,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:03,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:03,786 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,786 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,793 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,799 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,805 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,811 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,812 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:03,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:03,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:03,817 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,818 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,844 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,844 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,844 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:03,846 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:03,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:03,850 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,850 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,877 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,883 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,889 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,895 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,896 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:03,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:03,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:03,901 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,902 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,909 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,916 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,929 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,929 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,929 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:03,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:03,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:03,934 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,935 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,942 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,948 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,961 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,973 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:03,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:03,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:03,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:03,982 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:03,982 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:03,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:03,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,012 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,013 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:04,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:04,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:04,019 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,019 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,045 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:04,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:04,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:04,051 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,051 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,065 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,078 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,078 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,079 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:04,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:04,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:04,084 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,085 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,126 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,126 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:04,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:04,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:04,132 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,140 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,149 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,156 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,162 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,163 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,163 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:04,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:04,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:04,169 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,170 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,185 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,197 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,198 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,198 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:04,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:04,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:04,203 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,203 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,211 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,218 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,230 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,231 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:04,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:04,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:04,236 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,237 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,244 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,251 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,265 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:04,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:04,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:04,269 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,269 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,283 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,296 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:51:04,296 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:51:04,296 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:04,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:04,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:04,299 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:04,299 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:04,300 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,302 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,304 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:04,304 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:04,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:04,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:04,306 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:04,306 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:04,317 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:04,326 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:04,335 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:04,347 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:04,365 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:04,365 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:04,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:04,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:04,372 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:04,372 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:04,375 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,375 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,376 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,377 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,377 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:04,377 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:04,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:04,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:04,380 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:04,380 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:04,381 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,381 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,382 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,383 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:04,383 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:04,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:04,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:04,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:04,387 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,387 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,395 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,401 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,408 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,414 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,414 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,415 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:04,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:04,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:04,420 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,420 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,428 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,441 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,447 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,447 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,448 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:04,449 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:04,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:04,453 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,453 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,467 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,480 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,480 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,480 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:04,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:04,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:04,486 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,486 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,511 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,517 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,518 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,518 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:04,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:04,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:04,524 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,524 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,531 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,537 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,544 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,550 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,550 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,550 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:04,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:04,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:04,556 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,556 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,564 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,571 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,589 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,596 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,596 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,597 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:04,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:04,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:04,603 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,603 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,611 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,618 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,631 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,631 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:04,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:04,635 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:04,638 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,638 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,657 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,663 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,669 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,670 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,670 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:04,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:04,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:04,676 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,676 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,684 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,697 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,703 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,704 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,704 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:04,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:04,708 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:04,710 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,711 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,720 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,727 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,742 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,742 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:04,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:04,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:04,749 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,749 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,757 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,766 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,780 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,780 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:04,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:04,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:04,786 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,786 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,797 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,805 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,812 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,820 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,820 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:04,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:04,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:04,826 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,827 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,835 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,852 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,880 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:04,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:04,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:04,885 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,886 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,895 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,903 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,911 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,919 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:04,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:04,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:04,925 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,925 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,941 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,950 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,957 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:04,957 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:04,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:04,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:04,963 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:04,963 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:04,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,977 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:04,999 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,000 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:05,000 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:05,001 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:05,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:05,005 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,005 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,025 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,031 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,031 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:05,031 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:05,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:05,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:05,037 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,037 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,044 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,049 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,055 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,060 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,060 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:05,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:05,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:05,064 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:05,066 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,066 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,092 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,097 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,103 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,103 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:05,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:05,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:05,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:05,106 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,107 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,114 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,124 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,130 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:51:05,130 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:51:05,130 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:05,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:05,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:05,133 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:05,133 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:05,133 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,134 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,135 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,136 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,136 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:05,136 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:05,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:05,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:05,138 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:05,138 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:05,148 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:05,157 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:05,166 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:05,174 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:05,176 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:05,176 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:05,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:05,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:05,183 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:05,183 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:05,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,186 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,187 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,188 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,188 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:05,188 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:05,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:05,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:05,191 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:05,191 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:05,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,192 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,193 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,193 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,193 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:05,193 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:05,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:05,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:05,197 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,197 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,210 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,244 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,245 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,245 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:05,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:05,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:05,250 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,250 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,271 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,277 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,277 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:05,279 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:05,281 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:05,283 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,283 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,309 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,310 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:05,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:05,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:05,316 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,316 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,323 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,329 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,359 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,359 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:05,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:05,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:05,364 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,365 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,378 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,385 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,394 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,402 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,402 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,402 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:05,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:05,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:05,407 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,408 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,415 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,422 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,429 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,435 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:05,437 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:05,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:05,441 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,441 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,454 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,462 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,469 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,469 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,469 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:05,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:05,472 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:05,475 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,475 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,503 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:05,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:05,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:05,508 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,509 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,516 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,535 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,536 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,536 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:05,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:05,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:05,541 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,542 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,549 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,568 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,568 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:05,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:05,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:05,574 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,574 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,601 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,607 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,613 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,614 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,614 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:05,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:05,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:05,620 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,620 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,643 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,652 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:05,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:05,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:05,658 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,658 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,665 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,672 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,686 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:05,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:05,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:05,692 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,692 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,700 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,707 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,715 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,721 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,722 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,722 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:05,724 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:05,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:05,728 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,728 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,742 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,752 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,760 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,760 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:05,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:05,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:05,766 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,766 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,780 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,787 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,794 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,794 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,794 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:05,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:05,798 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:05,800 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,800 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,808 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,815 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,822 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,829 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,830 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,830 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:05,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:05,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:05,836 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,837 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,844 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,865 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,875 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,891 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,891 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:05,893 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:05,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:05,897 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,897 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,905 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,912 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,926 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,926 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:05,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:05,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:05,931 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:05,931 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:05,938 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,946 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,960 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:51:05,960 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:51:05,961 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:05,962 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:05,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:05,963 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:05,963 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:05,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,965 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,966 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:05,967 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:05,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:05,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:05,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:05,969 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:05,969 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:05,980 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:05,990 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:06,000 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:06,010 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:06,012 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:06,012 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:06,018 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:06,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:06,019 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:06,019 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:06,020 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,020 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,021 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,022 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,022 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:06,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:06,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:06,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:06,025 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:06,025 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:06,025 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,027 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:06,028 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:06,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:06,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:06,031 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,031 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,044 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,050 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,057 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,057 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:06,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:06,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:06,063 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,063 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,070 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,075 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,081 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,086 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,086 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,086 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:06,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:06,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:06,091 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,092 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,109 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,115 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,121 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,126 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,127 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:06,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:06,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:06,132 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,150 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,165 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,165 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:06,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:06,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:06,170 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,171 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,183 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,195 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,195 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:06,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:06,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:06,200 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,200 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,212 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,218 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,224 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,224 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:06,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:06,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:06,229 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,229 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,242 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,249 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,259 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,259 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,259 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:06,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:06,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:06,265 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,265 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,283 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,289 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,289 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:06,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:06,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:06,295 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,295 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,302 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,308 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,314 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,320 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,321 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,321 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:06,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:06,324 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:06,326 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,326 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,333 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,345 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,351 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,351 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:06,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:06,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:06,357 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,357 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,377 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,383 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,388 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,393 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,394 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,394 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:06,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:06,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:06,399 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,399 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,407 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,412 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,418 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,424 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,424 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,424 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:06,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:06,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:06,429 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,429 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,436 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,442 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,447 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,453 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,453 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,453 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:06,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:06,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:06,459 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,460 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,467 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,485 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,485 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:06,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:06,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:06,490 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,491 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,498 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,510 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,516 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,516 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:06,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:06,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:06,522 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,522 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,558 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,575 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,631 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,639 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,639 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,640 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:06,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:06,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:06,646 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,647 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,657 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,664 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,671 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,678 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,678 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,679 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:06,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:06,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:06,684 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,685 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,692 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,698 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,712 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,712 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,712 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:06,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:06,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:06,718 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,719 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,727 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,733 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,739 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,745 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,745 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,745 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:06,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:06,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:06,749 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,749 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,772 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,777 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:51:06,777 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:51:06,778 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:06,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:06,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:06,780 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:06,780 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:06,781 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,782 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,784 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,784 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:06,784 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:06,785 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:06,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:06,786 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:06,786 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:06,797 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:06,807 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:06,817 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:06,826 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:06,828 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:06,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:06,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:06,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:06,835 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:06,835 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:06,836 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,840 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,842 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:06,842 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:06,842 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:06,842 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:06,844 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:06,845 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:06,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,847 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,848 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:06,848 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:06,848 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:06,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:06,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:06,852 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,852 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,869 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,876 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,888 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:06,888 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:06,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:06,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:06,894 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,894 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,901 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,913 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,930 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,930 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:06,931 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:06,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:06,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:06,936 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,936 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,951 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,956 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,962 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,962 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:06,962 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:06,964 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:06,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:06,967 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:06,967 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:06,975 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,980 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:06,986 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,002 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,002 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,002 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:07,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:07,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:07,008 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,008 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,016 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,021 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,033 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,033 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:07,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:07,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:07,039 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,039 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,047 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,053 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,058 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,063 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,063 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:07,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:07,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:07,069 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,069 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,076 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,082 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,087 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,093 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:07,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:07,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:07,099 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,099 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,112 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,117 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,123 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,123 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,123 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:07,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:07,126 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:07,128 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,129 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,137 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,143 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,148 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,154 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,154 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,155 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:07,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:07,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:07,159 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,160 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,167 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,199 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,199 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:07,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:07,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:07,209 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,209 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,232 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,238 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,244 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,245 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,245 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:07,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:07,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:07,251 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,251 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,259 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,271 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,278 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,278 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:07,279 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:07,281 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:07,284 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,284 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,292 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,298 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,310 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,310 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:07,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:07,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:07,316 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,316 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,324 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,341 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,347 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,347 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,348 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:07,349 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:07,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:07,356 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,356 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,365 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,372 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,385 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,385 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,385 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:07,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:07,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:07,391 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,391 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,398 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,404 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,410 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,417 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,417 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,417 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:07,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:07,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:07,422 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,423 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,444 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,450 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,456 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,457 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,457 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:07,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:07,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:07,462 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,463 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,477 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,487 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,487 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:07,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:07,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:07,493 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,493 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,500 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,512 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,517 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,517 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:07,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:07,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:07,522 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,522 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,534 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,541 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,547 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:51:07,547 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:51:07,547 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:07,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:07,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:07,549 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:07,550 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:07,550 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,551 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,553 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,553 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:07,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:07,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:07,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:07,555 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:07,555 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:07,565 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:07,574 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:07,583 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:07,593 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:07,598 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:07,598 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:07,604 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:07,605 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:07,605 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:07,605 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:07,606 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,607 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,609 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,609 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:07,609 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:07,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:07,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:07,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:07,613 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:07,613 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,614 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:07,615 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:07,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:07,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:07,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:07,620 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,620 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,628 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,634 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,648 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,648 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,648 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:07,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:07,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:07,654 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,654 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,663 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,669 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,676 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,696 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,696 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,696 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:07,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:07,700 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:07,702 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,702 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,713 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,733 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,733 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,733 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:07,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:07,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:07,739 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,740 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,747 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,754 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,761 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,768 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:07,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:07,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:07,775 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,775 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,787 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,792 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,798 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,804 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,804 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:07,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:07,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:07,809 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,810 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,817 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,829 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,835 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,835 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,835 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:07,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:07,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:07,841 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,841 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,848 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,853 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,865 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,866 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:07,867 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:07,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:07,871 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,871 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,878 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,884 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,912 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,912 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:07,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:07,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:07,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:07,918 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:07,918 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:07,954 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:07,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,000 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,001 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,001 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:08,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:08,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:08,009 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,009 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,018 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,025 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,031 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,038 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,038 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,038 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:08,040 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:08,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:08,044 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,044 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,052 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,066 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,073 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,073 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:08,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:08,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:08,079 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,079 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,087 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,108 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,108 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,108 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:08,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:08,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:08,114 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,114 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,130 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,141 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,147 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,147 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,148 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:08,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:08,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:08,154 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,154 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,161 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,168 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,181 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,182 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,182 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:08,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:08,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:08,188 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,188 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,211 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,231 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:08,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:08,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:08,237 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,238 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,245 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,251 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,257 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,264 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,265 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:08,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:08,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:08,271 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,271 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,281 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,288 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,301 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,301 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:08,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:08,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:08,307 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,307 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,321 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,327 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,343 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,343 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,343 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:08,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:08,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:08,349 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,349 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,357 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,363 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,370 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,376 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,377 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,377 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:08,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:08,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:08,381 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,381 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,389 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,395 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,402 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,408 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:51:08,409 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:51:08,409 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:08,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:08,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:08,412 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:08,412 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:08,413 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,413 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,414 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,415 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,415 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:08,415 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:08,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:08,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:08,417 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:08,418 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:08,440 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:08,450 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:08,459 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:08,468 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:08,470 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:08,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:08,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:08,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:08,478 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:08,478 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:08,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,482 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:08,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:08,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:08,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:08,485 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:08,485 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:08,485 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,486 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,487 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,487 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:08,488 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:08,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:08,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:08,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:08,491 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,492 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,505 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,511 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,517 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,517 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:08,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:08,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:08,523 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,523 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,530 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,536 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,541 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,548 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,548 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,548 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:08,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:08,551 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:08,553 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,554 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,561 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,567 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,580 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,580 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:08,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:08,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:08,585 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,592 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,598 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,616 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:08,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:08,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:08,623 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,623 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,631 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,638 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,645 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,652 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:08,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:08,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:08,657 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,658 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,665 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,673 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,692 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,693 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,693 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:08,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:08,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:08,698 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,699 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,715 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,722 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,728 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,735 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:08,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:08,739 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:08,741 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,741 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,749 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,755 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,762 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,768 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,769 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:08,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:08,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:08,775 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,775 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,796 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,803 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,803 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:08,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:08,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:08,809 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,809 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,817 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,823 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,836 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,836 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:08,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:08,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:08,842 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,842 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,863 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,872 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,872 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:08,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:08,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:08,877 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,878 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,886 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,892 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,899 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,906 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,906 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:08,908 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:08,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:08,911 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,912 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,920 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,942 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,956 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,957 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,957 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:08,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:08,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:08,962 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:08,963 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:08,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,979 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,986 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:08,994 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:08,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:08,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:08,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:09,001 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,001 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,010 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,024 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,031 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,032 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:09,032 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:09,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:09,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:09,038 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,038 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,049 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,058 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,065 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,074 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,074 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:09,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:09,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:09,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:09,080 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,081 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,089 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,097 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,107 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,115 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,115 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:09,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:09,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:09,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:09,122 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,122 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,130 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,137 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,152 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:09,152 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:09,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:09,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:09,158 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,158 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,166 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,196 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,197 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:09,197 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:09,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:09,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:09,202 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,202 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,225 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,232 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,239 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:51:09,240 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:51:09,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:09,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:09,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:09,243 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:09,244 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:09,245 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,246 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,247 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,248 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:09,248 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:09,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:09,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:09,250 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:09,250 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:09,263 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:09,274 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:09,284 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:09,294 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:09,296 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:09,296 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:09,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:09,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:09,303 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:09,303 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:09,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,305 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,306 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:09,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:09,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:09,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:09,309 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:09,309 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:09,310 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,313 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:09,313 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:09,313 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:09,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:09,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:09,317 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,317 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,341 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,349 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,350 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:09,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:09,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:09,355 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,356 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,386 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,386 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,386 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:09,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:09,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:09,392 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,393 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,401 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,408 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,416 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,456 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,456 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,457 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:09,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:09,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:09,462 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,462 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,469 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,477 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,483 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,489 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,489 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:09,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:09,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:09,494 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,494 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,514 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,520 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,520 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:09,522 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:09,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:09,525 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,526 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,533 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,538 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,545 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,551 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,551 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,551 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:09,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:09,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:09,556 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,557 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,580 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,599 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,609 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,609 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:09,611 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:09,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:09,618 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,618 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,626 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,633 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,642 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,649 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,649 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:09,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:09,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:09,654 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,655 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,671 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,678 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,709 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,709 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:09,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:09,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:09,715 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,715 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,723 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,730 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,744 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,744 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:09,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:09,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:09,750 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,750 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,758 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,764 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,771 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,776 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,777 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:09,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:09,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:09,782 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,782 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,795 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,801 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,808 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,808 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:09,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:09,811 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:09,813 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,813 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,820 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,826 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,832 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,837 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,838 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:09,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:09,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:09,843 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,843 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,855 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,861 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,868 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:09,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:09,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:09,873 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,873 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,880 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,886 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,892 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,897 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,898 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,898 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:09,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:09,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:09,903 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,903 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,910 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,916 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,931 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,945 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:09,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:09,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:09,950 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,951 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,958 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,978 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,978 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:09,978 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:09,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:09,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:09,984 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:09,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:09,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:09,997 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,003 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,009 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,010 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:10,010 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:10,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:10,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:10,015 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,016 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,023 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,028 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,035 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,040 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:10,041 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:10,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:10,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:10,044 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,044 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,051 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,068 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:51:10,068 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:51:10,068 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:10,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:10,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:10,071 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:10,071 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,074 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,074 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:10,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:10,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:10,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:10,076 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:10,077 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,087 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,096 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,105 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,114 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,115 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:10,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:10,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:10,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:10,123 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:10,123 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,124 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,126 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:10,127 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:10,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:10,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:10,130 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:10,130 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,131 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,131 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,133 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:10,133 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:10,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:10,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:10,137 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,137 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,151 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,157 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,164 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,165 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,165 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:10,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:10,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:10,170 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,171 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,190 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,212 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,212 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,212 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:10,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:10,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:10,218 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,218 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,246 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,255 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,263 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,271 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,272 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,272 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:10,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:10,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:10,278 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,278 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,309 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,309 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:10,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:10,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:10,314 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,315 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,323 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,330 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,336 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,343 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,343 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,343 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:10,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:10,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:10,349 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,349 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,357 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,377 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,378 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,378 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:10,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:10,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:10,383 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,384 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,391 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,398 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,404 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,410 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,411 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,411 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:10,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:10,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:10,417 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,417 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,424 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,446 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,453 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,460 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,460 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,460 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:10,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:10,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:10,466 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,466 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,474 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,488 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,495 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,495 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:10,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:10,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:10,501 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,502 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,510 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,517 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,531 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,531 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,531 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:10,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:10,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:10,537 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,537 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,545 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,558 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,565 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,565 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,565 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:10,567 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:10,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:10,571 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,572 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,594 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,601 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,602 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,602 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:10,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:10,605 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:10,608 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,608 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,616 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,623 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,637 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,637 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:10,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:10,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:10,643 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,643 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,657 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,663 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,670 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,671 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,671 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:10,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:10,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:10,676 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,676 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,684 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,727 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:10,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:10,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:10,733 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,733 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,747 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,754 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,760 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,760 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:10,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:10,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:10,767 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,767 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,775 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,782 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,796 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,796 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:10,798 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:10,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:10,802 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,803 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,811 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,818 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,825 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,831 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,832 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,832 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:10,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:10,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:10,838 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,838 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,853 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,859 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,866 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,866 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:10,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:10,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:10,871 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,871 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,895 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,902 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:51:10,902 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:51:10,903 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:10,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:10,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:10,906 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:10,906 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,909 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,909 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:10,910 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:10,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:10,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:10,912 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:10,912 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,925 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,942 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,954 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,964 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:10,965 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:10,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:10,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:10,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:10,973 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:10,973 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,977 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,977 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:10,977 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:10,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:10,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:10,980 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:10,980 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:10,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,982 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:10,984 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:10,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:10,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:10,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:10,988 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:10,988 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:10,996 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,002 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,013 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,020 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,021 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:11,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:11,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:11,027 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,027 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,036 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,043 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,050 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,057 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,057 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:11,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:11,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:11,063 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,064 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,078 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,091 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,096 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,097 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,097 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:11,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:11,100 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:11,102 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,103 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,110 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,116 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,129 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,129 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,129 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:11,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:11,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:11,134 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,134 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,141 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,147 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,153 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,159 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:11,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:11,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:11,164 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,164 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,171 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,183 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,189 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,189 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:11,191 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:11,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:11,194 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,195 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,213 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,230 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:11,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:11,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:11,241 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,241 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,254 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,259 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,266 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,266 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,266 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:11,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:11,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:11,272 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,272 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,284 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,290 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,296 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,296 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,296 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:11,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:11,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:11,301 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,301 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,309 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,314 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,320 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,326 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,326 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:11,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:11,329 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:11,331 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,331 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,346 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,352 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,358 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:11,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:11,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:11,363 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,377 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,385 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,396 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,396 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,396 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:11,398 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:11,400 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:11,402 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,402 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,411 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,418 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,425 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,436 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,437 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:11,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:11,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:11,443 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,460 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,488 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,488 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:11,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:11,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:11,495 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,495 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,534 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,534 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,535 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:11,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:11,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:11,541 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,542 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,561 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,570 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,579 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,580 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:11,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:11,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:11,585 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,586 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,595 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,612 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,621 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,621 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,622 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:11,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:11,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:11,627 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,628 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,645 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,658 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,658 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,658 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:11,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:11,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:11,664 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,673 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,687 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,694 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,694 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,694 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:11,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:11,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:11,698 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,699 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,733 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,739 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,746 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:51:11,746 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:51:11,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:11,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:11,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:11,750 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:11,750 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:11,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,752 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,753 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,753 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:11,753 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:11,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:11,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:11,755 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:11,756 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:11,766 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:11,776 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:11,785 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:11,795 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:11,797 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:11,797 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:11,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:11,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:11,804 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:11,804 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:11,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,808 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,808 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,808 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:11,809 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:11,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:11,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:11,811 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:11,812 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:11,812 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,813 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,814 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,814 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:11,815 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:11,815 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:11,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:11,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:11,819 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,819 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,827 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,833 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,848 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,848 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:11,848 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:11,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:11,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:11,854 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,854 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,861 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,868 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,874 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,881 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:11,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:11,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:11,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:11,887 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,887 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,894 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,901 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,913 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,913 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:11,913 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:11,915 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:11,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:11,919 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,919 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,927 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,933 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,956 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,963 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,963 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:11,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:11,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:11,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:11,969 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:11,969 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:11,977 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,989 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,995 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:11,995 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:11,996 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:11,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:11,999 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:12,001 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,001 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,010 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,023 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,033 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,033 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:12,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:12,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:12,039 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,039 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,047 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,053 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,065 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,065 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,065 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:12,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:12,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:12,074 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,075 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,126 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,126 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:12,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:12,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:12,132 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,141 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,149 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,156 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,163 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,163 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,163 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:12,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:12,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:12,169 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,169 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,185 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,195 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,215 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,216 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:12,217 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:12,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:12,222 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,222 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,232 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,239 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,245 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,251 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,252 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,252 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:12,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:12,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:12,258 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,258 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,266 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,279 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,286 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,286 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,286 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:12,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:12,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:12,292 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,292 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,300 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,318 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,318 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:12,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:12,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:12,324 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,324 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,332 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,345 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,351 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,352 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:12,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:12,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:12,358 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,358 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,366 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,372 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,385 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,385 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,386 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:12,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:12,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:12,391 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,391 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,411 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,417 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,418 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,418 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:12,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:12,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:12,423 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,431 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,456 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,462 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,463 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:12,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:12,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:12,468 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,468 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,476 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,483 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,496 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,496 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,496 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:12,498 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:12,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:12,503 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,504 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,512 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,519 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,526 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,532 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,532 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:12,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:12,536 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:12,537 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,537 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,545 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,551 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,558 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,565 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:51:12,565 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:51:12,566 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:12,568 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:12,568 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:12,569 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:12,569 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:12,570 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,571 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,572 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,573 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,573 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:12,573 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:12,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:12,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:12,575 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:12,576 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:12,587 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:12,598 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:12,608 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:12,617 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:12,619 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:12,619 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:12,626 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:12,626 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:12,626 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:12,626 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:12,627 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,628 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,629 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:12,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:12,630 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:12,630 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:12,633 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:12,633 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:12,634 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,634 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,635 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:12,636 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:12,636 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:12,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:12,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:12,640 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,640 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,664 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,670 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,677 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,678 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,678 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:12,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:12,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:12,684 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,684 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,693 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,700 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,721 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,732 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,732 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:12,734 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:12,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:12,739 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,739 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,748 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,773 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,773 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:12,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:12,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:12,779 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,780 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,797 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,806 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,814 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,814 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,815 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:12,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:12,818 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:12,821 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,821 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,838 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,853 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,853 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,853 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:12,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:12,857 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:12,859 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,859 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,875 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,890 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,890 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:12,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:12,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:12,896 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,896 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,905 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,912 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,926 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,926 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:12,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:12,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:12,932 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,932 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,948 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,959 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,965 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,973 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:12,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:12,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:12,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:12,979 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:12,979 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:12,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:12,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,001 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,009 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,009 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,009 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:13,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:13,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:13,015 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,016 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,024 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,030 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,037 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,044 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,045 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:13,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:13,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:13,050 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,051 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,070 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,076 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,083 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,083 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:13,085 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:13,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:13,089 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,090 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,098 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,112 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,120 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,120 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,120 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:13,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:13,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:13,126 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,126 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,134 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,141 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,159 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:13,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:13,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:13,165 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,180 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,187 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,194 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,194 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:13,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:13,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:13,201 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,234 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,241 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,254 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,254 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,255 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:13,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:13,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:13,260 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,260 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,268 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,275 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,281 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,288 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,288 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,288 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:13,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:13,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:13,294 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,295 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,310 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,316 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,323 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,323 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,323 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:13,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:13,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:13,329 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,329 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,338 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,344 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,351 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,358 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:13,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:13,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:13,364 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,373 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,386 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,393 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,393 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:13,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:13,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:13,397 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,397 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,412 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,419 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,427 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:51:13,427 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:51:13,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:13,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:13,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:13,430 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:13,430 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:13,431 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,432 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,433 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,434 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,434 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:13,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:13,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:13,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:13,436 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:13,436 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:13,453 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:13,464 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:13,473 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:13,483 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:13,485 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:13,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:13,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:13,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:13,492 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:13,492 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:13,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,494 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,494 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,495 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:13,495 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:13,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:13,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:13,498 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:13,498 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:13,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,500 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:13,501 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:13,501 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:13,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:13,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:13,505 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,505 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,513 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,527 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,533 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,534 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,534 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:13,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:13,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:13,540 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,540 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,550 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,557 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,563 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,571 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,572 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:13,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:13,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:13,578 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,578 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,586 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,592 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,599 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,606 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,607 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,607 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:13,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:13,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:13,612 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,613 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,620 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,627 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,633 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,641 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,641 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:13,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:13,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:13,647 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,647 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,655 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,662 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,669 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,677 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,677 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,677 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:13,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:13,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:13,683 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,683 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,697 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,704 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,729 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,729 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,729 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:13,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:13,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:13,735 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,735 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,746 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,753 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,766 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,767 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,767 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:13,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:13,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:13,773 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,773 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,781 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,797 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,803 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,803 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,804 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:13,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:13,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:13,810 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,810 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,819 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,826 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,833 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,840 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,840 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,841 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:13,842 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:13,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:13,846 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,847 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,854 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,868 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,874 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,875 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,875 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:13,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:13,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:13,881 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,881 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,895 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,902 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,909 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,909 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:13,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:13,913 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:13,915 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,915 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,929 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,935 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,942 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,942 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,943 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:13,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:13,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:13,948 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,949 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:13,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,977 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,984 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,990 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:13,991 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:13,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:13,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:13,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:13,996 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:13,997 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,027 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:14,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:14,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:14,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:14,032 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,032 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,046 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,053 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,060 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:14,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:14,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:14,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:14,065 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,065 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,079 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,086 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,093 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:14,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:14,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:14,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:14,100 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,100 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,109 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,115 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,129 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:14,129 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:14,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:14,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:14,134 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,134 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,142 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,149 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,156 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,162 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,163 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:14,163 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:14,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:14,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:14,169 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,169 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,176 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,197 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,198 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:14,198 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:14,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:14,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:14,202 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,202 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,210 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,246 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,253 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:51:14,253 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:51:14,253 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:14,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:14,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:14,256 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:14,256 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:14,257 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,259 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,259 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:14,259 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:14,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:14,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:14,261 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:14,262 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:14,272 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:14,281 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:14,290 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:14,299 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:14,301 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:14,301 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:14,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:14,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:14,307 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:14,308 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:14,308 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,312 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:14,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:14,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:14,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:14,315 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:14,315 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:14,316 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,317 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,317 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,318 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:14,318 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:14,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:14,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:14,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:14,323 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,323 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,331 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,338 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,346 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,353 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,353 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,353 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:14,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:14,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:14,358 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,359 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,378 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,384 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,391 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,391 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,391 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:14,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:14,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:14,397 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,397 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,412 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,420 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,428 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,428 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,428 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:14,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:14,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:14,434 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,434 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,442 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,475 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,476 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,476 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:14,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:14,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:14,481 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,482 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,496 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,510 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,511 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:14,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:14,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:14,517 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,517 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,525 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,539 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,546 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,546 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,546 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:14,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:14,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:14,552 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,552 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,560 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,567 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,581 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,581 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:14,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:14,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:14,587 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,587 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,595 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,602 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,609 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,616 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,616 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:14,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:14,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:14,622 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,630 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,658 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,658 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,659 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:14,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:14,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:14,664 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,672 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,692 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,693 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,693 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:14,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:14,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:14,699 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,699 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,733 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,741 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:14,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:14,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:14,747 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,747 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,755 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,763 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,770 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,778 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,778 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:14,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:14,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:14,786 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,786 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,797 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,805 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,814 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,821 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,821 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:14,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:14,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:14,829 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,829 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,838 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,847 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,854 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,865 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,865 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,865 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:14,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:14,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:14,873 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,873 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,901 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,908 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:14,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:14,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:14,914 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,915 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,930 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,937 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,945 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:14,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:14,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:14,951 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:14,951 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:14,970 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,977 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:14,993 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:14,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:14,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:14,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:14,999 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,000 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,008 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,016 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,024 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,031 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,031 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:15,031 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:15,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:15,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:15,037 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,038 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,052 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,067 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,067 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:15,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:15,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:15,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:15,074 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,074 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,083 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,090 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,098 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:51:15,107 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:51:15,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:15,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:15,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:15,110 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:15,110 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:15,111 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,112 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,114 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,114 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:15,114 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:15,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:15,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:15,116 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:15,116 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:15,128 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:15,137 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:15,146 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:15,155 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:15,157 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:15,157 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:15,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:15,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:15,164 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:15,165 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:15,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,168 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,168 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,169 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,169 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:15,169 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:15,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:15,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:15,172 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:15,172 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:15,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,174 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,174 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:15,175 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:15,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:15,176 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:15,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:15,180 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,180 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,195 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,203 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,211 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,211 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,211 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:15,213 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:15,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:15,217 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,218 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,245 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,266 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,266 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:15,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:15,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:15,272 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,272 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,280 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,300 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,301 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,301 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:15,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:15,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:15,306 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,307 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,315 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,322 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,329 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,335 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,336 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:15,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:15,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:15,341 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,341 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,350 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,357 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,371 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:15,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:15,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:15,376 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,377 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,384 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,392 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,406 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,406 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,406 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:15,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:15,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:15,412 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,412 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,420 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,427 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,443 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,443 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:15,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:15,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:15,449 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,449 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,467 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,475 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,491 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,491 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:15,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:15,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:15,497 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,497 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,505 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,512 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,519 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,526 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,526 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,526 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:15,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:15,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:15,532 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,532 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,539 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,546 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,553 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,562 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,563 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:15,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:15,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:15,568 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,569 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,576 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,583 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,590 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,596 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,596 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,597 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:15,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:15,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:15,602 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,602 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,610 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,664 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,682 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,690 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,690 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:15,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:15,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:15,696 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,696 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,705 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,712 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,729 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,736 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,737 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,737 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:15,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:15,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:15,742 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,743 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,750 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,757 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,764 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,771 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,771 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,771 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:15,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:15,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:15,777 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,777 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,792 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,798 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,805 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,805 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,805 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:15,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:15,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:15,811 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,811 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,819 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,826 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,833 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,840 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,840 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,841 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:15,842 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:15,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:15,846 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,846 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,854 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,861 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,869 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,876 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,876 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,876 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:15,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:15,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:15,882 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,882 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:15,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,903 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,933 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:15,953 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:15,954 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:15,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:15,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:15,959 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:15,959 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,016 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,165 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:16,165 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:16,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:16,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:16,169 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,169 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,209 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,216 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,223 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,230 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:51:16,230 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:51:16,230 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:16,231 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:16,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:16,233 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:16,233 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:16,234 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:16,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:16,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:16,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:16,238 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:16,239 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:16,250 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:16,260 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:16,273 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:16,282 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:16,284 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:16,284 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:16,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:16,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:16,291 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:16,291 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:16,292 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,295 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,296 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,296 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:16,296 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:16,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:16,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:16,299 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:16,299 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:16,299 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,300 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:16,302 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:16,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:16,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:16,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:16,306 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,306 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,314 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,321 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,328 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,335 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:16,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:16,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:16,341 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,341 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,494 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,551 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,552 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:16,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:16,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:16,559 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,559 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,575 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,588 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,589 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,589 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:16,590 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:16,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:16,594 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,595 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,602 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,609 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,619 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,626 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,626 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:16,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:16,630 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:16,632 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,632 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,640 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,647 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,654 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,660 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,661 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:16,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:16,664 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:16,666 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,666 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,698 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,699 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,699 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:16,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:16,702 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:16,704 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,705 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,725 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,742 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,748 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,755 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,755 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,755 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:16,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:16,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:16,760 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,761 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,769 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,875 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,876 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,876 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:16,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:16,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:16,881 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,882 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,900 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,922 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,922 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:16,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:16,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:16,928 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,928 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,936 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,942 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,955 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,955 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:16,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:16,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:16,961 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,962 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:16,969 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:16,992 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:16,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:16,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:16,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:16,997 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:16,998 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,021 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,028 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,029 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:17,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:17,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:17,034 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,035 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,051 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,064 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,074 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,075 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,075 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:17,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:17,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:17,080 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,080 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,089 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,123 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,152 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,153 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:17,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:17,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:17,158 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,159 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,167 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,189 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,189 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:17,191 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:17,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:17,195 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,195 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,203 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,209 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,225 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,225 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:17,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:17,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:17,230 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,231 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,238 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,245 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,253 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,260 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,260 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,260 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:17,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:17,264 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:17,266 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,266 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,275 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,282 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,288 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,298 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,299 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,299 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:17,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:17,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:17,308 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,308 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,330 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,337 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,343 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,350 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,350 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:17,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:17,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:17,354 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,355 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,383 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,389 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:51:17,390 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:51:17,390 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:17,391 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:17,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:17,393 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:17,393 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:17,395 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,398 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,401 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,404 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,405 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:17,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:17,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:17,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:17,408 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:17,408 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:17,423 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:17,433 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:17,443 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:17,452 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:17,454 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:17,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:17,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:17,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:17,460 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:17,461 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:17,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,464 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,464 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,465 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,465 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:17,465 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:17,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:17,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:17,468 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:17,468 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:17,469 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,471 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:17,471 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:17,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:17,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:17,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:17,475 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,475 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,528 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,543 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,549 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,556 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,556 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:17,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:17,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:17,561 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,561 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,569 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,575 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,587 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,588 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:17,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:17,591 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:17,593 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,593 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,601 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,633 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,642 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,642 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:17,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:17,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:17,647 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,647 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,654 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,660 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,666 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,672 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,672 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,673 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:17,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:17,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:17,677 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,678 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,697 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,704 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,704 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,704 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:17,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:17,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:17,709 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,709 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,717 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,725 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,736 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,737 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,737 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:17,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:17,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:17,742 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,742 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,749 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,762 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,769 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,769 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:17,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:17,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:17,774 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,774 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,795 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,801 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,801 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,801 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:17,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:17,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:17,806 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,806 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,818 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,826 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,831 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,837 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,838 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:17,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:17,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:17,843 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,843 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,862 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,868 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:17,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:17,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:17,873 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,873 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,887 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,898 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,899 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,899 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:17,900 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:17,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:17,904 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,904 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,912 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,952 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,953 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,953 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:17,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:17,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:17,958 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,958 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,966 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,972 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,978 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,984 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:17,984 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:17,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:17,985 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:17,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:17,990 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:17,990 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:17,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,004 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,010 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,016 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,017 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:18,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:18,018 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:18,020 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:18,022 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,022 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,029 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,036 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,042 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,048 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,048 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:18,048 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:18,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:18,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:18,053 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,053 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,061 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,083 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,089 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,095 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,095 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:18,095 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:18,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:18,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:18,101 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,101 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,132 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:18,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:18,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:18,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:18,137 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,138 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,158 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,165 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:18,165 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:18,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:18,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:18,170 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,171 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,197 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,198 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:18,198 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:18,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:18,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:18,201 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,202 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,210 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,216 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,229 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:51:18,229 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:51:18,230 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:18,231 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:18,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:18,232 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:18,232 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:18,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,234 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:18,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:18,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:18,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:18,238 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:18,238 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:18,249 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:18,258 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:18,268 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:18,278 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:18,280 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:18,280 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:18,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:18,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:18,286 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:51:18,287 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:18,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,288 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,288 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,289 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:18,289 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:51:18,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:51:18,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:18,292 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:18,292 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:18,293 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,295 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:18,295 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:18,295 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:51:18,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:51:18,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:18,299 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,299 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,345 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,351 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,351 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:51:18,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:51:18,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:18,356 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,357 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,365 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,377 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,383 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,383 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:51:18,384 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:51:18,386 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:18,388 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,388 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,400 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,406 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,412 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,418 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,418 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,418 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:51:18,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:51:18,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:18,424 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,431 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,443 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,449 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,449 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:51:18,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:51:18,453 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:18,455 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,455 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,462 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,474 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,480 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,480 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,480 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:51:18,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:51:18,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:18,485 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,485 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,492 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,511 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,511 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:51:18,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:51:18,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:18,516 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,517 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,525 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,531 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,540 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,546 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,546 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,547 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:51:18,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:51:18,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:18,552 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,552 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,560 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,566 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,572 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,578 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,578 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,578 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:51:18,580 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:51:18,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:18,583 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,584 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,606 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,614 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,620 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,626 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,626 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:51:18,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:51:18,629 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:18,631 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,631 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,639 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,644 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,650 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,657 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:51:18,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:51:18,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:18,665 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,680 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,692 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,692 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,693 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:51:18,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:51:18,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:18,698 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,698 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,712 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,725 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,725 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,726 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:51:18,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:51:18,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:18,731 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,731 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,738 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,750 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,757 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,757 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,757 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:51:18,759 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:51:18,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:18,762 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,763 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,770 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,776 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,789 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,789 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:51:18,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:51:18,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:18,794 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,795 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,803 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,809 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,815 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,836 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,837 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,837 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:51:18,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:51:18,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:18,842 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,842 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,849 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,855 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,862 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,868 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,868 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:51:18,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:51:18,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:18,873 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,874 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,887 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,899 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,906 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,906 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:51:18,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:51:18,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:18,911 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,911 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,925 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,931 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,937 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,937 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,938 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:51:18,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:51:18,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:18,943 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,943 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,950 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,974 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,980 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,980 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:18,981 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:51:18,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:51:18,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:18,984 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:51:18,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:51:18,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:18,999 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:19,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:19,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:51:19,011 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:51:19,011 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:51:19,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:51:19,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:19,014 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:19,014 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:19,015 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:19,015 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:19,016 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:19,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:51:19,017 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:51:19,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:51:19,018 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:51:19,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:51:19,019 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:51:19,019 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:51:19,030 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:19,040 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:19,050 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:19,060 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:51:19,062 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:51:19,062 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:51:19,069 [test.py:40 in test_hf_gen] INFO - for i in range(10): 
        print(i)
    print("\n")
    print("\n")
    print("\n")
    print
2023-10-12 02:51:19,069 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,069 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man?
2023-10-12 02:51:19,069 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,069 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?#!/usr/bin/env python

# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed
2023-10-12 02:51:19,069 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,069 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Examples for the
2023-10-12 02:51:19,069 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,069 [test.py:40 in test_hf_gen] INFO - for i in range(10): 
        print(i)
    print("\n")
    print("\n")
    print("\n")
    print
2023-10-12 02:51:19,069 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,069 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man?
2023-10-12 02:51:19,069 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,070 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?#!/usr/bin/env python

# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed
2023-10-12 02:51:19,070 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,070 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Examples for the
2023-10-12 02:51:19,070 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:51:19,082 [forward.py:23 in reset_forward] DEBUG - transformer.wte from flexgen to old.
2023-10-12 02:51:19,082 [forward.py:23 in reset_forward] DEBUG - transformer.drop from flexgen to old.
2023-10-12 02:51:19,082 [forward.py:23 in reset_forward] DEBUG - transformer.h.0 from flexgen to old.
2023-10-12 02:51:19,082 [forward.py:23 in reset_forward] DEBUG - transformer.h.1 from flexgen to old.
2023-10-12 02:51:19,082 [forward.py:23 in reset_forward] DEBUG - transformer.h.2 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.3 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.4 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.5 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.6 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.7 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.8 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.9 from flexgen to old.
2023-10-12 02:51:19,083 [forward.py:23 in reset_forward] DEBUG - transformer.h.10 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.11 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.12 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.13 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.14 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.15 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.16 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.17 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.18 from flexgen to old.
2023-10-12 02:51:19,084 [forward.py:23 in reset_forward] DEBUG - transformer.h.19 from flexgen to old.
2023-10-12 02:51:19,085 [forward.py:23 in reset_forward] DEBUG - transformer.ln_f from flexgen to old.
2023-10-12 02:51:19,085 [forward.py:23 in reset_forward] DEBUG - lm_head from flexgen to old.
