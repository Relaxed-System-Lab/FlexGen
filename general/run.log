2023-10-29 10:27:09,114 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpcct5895l
2023-10-29 10:27:09,114 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpcct5895l/_remote_module_non_scriptable.py
2023-10-29 10:27:09,861 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-29 10:27:09,926 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-29 10:27:13,649 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-29 10:27:14,728 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-29 10:27:14,728 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-29 10:27:14,728 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-29 10:27:14,728 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-29 10:27:16,177 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-29 10:27:16,269 [model.py:111 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-29 10:27:16,269 [model.py:60 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-29 10:27:16,311 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-29 10:27:16,404 [model.py:68 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-29 10:27:16,412 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-29 10:27:16,413 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-29 10:27:16,415 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-29 10:27:16,416 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-29 10:27:16,417 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-29 10:27:16,418 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-29 10:27:16,418 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-29 10:27:16,419 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-29 10:27:16,420 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-29 10:27:16,421 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-29 10:27:16,422 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-29 10:27:16,423 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-29 10:27:16,424 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-29 10:27:16,425 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-29 10:27:16,426 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-29 10:27:16,426 [model.py:211 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-29 10:27:16,427 [model.py:215 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-29 10:27:16,429 [model.py:221 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-29 10:27:16,469 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-29 10:27:16,706 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-29 10:27:16,706 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-29 10:27:16,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-29 10:27:16,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-29 10:27:16,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-29 10:27:16,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-29 10:27:16,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-29 10:27:16,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-29 10:27:16,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-29 10:27:16,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-29 10:27:16,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-29 10:27:16,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-29 10:27:16,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-29 10:27:16,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-29 10:27:16,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-29 10:27:16,709 [model.py:298 in to_test_forward] DEBUG - lm_head to test forward
2023-10-29 10:27:16,755 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:16,768 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-29 10:27:16,784 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:16,794 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-29 10:27:16,794 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:16,922 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-29 10:27:16,926 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:16,991 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-29 10:27:16,995 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:17,055 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-29 10:27:17,058 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:17,116 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-29 10:27:17,120 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:17,191 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-29 10:27:17,195 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:17,255 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-29 10:27:17,258 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:17,325 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-29 10:27:17,328 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:17,392 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-29 10:27:17,395 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:17,464 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-29 10:27:17,467 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:17,532 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-29 10:27:17,535 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:17,598 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-29 10:27:17,601 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:17,662 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-29 10:27:17,665 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:17,667 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-29 10:27:17,667 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:17,810 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-29 10:27:17,824 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-29 10:27:17,825 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-29 10:27:17,826 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-29 10:27:17,826 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-29 10:27:17,826 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-29 10:27:17,826 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-29 10:27:17,826 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-29 10:27:17,826 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-29 10:27:17,826 [model.py:306 in reset_forward] DEBUG - lm_head from test to old.
2023-10-29 10:27:17,837 [model.py:400 in init_all_weights] DEBUG - init all weights...
2023-10-29 10:27:17,865 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-29 10:27:17,866 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-29 10:27:17,866 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-29 10:27:17,866 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-29 10:27:17,866 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-29 10:27:17,866 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-29 10:27:17,866 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-29 10:27:17,867 [flexgen.py:143 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-29 10:27:17,868 [flexgen.py:143 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-29 10:27:17,911 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-29 10:27:18,062 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:18,063 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:18,063 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])",)
2023-10-29 10:27:18,063 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,072 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,078 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,082 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,087 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,087 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-29 10:27:18,087 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:18,088 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:18,088 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:18,092 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])", "<class 'int'>: 0")
2023-10-29 10:27:18,092 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,093 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,094 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,094 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,095 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,095 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-29 10:27:18,095 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:18,099 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:18,102 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:18,106 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,106 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,119 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,126 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,132 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,137 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,137 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,137 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:18,140 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:18,143 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:18,147 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,147 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,154 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,158 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,162 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,166 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,166 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,166 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:18,169 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:18,172 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:18,176 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,176 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,180 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,184 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,188 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,191 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,191 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,191 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:18,194 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:18,197 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:18,201 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,201 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,212 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,230 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,234 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,238 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,238 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,239 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:18,241 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:18,245 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:18,249 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,249 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,255 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,259 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,264 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,268 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,268 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,268 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:18,271 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:18,275 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:18,279 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,279 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,285 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,290 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,294 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,304 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,304 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,304 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:18,306 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:18,310 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:18,313 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,314 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,319 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,323 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,326 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,330 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,330 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,330 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:18,333 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:18,337 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:18,340 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,340 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,345 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,349 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,353 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,357 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,357 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,357 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:18,359 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:18,363 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:18,367 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,367 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,372 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,376 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,380 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,383 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,383 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,383 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:18,386 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:18,390 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:18,393 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,393 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,398 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,402 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,406 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,410 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,410 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,410 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:18,412 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:18,415 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:18,419 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,419 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,424 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,428 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,432 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,436 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,437 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,437 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:18,439 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:18,443 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:18,444 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,444 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,449 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,453 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,457 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,461 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-29 10:27:18,461 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-29 10:27:18,461 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:18,463 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:18,463 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:18,464 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,464 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,465 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,466 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,467 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,467 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-29 10:27:18,467 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-29 10:27:18,468 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:18,468 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:18,468 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:18,469 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-29 10:27:18,469 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,479 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-29 10:27:18,487 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-29 10:27:18,495 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-29 10:27:18,503 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-29 10:27:18,505 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 9, 50272])
2023-10-29 10:27:18,505 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:18,517 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:18,518 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:18,518 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:18,518 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,519 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,521 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,522 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,523 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,523 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:18,523 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:18,523 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:18,524 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:18,529 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 10])", "<class 'int'>: 9")
2023-10-29 10:27:18,529 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,530 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,530 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,531 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,532 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,532 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:18,532 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:18,535 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:18,538 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:18,543 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,543 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,554 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,558 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,561 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,565 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,565 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,565 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:18,567 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:18,570 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:18,574 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,574 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,579 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,583 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,586 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,589 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,590 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,590 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:18,591 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:18,595 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:18,599 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,600 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,604 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,608 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,611 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,615 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,615 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,615 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:18,617 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:18,620 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:18,625 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,625 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,629 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,633 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,636 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,640 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,640 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,640 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:18,642 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:18,645 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:18,650 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,650 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,656 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,659 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,664 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,667 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,667 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,667 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:18,668 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:18,672 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:18,676 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,676 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,680 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,684 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,687 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,691 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,691 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,691 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:18,692 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:18,695 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:18,700 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,700 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,705 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,709 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,712 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,715 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,716 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,716 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:18,717 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:18,721 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:18,725 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,725 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,730 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,733 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,737 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,740 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,740 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,741 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:18,742 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:18,745 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:18,750 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,750 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,754 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,758 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,761 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,765 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,765 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,766 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:18,767 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:18,771 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:18,775 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,775 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,779 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,792 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,826 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,855 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,855 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,855 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:18,857 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:18,860 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:18,864 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,865 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,869 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,873 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,876 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,879 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,880 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,880 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:18,881 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:18,885 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:18,886 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,886 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,890 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,894 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,897 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,900 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-29 10:27:18,900 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-29 10:27:18,901 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:18,902 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:18,903 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:18,903 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,903 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,904 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,905 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,905 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,906 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,906 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:18,906 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:18,907 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:18,907 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:18,908 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,908 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,916 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:18,923 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:18,930 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:18,936 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:18,938 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:18,938 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:18,948 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:18,949 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:18,949 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:18,950 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,951 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,952 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,952 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,962 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,962 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:18,962 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:18,963 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:18,964 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:18,970 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 11])", "<class 'int'>: 10")
2023-10-29 10:27:18,970 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:18,971 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,972 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,973 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,974 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:18,974 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:18,974 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:18,977 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:18,980 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:18,984 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:18,984 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:18,989 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:18,993 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:18,998 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,001 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,002 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,002 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:19,003 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:19,007 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:19,011 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,011 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,016 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,021 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,024 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,028 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,028 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,028 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:19,030 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:19,034 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:19,038 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,038 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,060 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,064 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,067 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,071 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,071 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,071 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:19,073 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:19,076 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:19,079 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,079 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,084 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,088 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,092 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,097 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,097 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,097 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:19,100 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:19,105 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:19,112 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,112 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,144 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,154 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,158 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,161 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,161 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,161 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:19,163 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:19,166 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:19,170 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,170 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,174 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,177 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,181 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,184 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,184 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,185 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:19,186 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:19,189 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:19,193 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,193 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,197 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,201 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,204 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,207 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,207 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,207 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:19,209 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:19,213 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:19,216 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,217 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,222 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,225 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,229 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,233 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,233 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,233 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:19,234 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:19,237 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:19,241 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,241 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,246 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,250 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,254 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,257 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,258 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,258 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:19,259 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:19,262 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:19,266 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,266 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,270 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,273 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,277 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,280 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,280 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,280 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:19,281 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:19,284 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:19,288 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,288 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,300 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,330 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,344 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,348 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,348 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,348 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:19,350 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:19,353 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:19,354 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,354 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,359 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,363 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,367 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,370 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-29 10:27:19,371 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-29 10:27:19,371 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:19,372 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:19,373 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:19,373 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,374 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,374 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,375 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,376 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,377 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,377 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:19,377 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:19,378 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:19,378 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:19,379 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,379 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,386 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,392 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,398 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,403 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,404 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:19,404 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:19,411 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:19,412 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:19,413 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:19,413 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,414 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,416 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,416 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,418 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,418 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:19,419 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:19,419 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:19,420 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:19,424 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 12])", "<class 'int'>: 11")
2023-10-29 10:27:19,424 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,425 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,425 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,426 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,427 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,427 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:19,427 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:19,430 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:19,434 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:19,438 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,438 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,446 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,450 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,454 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,458 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,458 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,458 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:19,460 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:19,464 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:19,468 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,468 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,473 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,477 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,480 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,484 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,485 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,485 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:19,487 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:19,490 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:19,495 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,495 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,500 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,504 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,508 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,511 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,511 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,512 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:19,513 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:19,517 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:19,521 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,522 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,526 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,530 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,534 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,538 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,538 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,538 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:19,540 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:19,544 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:19,549 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,549 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,554 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,559 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,563 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,567 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,567 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,567 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:19,569 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:19,573 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:19,577 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,578 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,607 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,617 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,620 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,624 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,624 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,624 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:19,626 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:19,631 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:19,637 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,637 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,643 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,647 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,650 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,653 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,654 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,654 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:19,655 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:19,659 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:19,663 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,663 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,667 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,671 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,675 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,680 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,680 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,680 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:19,681 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:19,685 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:19,689 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,690 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,694 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,698 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,701 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,706 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,706 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,706 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:19,708 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:19,711 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:19,715 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,715 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,720 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,723 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,727 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,731 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,731 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,731 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:19,732 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:19,736 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:19,740 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,741 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,745 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,750 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,753 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,756 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,756 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,757 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:19,758 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:19,762 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:19,762 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,762 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,767 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,771 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,774 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,798 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-29 10:27:19,798 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-29 10:27:19,798 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:19,799 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:19,800 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:19,800 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,800 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,801 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,802 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,803 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,804 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,804 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:19,804 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:19,804 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:19,805 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:19,805 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,805 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,813 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,820 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,826 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,832 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:19,833 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:19,833 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:19,838 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:19,839 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:19,839 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:19,839 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,840 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,841 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,841 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,842 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,842 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:19,842 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:19,843 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:19,843 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:19,847 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 13])", "<class 'int'>: 12")
2023-10-29 10:27:19,847 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:19,848 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,849 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,849 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,850 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:19,850 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:19,851 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:19,853 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:19,857 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:19,861 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,861 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,866 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,870 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,874 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,877 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,877 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:19,878 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:19,879 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:19,883 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:19,886 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,887 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,891 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,895 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,898 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,902 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,902 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:19,902 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:19,904 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:19,907 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:19,911 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,911 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,916 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,919 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,923 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,926 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,926 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:19,927 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:19,928 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:19,932 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:19,936 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,936 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,940 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,944 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,948 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,951 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,951 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:19,952 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:19,953 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:19,957 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:19,961 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,961 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,966 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,971 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,974 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,978 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,978 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:19,978 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:19,980 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:19,983 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:19,987 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:19,987 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:19,992 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,996 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:19,999 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,002 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,003 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:20,003 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:20,004 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:20,007 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:20,011 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,012 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,017 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,021 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,024 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,028 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,028 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:20,028 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:20,029 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:20,033 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:20,037 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,037 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,041 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,051 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,081 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,087 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,088 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:20,089 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:20,090 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:20,094 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:20,098 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,099 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,103 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,107 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,111 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,115 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,115 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:20,115 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:20,117 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:20,120 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:20,124 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,124 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,129 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,132 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,135 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,139 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,139 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:20,139 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:20,140 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:20,144 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:20,147 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,148 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,152 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,155 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,158 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,161 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,162 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:20,162 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:20,163 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:20,166 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:20,167 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,167 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,171 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,175 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,178 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,181 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-29 10:27:20,181 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-29 10:27:20,182 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:20,183 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:20,183 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:20,184 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,184 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,185 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,185 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,186 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,187 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,187 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:20,187 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:20,187 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:20,188 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:20,188 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,188 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,195 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,201 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,206 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,212 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,213 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:20,213 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:20,217 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:20,218 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:20,218 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:20,218 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,219 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,220 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,221 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,222 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,222 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:20,222 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:20,223 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:20,223 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:20,227 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 14])", "<class 'int'>: 13")
2023-10-29 10:27:20,227 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,228 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,228 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,229 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,230 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,230 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:20,230 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:20,233 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:20,236 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:20,240 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,240 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,245 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,248 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,252 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,255 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,255 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,255 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:20,257 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:20,260 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:20,263 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,264 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,268 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,271 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,275 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,278 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,278 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,278 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:20,280 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:20,283 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:20,286 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,286 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,309 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,312 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,316 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,320 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,320 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,320 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:20,322 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:20,326 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:20,329 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,330 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,334 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,338 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,341 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,345 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,345 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,345 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:20,347 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:20,351 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:20,355 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,355 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,359 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,363 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,367 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,370 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,371 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,371 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:20,372 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:20,376 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:20,380 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,380 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,385 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,389 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,392 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,396 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,397 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,397 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:20,398 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:20,402 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:20,406 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,406 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,411 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,415 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,418 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,421 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,421 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,422 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:20,423 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:20,427 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:20,431 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,431 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,436 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,439 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,443 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,447 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,447 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,447 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:20,448 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:20,452 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:20,456 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,456 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,460 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,464 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,467 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,472 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,472 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,472 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:20,473 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:20,477 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:20,481 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,481 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,486 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,489 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,493 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,496 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,496 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,497 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:20,498 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:20,502 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:20,506 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,506 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,511 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,514 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,518 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,521 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,522 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,522 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:20,523 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:20,527 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:20,527 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,528 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,532 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,536 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,540 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,543 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-29 10:27:20,543 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-29 10:27:20,544 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:20,545 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:20,545 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:20,546 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,546 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,547 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,548 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,548 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,549 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,550 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:20,550 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:20,550 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:20,551 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:20,551 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,552 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,564 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,574 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,585 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,596 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,605 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:20,605 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:20,610 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:20,610 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:20,611 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:20,611 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,612 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,612 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,613 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,613 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,614 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:20,614 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:20,614 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:20,615 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:20,618 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 15])", "<class 'int'>: 14")
2023-10-29 10:27:20,618 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,619 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,620 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,620 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,621 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,621 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:20,621 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:20,624 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:20,628 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:20,631 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,631 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,636 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,648 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,652 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,656 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,656 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,656 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:20,658 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:20,662 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:20,666 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,666 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,672 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,675 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,679 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,683 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,684 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,684 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:20,685 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:20,689 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:20,693 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,694 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,699 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,702 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,707 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,711 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,711 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,711 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:20,713 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:20,717 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:20,721 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,721 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,725 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,729 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,733 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,737 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,737 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,737 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:20,739 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:20,742 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:20,747 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,747 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,752 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,755 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,759 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,762 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,762 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,763 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:20,764 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:20,768 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:20,772 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,772 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,777 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,780 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,784 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,787 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,787 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,787 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:20,789 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:20,792 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:20,797 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,797 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,802 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,811 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,840 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,845 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,845 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,845 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:20,846 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:20,850 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:20,853 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,853 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,858 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,861 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,864 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,868 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,868 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,868 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:20,869 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:20,873 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:20,877 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,877 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,881 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,885 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,888 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,892 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,892 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,892 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:20,894 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:20,897 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:20,900 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,901 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,905 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,909 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,912 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,915 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,915 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,915 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:20,916 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:20,920 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:20,923 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,924 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,928 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,932 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,935 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,938 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,938 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,939 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:20,940 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:20,944 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:20,944 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,944 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:20,949 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,953 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,956 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,959 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-29 10:27:20,960 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-29 10:27:20,960 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:20,961 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:20,962 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:20,962 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,962 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,963 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,964 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,965 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,966 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:20,966 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:20,966 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:20,966 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:20,967 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:20,967 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:20,967 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:20,975 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,984 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:20,993 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:21,004 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:21,005 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:21,005 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:21,011 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:21,012 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:21,013 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:21,013 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:21,014 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,015 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,017 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,017 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,018 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:21,018 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:21,019 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:21,020 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:21,026 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 16])", "<class 'int'>: 15")
2023-10-29 10:27:21,026 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:21,027 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,028 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,028 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,029 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,029 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:21,029 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:21,033 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:21,036 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:21,040 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,040 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,051 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,054 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,057 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,061 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,061 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,061 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:21,062 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:21,066 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:21,069 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,070 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,074 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,078 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,081 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,084 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,084 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,085 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:21,086 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:21,089 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:21,092 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,093 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,121 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,148 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,157 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,161 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,161 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,161 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:21,164 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:21,167 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:21,171 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,171 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,176 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,180 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,184 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,187 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,188 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,188 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:21,190 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:21,193 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:21,196 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,197 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,201 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,204 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,207 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,210 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,211 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,211 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:21,212 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:21,215 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:21,218 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,219 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,223 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,226 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,229 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,233 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,233 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,233 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:21,235 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:21,238 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:21,242 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,242 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,246 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,250 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,253 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,256 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,256 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,256 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:21,257 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:21,260 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:21,264 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,264 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,268 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,273 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,276 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,291 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,292 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,292 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:21,293 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:21,297 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:21,301 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,301 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,312 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,319 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,322 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,325 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,326 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,326 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:21,327 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:21,331 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:21,334 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,334 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,338 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,342 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,345 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,348 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,348 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,349 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:21,350 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:21,353 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:21,356 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,357 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,361 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,364 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,367 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,371 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,371 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,371 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:21,372 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:21,376 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:21,376 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,376 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,381 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,384 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,387 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,391 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-29 10:27:21,391 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-29 10:27:21,391 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:21,392 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:21,393 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:21,393 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,393 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:21,394 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,395 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,395 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,396 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,396 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:21,396 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:21,397 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:21,397 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:21,397 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,397 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:21,407 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:21,413 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:21,419 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:21,425 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:21,425 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:21,426 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:21,430 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:21,431 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:21,431 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:21,431 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:21,432 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,432 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,433 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,433 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,434 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:21,434 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:21,434 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:21,435 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:21,438 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 17])", "<class 'int'>: 16")
2023-10-29 10:27:21,438 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:21,439 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,440 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,440 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,441 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:21,441 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:21,441 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:21,444 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:21,447 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:21,450 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,451 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,512 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,597 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,641 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,673 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,674 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,674 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:21,676 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:21,680 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:21,685 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,685 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,699 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,703 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,732 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,736 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,736 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,738 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:21,740 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:21,744 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:21,748 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,748 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,754 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,758 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,762 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,765 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,765 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,765 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:21,767 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:21,770 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:21,774 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,775 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,780 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,783 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,787 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,793 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,794 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,794 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:21,796 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:21,799 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:21,803 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,804 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,823 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,827 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,830 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,863 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,863 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,864 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:21,865 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:21,869 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:21,873 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,873 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,878 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,881 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,885 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,888 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,888 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,889 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:21,890 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:21,893 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:21,898 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,898 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,910 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,914 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,917 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,921 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,921 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,921 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:21,923 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:21,926 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:21,930 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,930 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,935 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,944 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,948 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,952 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,952 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,953 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:21,954 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:21,958 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:21,961 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,962 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,966 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,970 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,974 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,977 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,977 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:21,977 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:21,979 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:21,982 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:21,985 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:21,986 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:21,990 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:21,999 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,003 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,007 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,007 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:22,007 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:22,009 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:22,012 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:22,017 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,017 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,021 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,025 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,029 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,033 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,033 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:22,033 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:22,035 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:22,038 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:22,039 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,039 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,044 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,048 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,051 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,055 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-29 10:27:22,055 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-29 10:27:22,055 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:22,056 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:22,057 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:22,057 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,057 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,058 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,059 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,059 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,060 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,060 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:22,060 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:22,061 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:22,061 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:22,062 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,062 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,072 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,078 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,085 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,092 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,092 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:22,093 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:22,098 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:22,099 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:22,099 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:22,099 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,100 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,101 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,101 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,103 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,104 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:22,104 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:22,104 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:22,105 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:22,109 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 18])", "<class 'int'>: 17")
2023-10-29 10:27:22,109 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,110 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,110 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,111 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,112 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,112 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:22,112 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:22,115 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:22,119 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:22,122 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,123 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,133 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,137 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,140 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,144 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,144 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,144 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:22,146 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:22,149 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:22,152 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,152 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,157 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,161 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,164 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,168 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,168 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,168 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:22,170 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:22,173 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:22,177 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,177 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,183 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,187 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,191 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,196 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,196 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,196 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:22,198 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:22,201 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:22,205 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,205 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,211 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,214 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,218 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,222 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,222 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,222 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:22,224 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:22,227 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:22,232 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,232 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,236 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,240 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,243 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,247 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,247 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,247 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:22,249 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:22,252 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:22,256 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,256 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,261 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,264 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,271 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,305 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,306 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,306 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:22,307 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:22,311 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:22,316 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,316 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,369 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,426 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,439 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,477 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,477 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,478 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:22,479 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:22,483 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:22,487 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,487 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,493 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,496 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,499 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,503 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,503 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,503 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:22,505 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:22,509 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:22,513 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,514 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,519 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,523 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,527 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,531 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,531 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,531 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:22,534 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:22,537 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:22,541 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,542 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,547 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,550 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,553 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,557 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,557 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,557 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:22,558 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:22,562 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:22,566 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,566 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,571 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,575 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,578 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,582 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,582 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,582 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:22,584 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:22,587 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:22,588 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,588 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,592 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,596 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,600 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,603 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-29 10:27:22,603 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-29 10:27:22,603 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:22,605 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:22,605 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:22,606 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,606 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,606 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,607 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,608 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,609 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,609 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:22,609 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:22,609 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:22,610 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:22,610 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,610 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,617 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,623 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,629 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,635 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:22,647 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:22,648 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:22,654 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:22,655 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:22,656 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:22,656 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,657 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,657 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,658 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,660 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,660 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:22,660 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:22,661 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:22,661 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:22,665 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 19])", "<class 'int'>: 18")
2023-10-29 10:27:22,665 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:22,666 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,667 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,667 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,668 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:22,668 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:22,668 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:22,671 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:22,675 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:22,679 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,679 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,703 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,706 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,710 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,713 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,713 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,713 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:22,715 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:22,719 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:22,723 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,723 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,727 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,731 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,740 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,744 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,744 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,744 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:22,746 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:22,749 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:22,753 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,754 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,758 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,762 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,765 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,769 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,769 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,769 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:22,771 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:22,774 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:22,778 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,779 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,783 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,787 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,790 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,793 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,793 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,793 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:22,795 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:22,799 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:22,803 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,803 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,808 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,811 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,814 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,818 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,818 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,818 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:22,820 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:22,823 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:22,827 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,827 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,832 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,835 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,838 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,842 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,842 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,842 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:22,843 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:22,847 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:22,851 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,851 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,856 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,860 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,863 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,866 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,867 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,867 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:22,868 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:22,872 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:22,876 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,876 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,881 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,884 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,888 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,891 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,891 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,891 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:22,893 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:22,896 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:22,901 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,901 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,905 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,909 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,912 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,916 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,916 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,916 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:22,918 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:22,921 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:22,925 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,925 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,930 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,934 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,942 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,959 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,960 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,960 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:22,961 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:22,964 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:22,969 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,969 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,973 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,977 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,980 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,984 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:22,984 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:22,984 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:22,986 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:22,990 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:22,991 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:22,991 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:22,995 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:23,003 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:23,007 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:23,010 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-29 10:27:23,011 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-29 10:27:23,011 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:23,012 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:23,013 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:23,013 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,013 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,014 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,015 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,015 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,016 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,016 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,016 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:23,017 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:23,017 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:23,017 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,018 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,026 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,034 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,041 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,048 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,049 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:23,049 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:23,054 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:23,055 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:23,055 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:23,055 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,056 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,056 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,057 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,058 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,058 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,058 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:23,059 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:23,059 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:23,063 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 20])", "<class 'int'>: 19")
2023-10-29 10:27:23,063 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,064 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,065 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,065 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,066 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,066 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,066 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:23,069 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:23,073 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:23,077 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,077 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,082 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,086 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,089 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,093 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,093 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,093 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:23,094 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:23,098 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:23,102 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,102 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,106 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,111 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,114 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,117 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,118 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,118 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:23,119 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:23,123 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:23,127 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,127 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,134 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,137 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,141 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,144 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,144 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,144 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:23,146 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:23,149 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:23,153 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,154 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,158 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,162 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,165 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,169 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,169 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,169 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:23,171 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:23,174 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:23,178 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,179 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,183 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,187 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,191 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,194 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,194 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,194 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:23,196 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:23,199 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:23,203 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,204 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,208 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,212 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,215 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,219 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,219 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,219 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:23,220 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:23,224 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:23,228 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,228 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,233 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,254 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,258 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,261 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,261 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,262 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:23,263 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:23,267 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:23,271 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,271 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,276 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,279 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,283 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,286 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,287 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,287 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:23,288 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:23,292 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:23,296 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,296 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,301 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,304 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,308 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,312 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,312 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,312 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:23,314 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:23,318 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:23,321 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,322 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,326 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,330 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,333 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,337 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,337 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,337 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:23,339 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:23,342 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:23,347 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,347 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,352 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,356 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,360 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,364 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,364 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,364 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:23,366 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:23,370 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:23,370 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,370 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,375 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,379 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,383 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,387 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-29 10:27:23,387 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-29 10:27:23,387 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:23,388 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:23,389 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:23,389 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,390 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,390 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,391 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,392 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,393 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,393 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,393 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:23,393 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:23,394 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:23,394 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,394 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,401 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,407 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,414 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,420 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,421 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:23,421 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:23,427 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:23,427 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:23,428 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:23,428 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,429 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,429 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,430 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,430 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,431 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,431 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:23,431 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:23,432 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:23,435 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 21])", "<class 'int'>: 20")
2023-10-29 10:27:23,436 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,436 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,437 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,438 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,438 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,438 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,439 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:23,441 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:23,445 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:23,449 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,449 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,454 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,458 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,461 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,465 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,465 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,465 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:23,467 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:23,470 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:23,474 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,475 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,480 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,483 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,487 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,490 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,491 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,491 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:23,492 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:23,496 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:23,500 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,501 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,527 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,535 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,539 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,542 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,543 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,543 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:23,544 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:23,548 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:23,552 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,553 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,557 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,561 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,564 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,568 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,568 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,568 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:23,570 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:23,574 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:23,578 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,578 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,583 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,586 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,590 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,595 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,595 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,595 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:23,597 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:23,600 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:23,604 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,604 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,609 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,613 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,616 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,620 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,620 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,620 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:23,621 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:23,625 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:23,629 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,629 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,634 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,637 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,641 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,649 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,650 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,650 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:23,651 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:23,655 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:23,659 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,659 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,664 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,668 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,671 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,675 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,675 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,675 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:23,676 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:23,680 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:23,684 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,684 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,689 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,693 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,696 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,701 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,701 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,701 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:23,702 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:23,706 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:23,710 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,710 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,715 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,719 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,722 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,726 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,726 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,726 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:23,727 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:23,731 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:23,735 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,735 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,740 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,745 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,773 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,776 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,776 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,777 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:23,778 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:23,782 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:23,782 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,782 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,787 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,791 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,794 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,798 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-29 10:27:23,798 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-29 10:27:23,798 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:23,799 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:23,800 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:23,800 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,800 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,801 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,802 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,802 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,803 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,803 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,803 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:23,804 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:23,804 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:23,805 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,805 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,813 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,819 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,826 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,833 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:23,833 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:23,834 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:23,839 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:23,839 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:23,840 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:23,840 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,841 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,842 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,842 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,843 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,843 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,843 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:23,843 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:23,844 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:23,848 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 22])", "<class 'int'>: 21")
2023-10-29 10:27:23,848 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:23,849 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,849 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,850 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,851 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:23,851 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:23,851 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:23,854 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:23,857 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:23,861 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,861 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,866 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,870 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,874 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,878 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,878 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:23,878 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:23,879 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:23,883 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:23,886 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,887 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,891 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,895 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,899 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,902 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,902 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:23,902 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:23,904 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:23,907 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:23,911 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,911 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,915 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,919 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,923 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,926 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,926 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:23,926 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:23,928 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:23,931 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:23,935 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,935 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,940 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,944 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,947 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,951 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,951 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:23,951 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:23,953 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:23,956 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:23,960 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,960 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,965 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,969 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,973 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,976 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,976 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:23,976 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:23,978 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:23,981 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:23,985 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:23,985 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:23,989 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,993 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:23,997 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,000 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,000 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:24,001 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:24,002 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:24,005 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:24,009 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,009 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,021 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,049 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,062 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,065 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,065 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:24,065 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:24,067 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:24,070 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:24,074 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,074 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,079 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,083 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,087 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,090 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,090 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:24,091 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:24,092 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:24,095 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:24,100 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,100 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,105 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,108 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,112 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,116 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,117 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:24,117 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:24,119 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:24,122 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:24,126 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,126 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,131 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,135 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,139 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,142 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,143 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:24,143 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:24,144 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:24,147 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:24,152 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,152 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,157 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,161 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,165 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,168 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,169 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:24,169 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:24,170 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:24,174 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:24,174 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,175 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,179 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,184 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,187 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,191 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-29 10:27:24,191 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-29 10:27:24,192 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:24,193 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:24,193 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:24,194 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,194 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,195 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,196 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,196 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,197 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,197 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:24,197 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:24,198 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:24,198 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:24,199 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,199 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,206 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,213 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,219 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,225 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,226 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:24,227 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:24,232 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:24,232 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:24,233 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:24,233 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,234 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,234 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,235 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,235 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,236 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:24,236 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:24,236 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:24,237 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:24,241 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 23])", "<class 'int'>: 22")
2023-10-29 10:27:24,241 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,242 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,242 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,243 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,244 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,244 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:24,244 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:24,247 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:24,251 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:24,255 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,255 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,263 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,267 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,271 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,274 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,274 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,274 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:24,276 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:24,279 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:24,283 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,283 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,288 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,292 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,296 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,300 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,300 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,300 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:24,302 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:24,305 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:24,309 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,309 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,314 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,325 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,329 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,332 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,332 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,333 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:24,334 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:24,337 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:24,340 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,340 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,345 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,349 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,353 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,356 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,356 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,356 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:24,358 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:24,361 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:24,364 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,364 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,369 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,372 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,375 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,378 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,378 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,379 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:24,380 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:24,383 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:24,386 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,386 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,391 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,394 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,398 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,423 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,424 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,424 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:24,425 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:24,429 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:24,433 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,433 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,438 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,452 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,483 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,488 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,488 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,488 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:24,490 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:24,493 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:24,497 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,497 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,502 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,532 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,544 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,548 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,548 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,548 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:24,550 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:24,554 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:24,559 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,559 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,564 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,569 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,573 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,577 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,578 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,578 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:24,579 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:24,582 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:24,586 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,586 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,592 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,596 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,600 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,603 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,604 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,604 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:24,605 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:24,608 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:24,612 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,612 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,617 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,621 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,624 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,628 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,628 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,628 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:24,629 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:24,633 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:24,633 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,633 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,638 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,642 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,646 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,653 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-29 10:27:24,653 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-29 10:27:24,653 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:24,654 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:24,655 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:24,655 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,655 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,656 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,657 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,658 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,658 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,658 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:24,659 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:24,659 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:24,659 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:24,660 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,660 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,667 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,673 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,679 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,686 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:24,687 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:24,687 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:24,691 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:24,692 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:24,692 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:24,692 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,693 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,694 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,694 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,695 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,695 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:24,695 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:24,696 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:24,696 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:24,700 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 24])", "<class 'int'>: 23")
2023-10-29 10:27:24,700 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:24,701 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,701 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,702 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,702 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:24,703 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:24,703 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:24,706 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:24,709 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:24,712 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,712 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,717 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,721 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,724 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,727 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,727 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,728 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:24,729 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:24,733 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:24,736 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,736 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,741 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,745 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,748 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,751 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,751 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,751 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:24,752 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:24,755 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:24,759 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,759 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,769 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,782 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,786 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,789 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,790 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,790 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:24,791 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:24,795 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:24,798 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,799 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,803 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,807 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,810 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,814 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,814 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,814 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:24,815 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:24,819 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:24,822 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,822 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,827 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,831 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,834 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,838 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,838 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,838 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:24,839 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:24,843 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:24,846 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,846 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,851 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,855 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,858 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,862 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,862 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,862 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:24,863 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:24,867 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:24,870 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,871 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,876 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,880 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,883 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,886 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,887 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,887 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:24,888 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:24,892 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:24,895 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,895 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,900 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,903 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,907 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,916 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,917 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:24,917 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:24,918 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:24,921 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:24,925 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:24,925 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:24,930 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,933 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:24,937 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,144 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,144 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:25,145 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:25,146 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:25,149 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:25,152 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,153 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,157 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,161 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,164 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,167 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,167 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:25,167 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:25,169 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:25,172 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:25,175 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,176 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,180 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,184 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,187 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,191 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,191 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:25,191 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:25,192 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:25,196 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:25,196 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,196 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,201 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,205 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,208 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,211 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-29 10:27:25,212 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-29 10:27:25,212 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:25,213 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:25,213 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:25,214 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,214 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,215 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,215 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,216 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,217 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,217 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:25,217 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:25,218 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:25,218 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:25,218 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,219 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,228 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,234 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,240 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,249 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,260 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:25,260 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:25,266 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:25,267 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:25,268 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:25,268 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,269 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,270 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,270 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,271 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,271 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:25,271 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:25,272 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:25,272 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:25,276 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 25])", "<class 'int'>: 24")
2023-10-29 10:27:25,276 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,277 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,278 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,278 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,279 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,279 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:25,279 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:25,282 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:25,286 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:25,290 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,290 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,295 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,299 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,303 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,306 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,307 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,307 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:25,308 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:25,312 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:25,315 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,316 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,321 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,324 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,328 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,332 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,332 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,332 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:25,333 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:25,338 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:25,345 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,345 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,350 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,355 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,361 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,366 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,366 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,367 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:25,369 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:25,375 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:25,382 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,382 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,388 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,394 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,399 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,403 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,403 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,403 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:25,405 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:25,408 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:25,412 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,412 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,418 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,422 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,425 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,430 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,430 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,430 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:25,432 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:25,436 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:25,439 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,440 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,445 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,449 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,452 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,456 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,456 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,456 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:25,458 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:25,461 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:25,466 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,466 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,471 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,476 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,480 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,484 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,484 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,484 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:25,486 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:25,490 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:25,494 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,494 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,499 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,503 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,506 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,510 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,510 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,510 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:25,512 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:25,515 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:25,519 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,520 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,548 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,552 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,556 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,559 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,560 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,560 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:25,561 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:25,565 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:25,569 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,569 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,578 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,582 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,585 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,589 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,589 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,589 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:25,590 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:25,594 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:25,599 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,599 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,604 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,607 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,611 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,614 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,614 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,615 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:25,616 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:25,620 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:25,620 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,621 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,625 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,629 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,633 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,637 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-29 10:27:25,637 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-29 10:27:25,637 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:25,638 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:25,639 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:25,639 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,639 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,640 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,641 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,642 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,643 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,643 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:25,643 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:25,644 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:25,644 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:25,644 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,645 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,652 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,659 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,665 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,672 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:25,673 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:25,673 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:25,678 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:25,679 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:25,679 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:25,679 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,680 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,680 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,681 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,682 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,682 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:25,682 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:25,682 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:25,683 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:25,687 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 26])", "<class 'int'>: 25")
2023-10-29 10:27:25,687 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:25,688 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,688 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,689 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,690 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:25,690 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:25,690 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:25,693 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:25,696 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:25,700 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,700 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,705 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,709 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,712 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,716 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,716 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,716 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:25,717 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:25,721 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:25,725 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,725 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,730 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,734 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,737 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,740 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,741 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,741 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:25,742 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:25,746 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:25,749 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,750 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,754 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,758 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,761 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,765 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,765 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,765 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:25,766 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:25,770 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:25,773 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,774 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,778 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,782 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,785 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,789 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,789 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,789 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:25,791 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:25,794 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:25,798 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,798 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,803 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,807 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,835 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,843 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,843 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,844 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:25,845 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:25,849 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:25,852 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,852 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,857 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,861 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,865 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,868 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,869 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,869 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:25,870 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:25,873 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:25,877 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,877 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,882 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,886 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,889 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,893 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,894 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,894 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:25,895 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:25,899 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:25,902 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,902 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,907 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,911 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,914 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,917 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,918 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,918 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:25,919 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:25,922 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:25,926 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,926 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,931 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,935 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,938 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,941 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,942 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,942 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:25,943 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:25,947 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:25,951 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,951 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,956 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,959 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,963 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,966 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,966 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,966 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:25,967 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:25,971 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:25,975 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:25,975 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:25,984 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,987 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,990 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,994 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:25,994 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:25,994 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:25,996 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:25,999 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:26,000 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,000 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,004 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:26,008 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:26,012 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:26,015 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-29 10:27:26,015 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-29 10:27:26,015 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:26,016 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:26,017 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:26,017 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,017 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,018 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,019 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,020 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,020 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,020 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,021 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:26,021 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:26,021 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:26,022 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,022 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,031 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,038 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,044 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,050 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,051 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:26,051 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:26,057 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:26,057 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:26,058 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:26,058 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,059 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,059 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,060 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,060 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,060 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,061 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:26,061 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:26,061 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:26,065 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 27])", "<class 'int'>: 26")
2023-10-29 10:27:26,065 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,066 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,067 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,067 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,068 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,068 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,068 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:26,071 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:26,075 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:26,079 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,079 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,110 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,116 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,119 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,125 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,125 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,126 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:26,127 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:26,131 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:26,134 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,135 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,143 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,148 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,152 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,155 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,155 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,155 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:26,157 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:26,160 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:26,164 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,164 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,169 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,173 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,177 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,180 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,181 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,181 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:26,182 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:26,186 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:26,189 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,189 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,194 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,198 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,201 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,205 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,205 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,205 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:26,207 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:26,210 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:26,214 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,214 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,219 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,223 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,226 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,230 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,230 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,230 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:26,232 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:26,235 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:26,239 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,239 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,244 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,248 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,251 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,255 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,255 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,255 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:26,256 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:26,259 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:26,263 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,263 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,268 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,272 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,276 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,281 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,281 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,281 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:26,282 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:26,286 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:26,289 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,290 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,295 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,298 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,302 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,306 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,306 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,306 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:26,307 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:26,311 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:26,314 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,315 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,321 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,342 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,347 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,351 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,351 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,351 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:26,353 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:26,356 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:26,360 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,360 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,365 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,369 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,372 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,376 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,376 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,376 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:26,377 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:26,381 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:26,385 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,385 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,390 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,394 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,397 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,401 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,401 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,401 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:26,402 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:26,406 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:26,406 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,406 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,412 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,416 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,419 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,423 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-29 10:27:26,423 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-29 10:27:26,423 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:26,424 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:26,425 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:26,425 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,426 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,426 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,427 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,428 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,429 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,429 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,429 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:26,429 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:26,430 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:26,430 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,430 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,438 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,444 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,451 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,457 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,458 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:26,458 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:26,463 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:26,464 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:26,464 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:26,464 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,465 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,465 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,466 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,467 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,467 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,467 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:26,467 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:26,468 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:26,472 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 28])", "<class 'int'>: 27")
2023-10-29 10:27:26,472 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,472 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,473 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,474 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,475 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,475 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,475 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:26,478 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:26,481 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:26,485 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,485 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,490 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,493 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,497 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,500 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,501 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,501 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:26,502 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:26,506 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:26,509 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,509 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,514 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,518 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,522 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,528 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,528 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,528 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:26,530 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:26,533 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:26,537 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,537 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,543 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,547 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,550 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,554 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,554 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,554 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:26,556 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:26,559 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:26,562 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,563 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,567 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,573 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,586 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,615 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,615 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,615 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:26,617 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:26,620 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:26,624 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,624 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,630 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,634 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,637 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,640 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,641 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,641 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:26,642 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:26,645 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:26,649 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,649 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,654 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,658 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,661 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,665 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,666 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,666 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:26,667 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:26,670 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:26,674 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,675 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,679 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,683 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,686 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,691 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,691 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,691 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:26,692 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:26,696 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:26,700 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,700 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,720 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,725 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,729 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,732 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,733 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,733 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:26,735 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:26,738 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:26,742 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,743 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,747 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,752 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,757 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,760 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,761 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,761 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:26,762 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:26,766 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:26,769 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,769 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,774 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,778 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,782 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,786 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,786 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,786 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:26,787 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:26,791 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:26,794 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,795 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,799 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,803 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,807 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,820 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,820 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,820 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:26,822 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:26,825 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:26,826 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,826 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,838 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,842 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,845 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,849 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-29 10:27:26,849 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-29 10:27:26,849 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:26,850 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:26,851 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:26,851 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,851 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,852 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,853 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,853 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,854 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,854 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,854 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:26,855 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:26,855 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:26,856 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,856 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,867 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,873 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,879 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,886 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:26,887 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:26,887 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:26,892 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:26,893 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:26,893 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:26,893 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,894 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,895 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,895 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,896 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,896 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,896 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:26,897 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:26,897 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:26,901 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 29])", "<class 'int'>: 28")
2023-10-29 10:27:26,902 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:26,902 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,903 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,904 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,904 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:26,904 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:26,905 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:26,907 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:26,911 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:26,915 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,915 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,920 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,924 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,927 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,931 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,931 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:26,931 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:26,933 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:26,936 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:26,940 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,940 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,945 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,949 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,952 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,956 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,956 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:26,956 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:26,958 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:26,961 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:26,965 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,965 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,970 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,973 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,977 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,980 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,981 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:26,981 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:26,982 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:26,986 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:26,989 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:26,990 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:26,995 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:26,999 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,002 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,006 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,006 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,006 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:27,008 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:27,011 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:27,015 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,015 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,020 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,024 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,032 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,036 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,036 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,036 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:27,038 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:27,041 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:27,045 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,045 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,050 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,054 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,058 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,062 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,062 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,062 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:27,063 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:27,066 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:27,070 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,071 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,076 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,080 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,084 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,113 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,113 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,113 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:27,115 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:27,118 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:27,122 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,123 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,128 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,131 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,135 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,138 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,139 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,139 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:27,140 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:27,143 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:27,147 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,147 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,153 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,156 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,160 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,163 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,163 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,164 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:27,165 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:27,168 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:27,172 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,172 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,177 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,181 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,184 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,188 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,188 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,188 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:27,189 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:27,192 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:27,196 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,196 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,201 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,205 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,208 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,212 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,213 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,213 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:27,214 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:27,218 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:27,218 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,219 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,223 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,227 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,231 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,234 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-29 10:27:27,235 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-29 10:27:27,235 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:27,236 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:27,236 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:27,237 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,237 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,238 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,238 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,239 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,240 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,240 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:27,240 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:27,241 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:27,241 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:27,241 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,241 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,252 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,258 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,264 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,271 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,272 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:27,272 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:27,277 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:27,278 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:27,278 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:27,278 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,279 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,280 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,280 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,281 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,281 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:27,281 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:27,282 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:27,282 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:27,286 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 30])", "<class 'int'>: 29")
2023-10-29 10:27:27,286 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,287 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,287 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,288 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,289 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,289 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:27,289 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:27,292 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:27,295 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:27,299 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,299 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,304 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,314 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,327 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,331 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,331 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,331 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:27,333 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:27,336 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:27,340 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,340 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,345 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,349 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,352 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,355 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,356 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,356 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:27,357 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:27,360 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:27,364 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,364 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,368 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,372 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,375 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,379 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,379 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,379 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:27,380 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:27,383 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:27,387 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,387 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,392 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,396 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,399 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,403 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,403 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,403 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:27,404 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:27,407 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:27,411 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,411 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,416 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,420 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,423 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,427 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,427 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,427 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:27,429 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:27,432 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:27,436 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,436 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,441 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,445 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,451 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,455 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,455 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,455 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:27,457 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:27,460 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:27,464 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,464 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,469 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,474 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,478 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,482 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,482 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,482 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:27,483 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:27,486 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:27,490 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,490 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,494 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,499 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,502 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,506 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,506 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,506 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:27,507 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:27,510 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:27,514 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,514 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,519 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,523 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,527 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,530 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,530 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,531 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:27,532 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:27,535 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:27,539 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,539 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,543 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,547 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,551 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,554 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,555 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,555 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:27,556 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:27,559 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:27,563 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,563 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,568 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,572 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,600 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,606 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,607 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,607 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:27,608 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:27,612 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:27,612 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,613 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,617 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,622 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,625 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,629 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-29 10:27:27,629 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-29 10:27:27,629 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:27,630 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:27,631 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:27,631 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,631 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,632 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,633 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,633 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,634 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,634 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:27,635 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:27,635 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:27,635 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:27,636 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,636 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,644 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,650 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,656 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,663 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:27,663 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:27,664 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:27,668 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:27,669 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:27,670 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:27,670 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,670 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,671 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,671 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,672 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,672 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:27,672 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:27,673 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:27,673 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:27,677 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 31])", "<class 'int'>: 30")
2023-10-29 10:27:27,677 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:27,678 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,679 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,679 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,680 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:27,680 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:27,680 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:27,683 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:27,687 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:27,690 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,691 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,695 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,699 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,703 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,706 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,706 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,707 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:27,708 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:27,711 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:27,715 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,715 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,720 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,724 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,728 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,736 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,736 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,736 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:27,738 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:27,741 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:27,745 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,745 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,751 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,755 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,758 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,763 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,763 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,763 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:27,765 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:27,768 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:27,772 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,772 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,776 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,780 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,784 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,788 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,788 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,788 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:27,789 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:27,793 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:27,797 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,797 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,809 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,834 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,838 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,843 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,843 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,843 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:27,845 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:27,849 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:27,853 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,853 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,858 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,863 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,867 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,870 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,871 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,871 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:27,872 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:27,876 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:27,880 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,880 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,885 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,890 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,896 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,900 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,900 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,900 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:27,902 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:27,906 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:27,910 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,911 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,916 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,920 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,923 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,928 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,928 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,928 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:27,929 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:27,933 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:27,937 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,937 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,942 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,946 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,950 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,954 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,954 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,955 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:27,956 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:27,960 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:27,964 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,964 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,970 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,974 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,978 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,982 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:27,982 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:27,982 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:27,983 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:27,987 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:27,991 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:27,991 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:27,997 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,001 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,006 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,010 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,010 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:28,010 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:28,012 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:28,016 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:28,016 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,016 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,024 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,028 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,034 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,039 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-29 10:27:28,039 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-29 10:27:28,039 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:28,040 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:28,041 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:28,041 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,042 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,042 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,043 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,044 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,045 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,045 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,045 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:28,046 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:28,046 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:28,047 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,047 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,054 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,061 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,067 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,076 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,089 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:28,089 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:28,098 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:28,099 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:28,099 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:28,099 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,100 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,101 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,101 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,102 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,102 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,102 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:28,103 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:28,103 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:28,107 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 32])", "<class 'int'>: 31")
2023-10-29 10:27:28,108 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,108 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,109 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,109 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,110 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,110 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,111 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:28,113 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:28,117 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:28,121 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,121 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,127 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,131 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,135 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,138 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,138 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,139 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:28,140 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:28,144 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:28,147 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,148 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,153 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,157 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,168 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,172 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,172 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,172 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:28,173 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:28,177 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:28,180 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,180 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,185 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,189 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,193 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,197 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,197 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,197 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:28,199 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:28,202 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:28,206 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,206 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,211 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,215 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,218 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,222 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,222 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,222 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:28,224 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:28,227 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:28,231 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,231 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,241 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,245 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,249 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,253 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,253 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,253 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:28,255 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:28,258 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:28,262 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,262 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,266 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,270 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,274 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,278 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,278 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,278 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:28,279 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:28,283 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:28,286 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,287 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,291 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,295 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,299 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,302 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,303 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,303 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:28,304 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:28,307 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:28,311 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,311 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,316 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,334 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,337 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,342 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,342 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,342 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:28,343 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:28,347 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:28,351 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,351 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,356 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,360 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,364 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,367 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,368 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,368 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:28,369 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:28,373 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:28,377 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,377 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,382 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,386 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,389 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,393 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,393 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,393 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:28,394 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:28,398 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:28,402 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,402 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,407 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,411 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,414 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,418 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,419 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,419 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:28,420 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:28,424 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:28,424 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,424 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,434 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,438 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,442 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,445 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-29 10:27:28,445 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-29 10:27:28,446 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:28,447 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:28,447 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:28,448 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,448 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,449 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,449 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,450 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,451 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,451 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,451 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:28,452 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:28,452 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:28,452 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,453 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,462 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,468 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,475 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,482 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,483 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:28,483 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:28,489 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:28,489 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:28,490 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:28,490 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,491 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,492 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,493 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,493 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,494 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,494 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:28,495 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:28,495 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:28,499 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 33])", "<class 'int'>: 32")
2023-10-29 10:27:28,499 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,500 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,501 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,502 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,502 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,503 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,503 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:28,506 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:28,509 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:28,513 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,513 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,518 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,522 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,526 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,530 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,530 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,530 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:28,532 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:28,535 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:28,539 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,539 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,545 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,549 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,554 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,558 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,558 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,558 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:28,559 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:28,563 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:28,567 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,567 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,572 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,576 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,607 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,627 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,627 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,628 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:28,629 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:28,632 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:28,636 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,636 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,641 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,646 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,649 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,653 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,654 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,654 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:28,655 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:28,659 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:28,663 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,663 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,668 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,672 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,677 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,680 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,680 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,680 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:28,682 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:28,685 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:28,689 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,689 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,694 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,698 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,702 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,705 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,706 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,706 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:28,707 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:28,710 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:28,715 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,715 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,719 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,723 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,727 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,734 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,734 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,734 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:28,738 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:28,741 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:28,745 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,746 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,751 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,755 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,760 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,764 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,764 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,765 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:28,766 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:28,769 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:28,774 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,774 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,779 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,783 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,786 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,790 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,790 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,791 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:28,792 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:28,795 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:28,799 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,800 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,805 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,809 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,813 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,816 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,817 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,817 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:28,818 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:28,821 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:28,825 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,826 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,843 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,847 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,851 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,854 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,854 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,855 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:28,856 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:28,860 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:28,860 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,860 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,865 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,869 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,873 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,877 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-29 10:27:28,877 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-29 10:27:28,877 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:28,878 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:28,879 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:28,879 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,879 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,880 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,881 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,882 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,882 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,883 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,883 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:28,883 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:28,884 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:28,884 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,884 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,892 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,898 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,904 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,910 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:28,911 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:28,911 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:28,916 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:28,917 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:28,917 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:28,917 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,918 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,919 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,919 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,920 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,920 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,920 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:28,920 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:28,921 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:28,925 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 34])", "<class 'int'>: 33")
2023-10-29 10:27:28,925 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:28,926 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,927 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,927 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,928 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:28,928 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:28,928 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:28,931 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:28,935 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:28,939 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,939 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,944 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,948 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,952 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,956 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,956 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:28,956 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:28,958 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:28,961 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:28,965 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,965 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,970 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,974 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,978 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,981 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:28,982 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:28,982 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:28,983 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:28,987 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:28,991 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:28,991 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:28,996 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,000 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,004 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,008 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,008 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,008 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:29,009 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:29,013 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:29,017 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,017 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,022 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,026 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,029 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,033 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,033 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,033 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:29,035 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:29,038 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:29,042 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,042 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,047 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,051 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,055 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,058 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,059 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,059 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:29,060 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:29,064 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:29,068 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,068 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,073 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,077 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,080 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,085 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,085 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,085 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:29,087 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:29,090 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:29,094 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,095 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,113 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,117 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,121 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,124 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,125 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,125 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:29,126 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:29,130 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:29,133 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,133 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,139 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,143 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,148 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,153 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,153 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,153 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:29,154 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:29,158 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:29,163 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,163 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,168 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,172 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,176 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,180 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,180 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,180 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:29,182 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:29,186 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:29,189 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,190 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,195 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,199 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,203 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,207 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,207 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,208 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:29,209 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:29,213 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:29,217 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,217 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,222 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,227 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,230 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,234 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,235 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,235 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:29,236 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:29,240 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:29,241 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,241 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,247 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,251 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,255 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,259 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-29 10:27:29,259 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-29 10:27:29,259 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:29,260 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:29,261 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:29,262 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,262 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,263 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,264 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,265 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,265 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,266 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:29,266 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:29,266 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:29,267 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:29,267 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,267 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,276 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,282 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,288 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,294 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,295 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:29,295 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:29,300 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:29,301 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:29,301 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:29,301 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,302 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,303 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,304 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,304 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,304 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:29,305 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:29,305 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:29,305 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:29,309 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 35])", "<class 'int'>: 34")
2023-10-29 10:27:29,310 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,311 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,311 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,312 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,313 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,313 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:29,314 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:29,317 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:29,321 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:29,325 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,325 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,333 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,338 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,342 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,345 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,346 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,346 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:29,348 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:29,352 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:29,356 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,356 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,361 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,366 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,369 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,373 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,374 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,374 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:29,376 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:29,379 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:29,383 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,383 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,397 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,401 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,405 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,409 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,410 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,410 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:29,412 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:29,415 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:29,419 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,419 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,425 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,428 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,433 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,437 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,437 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,438 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:29,439 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:29,443 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:29,448 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,448 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,453 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,457 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,461 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,465 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,466 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,466 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:29,467 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:29,471 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:29,475 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,475 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,481 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,485 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,489 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,493 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,493 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,494 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:29,495 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:29,499 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:29,503 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,503 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,508 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,513 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,516 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,520 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,520 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,521 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:29,522 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:29,526 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:29,530 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,530 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,540 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,544 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,549 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,554 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,554 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,554 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:29,555 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:29,559 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:29,563 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,564 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,569 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,574 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,577 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,603 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,604 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,604 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:29,605 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:29,609 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:29,612 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,612 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,617 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,621 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,624 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,628 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,628 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,628 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:29,629 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:29,633 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:29,638 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,638 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,644 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,648 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,652 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,656 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,656 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,656 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:29,658 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:29,662 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:29,663 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,663 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,668 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,673 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,677 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,680 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-29 10:27:29,681 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-29 10:27:29,681 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:29,682 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:29,683 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:29,683 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,684 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,685 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,685 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,686 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,687 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,687 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:29,687 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:29,688 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:29,688 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:29,689 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,689 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,696 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,702 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,708 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,714 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:29,715 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:29,715 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:29,720 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:29,721 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:29,722 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:29,722 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,723 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,724 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,724 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,725 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,725 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:29,725 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:29,726 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:29,726 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:29,730 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 36])", "<class 'int'>: 35")
2023-10-29 10:27:29,731 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:29,731 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,732 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,733 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,734 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:29,734 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:29,734 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:29,737 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:29,741 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:29,745 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,746 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,751 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,755 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,759 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,763 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,763 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:29,764 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:29,765 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:29,769 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:29,773 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,773 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,778 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,783 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,787 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,791 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,791 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:29,791 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:29,793 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:29,797 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:29,801 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,801 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,807 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,811 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,815 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,839 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,840 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:29,840 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:29,841 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:29,845 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:29,849 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,849 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,882 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,914 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,944 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,974 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,974 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:29,975 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:29,976 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:29,981 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:29,985 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:29,985 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:29,991 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,995 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:29,999 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,003 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,003 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,003 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:30,005 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:30,008 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:30,012 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,012 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,018 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,022 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,026 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,031 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,031 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,031 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:30,032 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:30,036 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:30,040 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,040 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,045 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,049 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,053 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,056 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,057 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,057 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:30,058 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:30,062 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:30,065 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,066 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,070 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,074 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,078 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,085 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,086 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,086 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:30,087 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:30,091 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:30,095 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,095 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,105 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,108 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,112 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,116 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,116 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,116 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:30,117 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:30,121 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:30,125 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,125 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,130 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,134 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,137 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,141 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,141 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,141 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:30,142 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:30,146 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:30,150 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,150 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,155 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,159 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,162 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,166 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,166 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,166 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:30,168 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:30,171 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:30,172 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,172 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,176 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,180 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,184 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,187 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-29 10:27:30,188 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-29 10:27:30,188 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:30,189 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:30,189 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:30,190 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,190 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,191 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,191 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,192 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,193 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,193 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:30,193 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:30,194 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:30,194 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:30,194 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,194 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,204 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,210 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,215 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,221 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,222 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:30,222 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:30,227 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:30,228 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:30,228 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:30,228 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,229 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,229 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,230 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,231 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,231 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:30,231 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:30,231 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:30,232 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:30,236 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 37])", "<class 'int'>: 36")
2023-10-29 10:27:30,236 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,236 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,237 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,238 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,238 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,239 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:30,239 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:30,242 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:30,245 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:30,249 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,249 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,254 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,258 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,261 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,265 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,265 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,266 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:30,267 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:30,270 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:30,274 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,274 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,279 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,283 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,287 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,291 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,291 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,291 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:30,292 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:30,296 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:30,299 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,300 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,305 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,308 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,317 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,336 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,337 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,337 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:30,338 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:30,342 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:30,346 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,346 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,351 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,356 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,360 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,363 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,363 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,364 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:30,365 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:30,368 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:30,373 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,373 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,378 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,382 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,385 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,389 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,390 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,390 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:30,391 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:30,395 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:30,399 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,399 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,404 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,408 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,412 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,416 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,417 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,417 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:30,418 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:30,421 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:30,425 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,425 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,430 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,435 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,438 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,443 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,443 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,443 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:30,444 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:30,448 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:30,452 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,452 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,461 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,465 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,469 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,473 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,473 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,473 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:30,474 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:30,478 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:30,481 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,482 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,486 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,490 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,494 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,497 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,498 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,498 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:30,499 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:30,503 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:30,507 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,507 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,515 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,519 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,522 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,526 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,526 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,526 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:30,527 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:30,531 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:30,534 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,535 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,540 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,544 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,548 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,551 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,551 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,552 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:30,553 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:30,556 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:30,557 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,557 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,562 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,566 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,569 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,574 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-29 10:27:30,574 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-29 10:27:30,574 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:30,575 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:30,576 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:30,576 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,576 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,577 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,578 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,578 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,579 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,579 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:30,579 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:30,580 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:30,580 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:30,581 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,581 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,592 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,602 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,611 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,618 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:30,619 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:30,619 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:30,624 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:30,625 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:30,626 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-29 10:27:30,626 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,626 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,627 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,628 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,628 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,628 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:30,629 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-29 10:27:30,629 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-29 10:27:30,629 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:30,633 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 38])", "<class 'int'>: 37")
2023-10-29 10:27:30,633 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,634 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,635 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,636 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,636 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,637 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:30,637 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-29 10:27:30,640 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-29 10:27:30,644 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:30,650 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,650 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,656 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,660 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,665 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,668 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,668 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,669 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-29 10:27:30,670 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-29 10:27:30,673 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:30,677 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,677 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,682 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,685 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,689 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,692 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,693 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,693 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-29 10:27:30,694 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-29 10:27:30,698 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:30,701 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,702 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,707 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,710 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,714 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,717 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,717 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,717 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-29 10:27:30,719 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-29 10:27:30,722 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:30,726 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,726 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,731 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,735 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,738 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,742 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,742 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,742 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-29 10:27:30,744 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-29 10:27:30,747 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:30,751 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,751 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,756 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,760 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,772 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,777 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,777 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,778 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-29 10:27:30,780 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-29 10:27:30,785 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:30,788 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,789 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,794 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,798 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,802 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,805 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,806 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,806 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-29 10:27:30,807 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-29 10:27:30,810 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:30,815 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,815 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,823 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,831 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,851 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,855 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,855 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,855 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-29 10:27:30,857 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-29 10:27:30,860 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:30,865 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,865 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,870 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,874 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,878 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,883 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,883 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,883 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-29 10:27:30,884 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-29 10:27:30,888 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:30,893 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,894 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,899 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,904 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,908 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,911 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,912 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,912 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-29 10:27:30,913 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-29 10:27:30,917 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:30,920 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,920 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,926 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,929 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,933 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,937 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,937 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,937 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-29 10:27:30,938 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-29 10:27:30,942 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:30,946 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,946 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,951 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,955 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,959 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,963 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,963 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,963 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-29 10:27:30,965 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-29 10:27:30,968 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:30,969 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,969 [flexgen.py:103 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-29 10:27:30,974 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,978 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,981 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,986 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-29 10:27:30,986 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-29 10:27:30,986 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-29 10:27:30,987 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-29 10:27:30,988 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:30,989 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,989 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:30,989 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,990 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,991 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,992 [flexgen.py:124 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-29 10:27:30,992 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-29 10:27:30,992 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-29 10:27:30,992 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-29 10:27:30,993 [model.py:382 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-29 10:27:30,993 [flexgen.py:102 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-29 10:27:30,993 [flexgen.py:103 in new_forward] DEBUG - kwargs: {}
2023-10-29 10:27:31,000 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:31,006 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:31,012 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:31,019 [flexgen.py:124 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-29 10:27:31,020 [flexgen.py:136 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-29 10:27:31,020 [model.py:392 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-29 10:27:31,028 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-29 10:27:31,028 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-29 10:27:31,028 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-29 10:27:31,028 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-29 10:27:31,028 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-29 10:27:31,028 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-29 10:27:31,028 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-29 10:27:31,028 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-29 10:27:31,037 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-29 10:27:31,037 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-29 10:27:31,037 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-29 10:27:31,037 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-29 10:27:31,038 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-29 10:27:31,039 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-29 10:27:31,039 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-29 10:27:31,039 [flexgen.py:60 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-29 10:27:31,039 [flexgen.py:60 in layer_reset] DEBUG - lm_head from flexgen to old.
