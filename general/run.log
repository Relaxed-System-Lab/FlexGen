2023-10-31 08:51:36,631 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpmwmfc9af
2023-10-31 08:51:36,631 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpmwmfc9af/_remote_module_non_scriptable.py
2023-10-31 08:51:36,764 [connectionpool.py:957 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-31 08:51:36,991 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 08:51:37,280 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 08:51:37,360 [model.py:130 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-31 08:51:37,360 [model.py:69 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-31 08:51:37,518 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 08:51:37,594 [model.py:79 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-31 08:51:37,601 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-31 08:51:37,601 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0.         0.03918046 0.96081954], size_todo: 85056000
2023-10-31 08:51:37,602 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [1.91116887e-05 3.91789619e-02 9.60801926e-01], size_todo: 85054464
2023-10-31 08:51:37,603 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.04997319 0.08332656 0.86670024], size_todo: 77966592
2023-10-31 08:51:37,604 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.07605041 0.1268119  0.79713769], size_todo: 70878720
2023-10-31 08:51:37,605 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.10571046 0.15066678 0.74362275], size_todo: 63790848
2023-10-31 08:51:37,606 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.12062976 0.17819364 0.7011766 ], size_todo: 56702976
2023-10-31 08:51:37,607 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.14055185 0.19276117 0.66668698], size_todo: 49615104
2023-10-31 08:51:37,608 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.1499285  0.21196308 0.63810841], size_todo: 42527232
2023-10-31 08:51:37,608 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.16439317 0.22156559 0.61404124], size_todo: 35439360
2023-10-31 08:51:37,609 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.17065379 0.23585085 0.59349535], size_todo: 28351488
2023-10-31 08:51:37,610 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.18173357 0.24251579 0.57575064], size_todo: 21263616
2023-10-31 08:51:37,611 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.18608848 0.27488348 0.53902803], size_todo: 14175744
2023-10-31 08:51:37,612 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.194913   0.27840721 0.5266798 ], size_todo: 7087872
2023-10-31 08:51:37,613 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.19802909 0.30507996 0.49689094], size_todo: 0
2023-10-31 08:51:37,613 [model.py:261 in get_policy_weight_map] DEBUG - lm_head, [0.19802909 0.30507996 0.49689094], size_todo: 0
2023-10-31 08:51:37,614 [model.py:267 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-31 08:51:37,616 [model.py:303 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.05 GiB (19.80%), CPU Mem 0.07 GiB (30.51%), Disk Mem 0.12 Gib (49.69%)
2023-10-31 08:51:37,754 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 08:51:38,060 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-31 08:51:38,061 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-31 08:51:38,061 [model.py:393 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-31 08:51:38,061 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-31 08:51:38,061 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-31 08:51:38,061 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-31 08:51:38,062 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-31 08:51:38,062 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-31 08:51:38,062 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-31 08:51:38,062 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-31 08:51:38,062 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-31 08:51:38,062 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-31 08:51:38,063 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-31 08:51:38,063 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-31 08:51:38,063 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-31 08:51:38,063 [model.py:393 in to_test_forward] DEBUG - lm_head to test forward
2023-10-31 08:51:38,067 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 08:51:38,068 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-31 08:51:38,069 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 08:51:38,070 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-31 08:51:38,070 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 08:51:38,078 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-31 08:51:38,081 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 08:51:38,088 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-31 08:51:38,090 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 08:51:38,098 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-31 08:51:38,101 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 08:51:38,108 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-31 08:51:38,111 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 08:51:38,118 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-31 08:51:38,121 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 08:51:38,127 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-31 08:51:38,130 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 08:51:38,136 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-31 08:51:38,139 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 08:51:38,147 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-31 08:51:38,150 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 08:51:38,157 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-31 08:51:38,160 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 08:51:38,169 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-31 08:51:38,172 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 08:51:38,180 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-31 08:51:38,183 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 08:51:38,190 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-31 08:51:38,193 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 08:51:38,194 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-31 08:51:38,194 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 08:51:38,209 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-31 08:51:38,215 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-31 08:51:38,215 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-31 08:51:38,215 [model.py:401 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-31 08:51:38,215 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-31 08:51:38,215 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-31 08:51:38,215 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-31 08:51:38,216 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-31 08:51:38,217 [model.py:401 in reset_forward] DEBUG - lm_head from test to old.
2023-10-31 08:51:38,227 [model.py:516 in init_all_weights] DEBUG - init all weights...
2023-10-31 08:51:39,828 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-31 08:51:39,829 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-31 08:51:39,829 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-31 08:51:39,830 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-31 08:51:39,830 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-31 08:51:39,830 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-31 08:51:39,830 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-31 08:51:39,830 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-31 08:51:39,830 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-31 08:51:39,830 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-31 08:51:39,831 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-31 08:51:39,831 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-31 08:51:39,831 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-31 08:51:39,831 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-31 08:51:39,831 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-31 08:51:39,831 [wrapper.py:267 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-31 08:51:39,970 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 08:51:40,382 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:51:40,382 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64',)
2023-10-31 08:51:40,383 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:40,383 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:51:40,383 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:40,388 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:40,445 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64',)
2023-10-31 08:51:40,445 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:40,446 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64',), {})
2023-10-31 08:51:40,489 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:40,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:40,492 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:40,493 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 08:51:40,535 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:51:40,535 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64', '0')
2023-10-31 08:51:40,536 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:40,536 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:51:40,536 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:40,553 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:40,556 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64', '0')
2023-10-31 08:51:40,556 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:40,556 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64', '0'), {})
2023-10-31 08:51:40,600 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:40,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:40,602 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:40,603 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 08:51:40,634 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:51:40,635 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:40,635 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:40,635 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:51:40,642 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:40,657 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:40,671 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:40,671 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:40,673 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:51,785 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,795 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,799 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:51,800 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:51:51,800 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:51,800 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:51,801 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:51:51,811 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:51,825 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:51,839 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:51,839 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:51,841 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:51,845 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,855 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:51,855 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:51:51,855 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:51,855 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:51,855 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:51:51,866 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:51,880 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:51,894 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:51,894 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:51,896 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:51,899 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,907 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,910 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:51,910 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:51:51,910 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:51,910 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:51,911 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:51:51,920 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:51,934 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:51,948 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:51,948 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:51,950 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:51,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,957 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,962 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:51,964 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:51,964 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:51:51,965 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:51,965 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:51,965 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:51:51,977 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:51,991 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:52,005 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,005 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,007 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,014 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,018 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,021 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,021 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:51:52,021 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,021 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,021 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:51:52,031 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:52,046 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:52,059 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,060 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,061 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,075 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,075 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:51:52,076 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,076 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,076 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:51:52,088 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:52,102 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:52,116 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,116 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,118 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,125 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,129 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,132 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,132 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:51:52,132 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,132 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,133 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:51:52,142 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:52,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:52,172 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,172 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,174 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,182 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,186 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,188 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,189 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:51:52,189 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,189 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,189 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:51:52,201 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:52,217 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:52,231 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,231 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,233 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,238 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,242 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,247 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,250 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,250 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:51:52,250 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,250 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,250 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:51:52,261 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:52,278 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:52,294 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,295 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,296 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,305 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,313 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,313 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:51:52,313 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,313 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,313 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:51:52,335 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:52,350 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:52,365 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,365 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,366 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,374 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,380 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,380 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:51:52,380 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,380 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,380 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:51:52,390 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:52,391 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:52,406 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,406 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:52,411 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:52,415 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,422 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 08:51:52,425 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 08:51:52,425 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:51:52,425 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,425 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:52,425 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:51:52,444 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:52,491 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:52,493 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,493 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:52,494 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 08:51:52,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:52,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:52,497 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 08:51:52,498 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 08:51:52,498 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:51:52,498 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 08:51:52,498 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:52,498 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:51:52,499 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:52,543 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:52,587 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 08:51:52,587 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:52,588 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 08:51:52,599 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 08:51:52,602 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 08:51:52,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 08:51:52,608 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 9, 50272), torch.float32


2023-10-31 08:51:55,768 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:51:55,770 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:51:55,770 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:55,772 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:51:55,775 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:55,781 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:55,821 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:51:55,821 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:55,823 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:51:55,825 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:55,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:55,827 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:55,828 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:55,829 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:51:55,829 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 10), torch.int64', '9')
2023-10-31 08:51:55,829 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:55,830 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:51:55,832 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:55,849 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:55,852 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 10), torch.int64', '9')
2023-10-31 08:51:55,852 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:55,853 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 10), torch.int64', '9'), {})
2023-10-31 08:51:55,855 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:55,856 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:55,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:55,858 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:55,860 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:51:55,860 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:55,860 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:55,861 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:51:55,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:55,888 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:55,902 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:55,902 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:55,905 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:55,916 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:55,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:55,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:55,927 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:55,927 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:51:55,927 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:55,927 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:55,927 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:51:55,942 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:55,956 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:55,970 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:55,970 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:55,973 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:55,977 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:55,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:55,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:55,989 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:55,989 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:51:55,989 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:55,989 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:55,989 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:51:56,016 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:56,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:56,042 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,043 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,045 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,050 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,062 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,062 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:51:56,062 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,063 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,063 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:51:56,083 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:56,096 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:56,109 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,110 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,112 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,121 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,127 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,129 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,130 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:51:56,130 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,130 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,130 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:51:56,155 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:56,168 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:56,181 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,182 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,184 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,200 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,200 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:51:56,200 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,200 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,200 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:51:56,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:56,234 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:56,247 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,247 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,249 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,263 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,265 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,266 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:51:56,266 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,266 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,266 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:51:56,293 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:56,306 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:56,320 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,320 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,322 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,326 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,335 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,338 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,338 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:51:56,338 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,338 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,338 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:51:56,362 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:56,376 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:56,390 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,390 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,392 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,397 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,406 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,408 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,409 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:51:56,409 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,409 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,409 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:51:56,436 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:56,451 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:56,465 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,465 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,468 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,481 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,484 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,484 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:51:56,484 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,484 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,484 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:51:56,505 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:56,521 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:56,536 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,536 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,539 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,543 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,548 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,552 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,554 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,555 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:51:56,555 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,555 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,555 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:51:56,583 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:56,597 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:56,612 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,612 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,615 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,620 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,624 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,628 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,631 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,631 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:51:56,631 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,631 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,631 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:51:56,654 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:56,655 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:56,670 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,670 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,673 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:56,679 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,684 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 08:51:56,691 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 08:51:56,692 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:51:56,692 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,692 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:56,692 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:51:56,740 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:56,781 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:56,782 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,782 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:56,783 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:56,785 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,786 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,787 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:56,788 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:51:56,788 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,788 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:56,789 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:51:56,789 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:56,827 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:56,867 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:56,867 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:56,868 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:56,871 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:56,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:56,875 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:56,877 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:51:56,880 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:51:56,880 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:51:56,880 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:56,881 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:51:56,883 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:56,888 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:56,930 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:51:56,930 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:56,931 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:51:56,932 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,933 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,935 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:56,935 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:51:56,935 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 11), torch.int64', '10')
2023-10-31 08:51:56,936 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:56,936 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:51:56,937 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:56,954 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:56,957 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 11), torch.int64', '10')
2023-10-31 08:51:56,957 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:56,958 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 11), torch.int64', '10'), {})
2023-10-31 08:51:56,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,961 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,963 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:56,964 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:56,966 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:51:56,966 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:56,967 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:56,967 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:51:56,981 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:56,995 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:57,009 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,009 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,012 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,022 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,027 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,030 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,030 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:51:57,030 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,031 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,031 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:51:57,038 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:57,052 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:57,065 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,065 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,068 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,072 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,077 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,083 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,084 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:51:57,084 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,084 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,084 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:51:57,096 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:57,109 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:57,122 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,123 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,125 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,131 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,145 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,146 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:51:57,146 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,146 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,146 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:51:57,155 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:57,168 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:57,181 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,181 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,184 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,199 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,200 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:51:57,200 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,200 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,200 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:51:57,209 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:57,222 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:57,235 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,235 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,238 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,243 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,247 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,251 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,254 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,254 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:51:57,254 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,254 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,254 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:51:57,266 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:57,279 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:57,292 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,292 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,295 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,300 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,308 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,310 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,311 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:51:57,311 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,311 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,311 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:51:57,320 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:57,333 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:57,346 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,347 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,349 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,362 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,365 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,365 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:51:57,365 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,365 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,365 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:51:57,373 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:57,388 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:57,402 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,402 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,405 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,410 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,414 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,421 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,421 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:51:57,421 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,421 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,421 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:51:57,430 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:57,445 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:57,460 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,460 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,462 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,475 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,478 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,478 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:51:57,478 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,478 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,478 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:51:57,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:57,505 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:57,520 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,520 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,523 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,528 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,532 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,536 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,539 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,540 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:51:57,540 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,540 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,540 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:51:57,553 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:57,567 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:57,583 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,583 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,585 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,590 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,594 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,599 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,601 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,601 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:51:57,602 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,602 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,602 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:51:57,606 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:57,607 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:57,621 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,622 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,624 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,629 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,633 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,637 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 08:51:57,640 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 08:51:57,640 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:51:57,640 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,640 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:57,640 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:51:57,663 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:57,703 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:57,704 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,705 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:57,705 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:57,707 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,709 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,709 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:57,710 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:51:57,710 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,710 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:57,710 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:51:57,711 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:57,749 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:57,788 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,788 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:57,789 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:57,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:57,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:57,796 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:57,798 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:51:57,800 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:51:57,800 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:51:57,800 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:57,800 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:51:57,802 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:57,807 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:57,848 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:51:57,848 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:57,849 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:51:57,850 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,853 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:57,853 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:51:57,853 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 12), torch.int64', '11')
2023-10-31 08:51:57,853 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:57,853 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:51:57,856 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:57,872 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:57,874 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 12), torch.int64', '11')
2023-10-31 08:51:57,874 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:57,875 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 12), torch.int64', '11'), {})
2023-10-31 08:51:57,876 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,877 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:57,880 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:57,881 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:51:57,882 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,882 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,882 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:51:57,886 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:57,899 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:57,913 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,913 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,915 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:57,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:57,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:57,931 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:57,932 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:51:57,932 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,932 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,932 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:51:57,941 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:57,954 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:57,968 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:57,969 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,972 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:57,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:57,987 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:57,992 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:57,994 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:57,995 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:51:57,995 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:57,995 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:57,995 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:51:58,004 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:58,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:58,031 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,031 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,034 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,038 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,043 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,047 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,049 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,050 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:51:58,050 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,050 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,050 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:51:58,057 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:58,070 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:58,083 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,084 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,086 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,095 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,100 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,102 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,102 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:51:58,102 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,102 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,103 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:51:58,112 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:58,125 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:58,139 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,139 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,142 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,147 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,151 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,156 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,159 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,159 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:51:58,159 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,159 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,159 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:51:58,167 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:58,180 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:58,193 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,193 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,196 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,205 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,209 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,211 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,212 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:51:58,212 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,212 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,212 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:51:58,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:58,234 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:58,247 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,248 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,250 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,255 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,259 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,264 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,266 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,266 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:51:58,266 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,267 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,267 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:51:58,274 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:58,288 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:58,301 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,302 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,304 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,309 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,318 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,321 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,321 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:51:58,321 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,322 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,322 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:51:58,331 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:58,346 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:58,360 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,361 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,363 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,368 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,372 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,376 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,379 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,379 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:51:58,379 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,379 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,380 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:51:58,387 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:58,404 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:58,418 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,419 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,421 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,430 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,434 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,437 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,437 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:51:58,437 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,437 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,437 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:51:58,452 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:58,466 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:58,481 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,481 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,484 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,493 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,497 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,500 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,500 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:51:58,500 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,500 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,500 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:51:58,508 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:58,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:58,524 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,524 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,526 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,531 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,535 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,540 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 08:51:58,542 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 08:51:58,542 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:51:58,542 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,542 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:58,543 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:51:58,563 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:58,603 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:58,604 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,604 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:58,605 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:58,606 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,608 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,609 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:58,609 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:51:58,609 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,609 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:58,609 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:51:58,610 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:58,648 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:58,688 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,689 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:58,690 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:58,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:58,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:58,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:58,698 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:51:58,699 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:51:58,700 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:51:58,700 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:58,700 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:51:58,701 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:58,706 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:58,746 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:51:58,747 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:58,748 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:51:58,749 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,749 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,751 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:58,752 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:51:58,752 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 13), torch.int64', '12')
2023-10-31 08:51:58,752 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:58,752 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:51:58,755 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:58,770 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:58,773 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 13), torch.int64', '12')
2023-10-31 08:51:58,773 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:58,774 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 13), torch.int64', '12'), {})
2023-10-31 08:51:58,775 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,778 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:58,778 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:58,780 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:51:58,781 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,781 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,781 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:51:58,786 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:58,799 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:58,812 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,812 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,815 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,820 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,824 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,828 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,831 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:58,831 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:51:58,831 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,831 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,831 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:51:58,839 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:58,852 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:58,866 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,866 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,868 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,877 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,882 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,884 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:58,885 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:51:58,885 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,885 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,885 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:51:58,894 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:58,908 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:58,921 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,921 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,924 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,933 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,937 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,940 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:58,940 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:51:58,940 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,940 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,941 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:51:58,948 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:58,961 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:58,974 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:58,974 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,977 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:58,981 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,990 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:58,993 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:58,993 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:51:58,993 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:58,993 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:58,993 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:51:59,001 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:59,014 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:59,027 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,028 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,030 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,046 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,047 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:51:59,047 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,047 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,047 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:51:59,055 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:59,068 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:59,081 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,081 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,083 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,097 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,099 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,099 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:51:59,100 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,100 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,100 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:51:59,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:59,121 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:59,134 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,134 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,137 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,153 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,153 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:51:59,153 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,153 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,153 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:51:59,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:59,172 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:59,185 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,185 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,188 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,204 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,204 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:51:59,205 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,205 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,205 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:51:59,211 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:59,226 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:51:59,240 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,240 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,243 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,248 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,252 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,259 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,259 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:51:59,259 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,259 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,259 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:51:59,264 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:59,278 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:51:59,293 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,294 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,296 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,306 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,313 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,313 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:51:59,313 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,313 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,313 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:51:59,322 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:59,336 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:51:59,352 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,352 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,354 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,363 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,368 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,370 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,370 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:51:59,370 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,371 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,371 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:51:59,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:59,379 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:51:59,393 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,393 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,396 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,406 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,411 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 08:51:59,414 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 08:51:59,414 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:51:59,415 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,415 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:59,415 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:51:59,423 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:59,462 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:51:59,463 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,464 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:59,464 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:59,465 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,468 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:59,468 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:51:59,469 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,469 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:59,469 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:51:59,469 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:59,507 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:51:59,547 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,547 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:59,548 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:51:59,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:59,552 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:59,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:51:59,555 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:51:59,557 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:51:59,557 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:51:59,557 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:59,557 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:51:59,558 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:59,563 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:51:59,606 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:51:59,606 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:59,607 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:51:59,608 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,609 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,610 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,611 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:59,611 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:51:59,611 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 14), torch.int64', '13')
2023-10-31 08:51:59,611 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:51:59,611 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:51:59,612 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:59,630 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:51:59,633 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 14), torch.int64', '13')
2023-10-31 08:51:59,633 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:51:59,634 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 14), torch.int64', '13'), {})
2023-10-31 08:51:59,635 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,637 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:51:59,638 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:51:59,640 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:51:59,640 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,640 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,640 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:51:59,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:59,657 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:51:59,670 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,671 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,673 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,678 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,683 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,687 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,690 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:51:59,690 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:51:59,690 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,690 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,690 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:51:59,698 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:59,711 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:51:59,724 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,724 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,727 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,731 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,735 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,740 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,742 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:51:59,742 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:51:59,743 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,743 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,743 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:51:59,752 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:59,765 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:51:59,780 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,780 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,783 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,788 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,792 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,796 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,799 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:51:59,799 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:51:59,799 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,799 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,799 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:51:59,807 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:59,820 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:51:59,833 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,833 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,836 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,840 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,845 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,851 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:51:59,852 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:51:59,852 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,852 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,852 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:51:59,865 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:59,878 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:51:59,891 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,891 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,894 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,908 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,910 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:51:59,911 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:51:59,911 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,911 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,911 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:51:59,918 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:51:59,931 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:51:59,945 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:51:59,945 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,947 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:51:59,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,956 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:51:59,963 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:51:59,963 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:51:59,963 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:51:59,963 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:51:59,963 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:51:59,977 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:51:59,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:00,004 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,004 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,007 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,016 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,022 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:52:00,023 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:00,023 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,023 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,023 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:00,031 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:00,045 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:00,058 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,058 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,061 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,074 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,076 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:52:00,076 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:00,076 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,077 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,077 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:00,086 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:00,101 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:00,115 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,115 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,118 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,126 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,131 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,133 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:52:00,133 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:00,134 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,134 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,134 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:00,141 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:00,156 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:00,172 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,172 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,174 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,183 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,187 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,190 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:52:00,190 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:00,190 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,190 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,190 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:00,204 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:00,219 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:00,234 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,234 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,237 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,241 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,250 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,252 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:52:00,253 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:00,253 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,253 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,253 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:00,260 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:00,261 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:00,276 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,276 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,279 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,283 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,292 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 08:52:00,294 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 08:52:00,294 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:00,295 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,295 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:00,295 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:00,309 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:00,349 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:00,350 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,350 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:00,351 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:00,352 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,353 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,354 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:00,355 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:00,355 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,355 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:00,355 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:00,356 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:00,394 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:00,435 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,435 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:00,436 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:00,438 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:00,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:00,442 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:00,444 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:00,445 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:00,445 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:00,446 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:00,446 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:00,446 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:00,452 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:00,492 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:00,493 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:00,493 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:00,494 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,497 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:00,497 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:00,498 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 15), torch.int64', '14')
2023-10-31 08:52:00,498 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:00,498 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:00,499 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:00,516 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:00,518 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 15), torch.int64', '14')
2023-10-31 08:52:00,519 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:00,519 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 15), torch.int64', '14'), {})
2023-10-31 08:52:00,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:00,524 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:00,526 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:00,526 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,526 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,526 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:00,530 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:00,543 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:00,556 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,557 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,559 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,564 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,568 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,572 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,575 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,575 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:00,575 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,575 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,575 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:00,581 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:00,594 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:00,607 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,607 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,610 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,614 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,618 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,623 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,625 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,625 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:00,626 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,626 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,626 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:00,632 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:00,645 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:00,658 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,659 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,661 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,666 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,670 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,677 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,677 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:00,677 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,677 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,677 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:00,682 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:00,694 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:00,707 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,708 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,710 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,714 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,726 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,727 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:00,727 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,727 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,727 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:00,736 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:00,750 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:00,764 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,764 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,767 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,771 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,784 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,784 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:00,784 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,784 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,784 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:00,789 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:00,802 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:00,816 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,816 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,819 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,824 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,828 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,833 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,836 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,836 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:00,836 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,836 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,836 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:00,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:00,856 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:00,869 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,870 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,872 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,877 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,881 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,886 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,888 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,889 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:00,889 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,889 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,889 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:00,893 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:00,909 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:00,922 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,923 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,926 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,939 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,942 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:00,942 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:00,943 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:00,943 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,943 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:00,952 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:00,967 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:00,982 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:00,982 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:00,985 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:00,989 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:00,998 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,001 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:01,001 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:01,001 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,001 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,002 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:01,008 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:01,024 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:01,039 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,039 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,042 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,046 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,051 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,055 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,058 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:01,058 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:01,058 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,058 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,058 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:01,069 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:01,084 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:01,100 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,100 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,103 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,107 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,119 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:01,120 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:01,120 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,120 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,120 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:01,124 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:01,125 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:01,140 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,141 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,143 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,148 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 08:52:01,160 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 08:52:01,160 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:01,161 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,161 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:01,161 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:01,173 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:01,212 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:01,214 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,214 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:01,215 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:01,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,218 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,218 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:01,219 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:01,219 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,219 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:01,219 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:01,220 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:01,258 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:01,298 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,299 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:01,300 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:01,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:01,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:01,306 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:01,308 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:01,309 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:01,309 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:01,309 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:01,309 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:01,310 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:01,316 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:01,356 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:01,356 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:01,357 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:01,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,361 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:01,361 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:01,361 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 16), torch.int64', '15')
2023-10-31 08:52:01,361 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:01,362 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:01,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:01,380 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:01,383 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 16), torch.int64', '15')
2023-10-31 08:52:01,383 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:01,384 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 16), torch.int64', '15'), {})
2023-10-31 08:52:01,385 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,386 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,387 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:01,388 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:01,390 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:01,390 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,391 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,391 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:01,393 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:01,407 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:01,420 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,421 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,423 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,428 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,433 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,437 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,440 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,440 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:01,440 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,441 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,441 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:01,445 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:01,459 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:01,472 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,472 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,475 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,484 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,489 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,491 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,491 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:01,492 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,492 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,492 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:01,501 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:01,514 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:01,531 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,531 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,534 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,540 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,544 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,549 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,551 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,552 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:01,552 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,552 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,552 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:01,560 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:01,573 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:01,586 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,586 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,589 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,594 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,598 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,602 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,605 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,605 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:01,605 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,605 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,605 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:01,614 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:01,628 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:01,641 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,641 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,643 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,653 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,660 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,660 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:01,660 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,660 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,660 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:01,668 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:01,681 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:01,694 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,694 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,697 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,706 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,710 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,713 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,713 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:01,713 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,713 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,713 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:01,723 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:01,737 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:01,751 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,751 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,754 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,759 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,763 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,767 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,770 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,770 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:01,770 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,770 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,771 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:01,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:01,793 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:01,806 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,806 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,809 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,813 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,817 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,825 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,825 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:01,825 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,825 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,825 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:01,835 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:01,850 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:01,864 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,864 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,867 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,876 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,881 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,883 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,883 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:01,884 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,884 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,884 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:01,892 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:01,908 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:01,923 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,923 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,926 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,931 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,935 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,940 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:01,942 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:01,942 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:01,943 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:01,943 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,943 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:01,958 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:01,973 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:01,988 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:01,988 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:01,991 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:01,997 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:02,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:02,006 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:02,009 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:02,009 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:02,009 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,009 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,009 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:02,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:02,018 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:02,033 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,033 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,036 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,040 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:02,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:02,049 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 08:52:02,052 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 08:52:02,052 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:02,052 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,052 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:02,052 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:02,068 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:02,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:02,109 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,109 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:02,110 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:02,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,112 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,113 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,114 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:02,114 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:02,114 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,114 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:02,114 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:02,115 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:02,153 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:02,193 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,193 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:02,194 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:02,196 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:02,198 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:02,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:02,202 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:02,203 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:02,204 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:02,204 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:02,204 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:02,205 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:02,210 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:02,250 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:02,251 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:02,252 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:02,253 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,255 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,256 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:02,256 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:02,256 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 17), torch.int64', '16')
2023-10-31 08:52:02,256 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:02,256 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:02,257 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:02,275 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:02,277 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 17), torch.int64', '16')
2023-10-31 08:52:02,277 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:02,278 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 17), torch.int64', '16'), {})
2023-10-31 08:52:02,279 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,282 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:02,282 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:02,284 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:02,284 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,285 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,285 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:02,289 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:02,302 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:02,315 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,315 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,318 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,323 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,327 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,334 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,334 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:02,334 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,334 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,334 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:02,342 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:02,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:02,368 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,368 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,371 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,375 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,380 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,384 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,386 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,387 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:02,387 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,387 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,387 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:02,396 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:02,410 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:02,424 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,424 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,426 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,442 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,442 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:02,442 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,443 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,443 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:02,450 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:02,463 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:02,476 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,477 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,479 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,484 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,492 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,494 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,495 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:02,495 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,495 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,495 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:02,504 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:02,517 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:02,530 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,531 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,533 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,538 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,549 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,549 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:02,549 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,549 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,549 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:02,557 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:02,570 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:02,584 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,584 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,587 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,591 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,596 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,600 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,602 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,603 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:02,603 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,603 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,603 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:02,612 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:02,625 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:02,638 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,638 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,641 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,650 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,654 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,656 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,657 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:02,657 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,657 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,657 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:02,664 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:02,678 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:02,692 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,692 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,694 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:02,699 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,703 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:02,711 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:02,711 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:02,711 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:02,711 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,711 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:02,720 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:02,735 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:02,750 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:02,750 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:02,995 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,000 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,004 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,012 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:03,012 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:03,012 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,012 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,012 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:03,020 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:03,035 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:03,050 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,051 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,053 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,058 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,066 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,069 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:03,069 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:03,069 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,069 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,070 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:03,077 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:03,091 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:03,106 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,106 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,109 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,113 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,124 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:03,124 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:03,125 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,125 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,125 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:03,129 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:03,130 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:03,145 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,145 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,147 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,156 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,160 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 08:52:03,162 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 08:52:03,163 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:03,163 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,163 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:03,163 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:03,174 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:03,214 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:03,215 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,216 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:03,216 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:03,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,218 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,220 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:03,220 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:03,220 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,221 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:03,221 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:03,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:03,259 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:03,299 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,299 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:03,300 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:03,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:03,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:03,306 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:03,308 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:03,309 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:03,309 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:03,309 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:03,310 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:03,311 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:03,316 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:03,356 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:03,357 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:03,357 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:03,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,361 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:03,361 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:03,362 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 18), torch.int64', '17')
2023-10-31 08:52:03,362 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:03,362 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:03,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:03,380 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:03,382 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 18), torch.int64', '17')
2023-10-31 08:52:03,382 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:03,383 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 18), torch.int64', '17'), {})
2023-10-31 08:52:03,385 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,386 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,387 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:03,388 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:03,389 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:03,390 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,390 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,390 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:03,392 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:03,405 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:03,419 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,419 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,422 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,427 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,438 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,438 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:03,438 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,438 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,438 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:03,443 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:03,456 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:03,469 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,469 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,472 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,485 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,487 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,488 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:03,488 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,488 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,488 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:03,493 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:03,506 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:03,519 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,520 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,522 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,527 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,531 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,536 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,538 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,538 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:03,539 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,539 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,539 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:03,543 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:03,556 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:03,569 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,569 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,572 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,576 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,580 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,585 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,587 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,588 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:03,588 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,588 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,588 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:03,593 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:03,606 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:03,619 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,620 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,622 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,627 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,632 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,638 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,639 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:03,639 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,639 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,639 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:03,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:03,656 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:03,669 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,670 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,672 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,677 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,681 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,688 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,688 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:03,688 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,688 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,689 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:03,695 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:03,708 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:03,721 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,721 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,724 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,732 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,737 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,739 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,740 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:03,740 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,740 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,740 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:03,744 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:03,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:03,771 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,772 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,774 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,790 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,790 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:03,790 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,790 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,790 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:03,796 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:03,811 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:03,825 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,826 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,828 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,833 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,845 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,845 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:03,845 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,845 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,845 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:03,850 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:03,865 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:03,880 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,880 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,883 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,887 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,892 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,899 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,899 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:03,899 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,899 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,899 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:03,909 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:03,923 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:03,938 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,938 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,941 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,950 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,957 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,957 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:03,957 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,957 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,958 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:03,962 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:03,963 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:03,978 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:03,978 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:03,980 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:03,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,989 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 08:52:03,996 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 08:52:03,996 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:03,997 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:03,997 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:03,997 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:04,012 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:04,051 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:04,052 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,053 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:04,053 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:04,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,060 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,061 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:04,061 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:04,061 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,061 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:04,062 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:04,062 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:04,101 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:04,140 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,141 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:04,141 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:04,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:04,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:04,148 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:04,150 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:04,151 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:04,151 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:04,151 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:04,151 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:04,152 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:04,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:04,198 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:04,198 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:04,199 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:04,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,201 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,202 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:04,203 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:04,203 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 19), torch.int64', '18')
2023-10-31 08:52:04,203 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:04,203 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:04,205 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:04,222 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:04,224 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 19), torch.int64', '18')
2023-10-31 08:52:04,224 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:04,225 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 19), torch.int64', '18'), {})
2023-10-31 08:52:04,226 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,227 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,229 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,229 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:04,231 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:04,231 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,232 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,232 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:04,236 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:04,249 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:04,262 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,262 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,265 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,270 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,274 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,281 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,281 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:04,281 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,281 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,281 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:04,289 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:04,302 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:04,315 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,316 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,320 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,330 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,334 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,337 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,337 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:04,337 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,337 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,337 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:04,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:04,359 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:04,373 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,373 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,376 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,380 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,385 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,392 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,392 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:04,392 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,392 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,392 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:04,400 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:04,414 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:04,427 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,428 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,430 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,439 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,444 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,446 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,446 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:04,446 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,447 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,447 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:04,456 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:04,469 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:04,482 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,483 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,485 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,494 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,499 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,501 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,501 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:04,501 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,502 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,502 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:04,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:04,522 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:04,535 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,536 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,538 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,543 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,547 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,552 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,554 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,554 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:04,554 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,555 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,555 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:04,564 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:04,577 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:04,590 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,590 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,593 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,598 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,602 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,606 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,609 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,609 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:04,610 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,610 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,610 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:04,618 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:04,633 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:04,646 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,646 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,649 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,654 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,658 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,662 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,665 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,665 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:04,665 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,665 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,665 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:04,675 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:04,690 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:04,704 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,705 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,707 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,712 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,716 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,721 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,723 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,723 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:04,724 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,724 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,724 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:04,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:04,746 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:04,761 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,762 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,764 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,768 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,773 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,777 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,780 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,780 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:04,780 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,780 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,780 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:04,795 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:04,809 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:04,824 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,825 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,827 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,831 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,836 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,840 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,843 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,843 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:04,843 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,843 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,843 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:04,851 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:04,852 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:04,866 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,867 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:04,869 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:04,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,878 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,882 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 08:52:04,885 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 08:52:04,885 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:04,885 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,885 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:04,885 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:04,899 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:04,940 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:04,941 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:04,941 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:04,942 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:04,943 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,945 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:04,946 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:04,946 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:04,946 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:04,947 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:04,947 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:04,947 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:04,985 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:05,025 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,025 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:05,026 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:05,028 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:05,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:05,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:05,033 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:05,035 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:05,035 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:05,035 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:05,035 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:05,036 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:05,041 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:05,082 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:05,082 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:05,083 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:05,084 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,085 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,086 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,086 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:05,087 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:05,087 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 20), torch.int64', '19')
2023-10-31 08:52:05,087 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:05,087 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:05,088 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:05,105 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:05,108 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 20), torch.int64', '19')
2023-10-31 08:52:05,108 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:05,109 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 20), torch.int64', '19'), {})
2023-10-31 08:52:05,110 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,112 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,113 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:05,115 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:05,115 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,115 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,116 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:05,120 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:05,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:05,146 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,146 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,149 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,157 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,165 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,165 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:05,165 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,165 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,165 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:05,173 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:05,186 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:05,199 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,199 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,202 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,206 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,215 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,217 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,218 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:05,218 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,218 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,218 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:05,224 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:05,237 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:05,250 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,250 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,253 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,262 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,267 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,269 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,269 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:05,270 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,270 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,270 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:05,274 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:05,287 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:05,300 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,300 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,303 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,307 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,312 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,316 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,318 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,319 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:05,319 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,319 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,319 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:05,327 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:05,340 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:05,353 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,353 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,356 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,364 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,371 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,371 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:05,372 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,372 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,372 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:05,376 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:05,389 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:05,402 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,402 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,405 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,411 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,415 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,422 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,422 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:05,422 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,423 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,423 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:05,432 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:05,445 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:05,458 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,458 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,461 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,465 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,473 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,476 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,476 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:05,476 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,476 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,477 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:05,481 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:05,495 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:05,508 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,508 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,510 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,526 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,526 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:05,526 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,526 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,526 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:05,532 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:05,547 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:05,561 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,561 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,564 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,569 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,573 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,580 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,580 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:05,580 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,580 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,580 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:05,585 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:05,600 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:05,615 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,615 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,617 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,622 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,626 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,631 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,633 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,634 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:05,634 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,634 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,634 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:05,645 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:05,659 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:05,674 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,674 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,677 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,681 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,690 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,692 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,692 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:05,693 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,693 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,693 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:05,697 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:05,698 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:05,713 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,713 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,716 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,720 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 08:52:05,731 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 08:52:05,731 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:05,731 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,731 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:05,731 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:05,739 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:05,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:05,779 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,779 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:05,780 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:05,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,782 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,784 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:05,784 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:05,784 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,784 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:05,784 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:05,785 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:05,823 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:05,863 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,863 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:05,864 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:05,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:05,868 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:05,869 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:05,871 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:05,872 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:05,872 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:05,872 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:05,873 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:05,873 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:05,879 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:05,920 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:05,921 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:05,921 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:05,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,923 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,925 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:05,925 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:05,925 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 21), torch.int64', '20')
2023-10-31 08:52:05,926 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:05,926 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:05,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:05,944 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:05,946 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 21), torch.int64', '20')
2023-10-31 08:52:05,946 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:05,947 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 21), torch.int64', '20'), {})
2023-10-31 08:52:05,948 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,949 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,950 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:05,951 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:05,953 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:05,953 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:05,953 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,954 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:05,956 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:05,968 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:05,981 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:05,982 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:05,984 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:05,989 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:05,993 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:05,997 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,000 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,000 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:06,000 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,000 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,000 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:06,004 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:06,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:06,030 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,031 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,033 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,037 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,042 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,046 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,048 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,049 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:06,049 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,049 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,049 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:06,054 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:06,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:06,080 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,080 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,083 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,087 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,098 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,099 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:06,099 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,099 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,099 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:06,103 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:06,116 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:06,129 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,129 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,132 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,145 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,147 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,147 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:06,147 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,148 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,148 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:06,153 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:06,165 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:06,178 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,179 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,181 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,186 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,190 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,195 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,197 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,197 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:06,198 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,198 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,198 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:06,202 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:06,215 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:06,228 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,228 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,231 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,235 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,239 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,244 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,246 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,246 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:06,247 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,247 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,247 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:06,253 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:06,265 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:06,278 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,279 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,281 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,286 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,290 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,294 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,297 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,297 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:06,297 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,297 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,297 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:06,302 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:06,318 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:06,331 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,332 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,334 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,339 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,347 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,350 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,350 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:06,350 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,350 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,350 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:06,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:06,370 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:06,385 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,385 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,387 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,392 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,396 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,403 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,403 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:06,403 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,404 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,404 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:06,408 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:06,423 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:06,438 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,438 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,441 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,445 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,454 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,457 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,457 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:06,457 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,457 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,458 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:06,474 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:06,488 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:06,504 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,505 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,507 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,516 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,520 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,523 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,523 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:06,523 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,523 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,523 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:06,527 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:06,528 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:06,543 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,543 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,546 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 08:52:06,561 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 08:52:06,561 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:06,561 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,562 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:06,562 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:06,581 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:06,621 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:06,622 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,622 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:06,623 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:06,624 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,625 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,626 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,627 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:06,627 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:06,627 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,627 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:06,627 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:06,628 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:06,666 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:06,705 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,705 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:06,706 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:06,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:06,710 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:06,712 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:06,714 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:06,715 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:06,715 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:06,715 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:06,715 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:06,716 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:06,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:06,762 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:06,762 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:06,763 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:06,764 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,765 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,766 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,766 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:06,767 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:06,767 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 22), torch.int64', '21')
2023-10-31 08:52:06,767 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:06,767 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:06,768 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:06,785 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:06,787 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 22), torch.int64', '21')
2023-10-31 08:52:06,788 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:06,788 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 22), torch.int64', '21'), {})
2023-10-31 08:52:06,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,792 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:06,793 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:06,794 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:06,795 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,795 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,795 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:06,797 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:06,810 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:06,826 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,826 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,829 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,845 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:06,845 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:06,845 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,846 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,846 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:06,853 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:06,866 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:06,879 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,880 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,882 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,887 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,891 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,898 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:06,898 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:06,899 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,899 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,899 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:06,908 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:06,921 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:06,934 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,935 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,937 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,942 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,950 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,953 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:06,953 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:06,953 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:06,954 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,954 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:06,961 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:06,974 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:06,987 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:06,987 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:06,990 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:06,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:06,998 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,003 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,005 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,006 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:07,006 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,006 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,006 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:07,015 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:07,028 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:07,041 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,041 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,044 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,048 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,057 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,060 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,060 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:07,060 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,060 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,060 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:07,068 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:07,081 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:07,094 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,094 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,097 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,105 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,109 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,112 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,112 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:07,112 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,113 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,113 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:07,122 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:07,136 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:07,149 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,150 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,152 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,157 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,161 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,166 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,168 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,169 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:07,169 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,169 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,169 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:07,176 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:07,191 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:07,204 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,204 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,207 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,211 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,215 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,222 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,222 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:07,223 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,223 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,223 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:07,232 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:07,247 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:07,261 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,262 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,264 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,269 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,277 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,280 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,280 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:07,280 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,280 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,280 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:07,288 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:07,303 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:07,318 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,318 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,321 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,330 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,334 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,337 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,337 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:07,337 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,337 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,338 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:07,352 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:07,366 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:07,381 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,381 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,384 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,393 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,397 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,400 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,400 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:07,400 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,401 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,401 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:07,408 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:07,409 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:07,424 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,424 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,427 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 08:52:07,443 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 08:52:07,443 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:07,443 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,443 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:07,444 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:07,458 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:07,497 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:07,498 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,498 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:07,499 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:07,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,501 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,502 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,503 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:07,503 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:07,503 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,504 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:07,504 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:07,504 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:07,543 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:07,583 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,583 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:07,584 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:07,586 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:07,588 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:07,590 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:07,592 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:07,594 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:07,594 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:07,594 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:07,594 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:07,596 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:07,601 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:07,642 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:07,642 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:07,643 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:07,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,645 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,646 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:07,647 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:07,647 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 23), torch.int64', '22')
2023-10-31 08:52:07,647 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:07,647 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:07,648 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:07,666 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:07,668 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 23), torch.int64', '22')
2023-10-31 08:52:07,669 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:07,669 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 23), torch.int64', '22'), {})
2023-10-31 08:52:07,671 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,673 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:07,674 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:07,676 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:07,676 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,676 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,676 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:07,680 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:07,693 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:07,706 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,706 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,709 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,714 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,725 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:07,725 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:07,725 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,725 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,726 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:07,730 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:07,743 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:07,756 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,756 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,759 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,764 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,768 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,775 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:07,775 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:07,775 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,775 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,776 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:07,784 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:07,797 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:07,810 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,811 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,813 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,818 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,823 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,827 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,830 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:07,830 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:07,830 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,830 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,831 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:07,835 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:07,848 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:07,861 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,862 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,864 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,869 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,881 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:07,881 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:07,882 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,882 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,882 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:07,890 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:07,903 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:07,917 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,917 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,920 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,925 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,936 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:07,937 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:07,937 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,937 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,937 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:07,945 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:07,958 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:07,972 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:07,972 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,975 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:07,980 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,989 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:07,991 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:07,992 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:07,992 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:07,992 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:07,992 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:08,002 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:08,015 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:08,028 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:08,028 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,031 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:08,036 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,040 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,047 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:08,048 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:08,048 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:08,048 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,048 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:08,053 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:08,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:08,080 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:08,080 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,083 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:08,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,099 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:08,099 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:08,099 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:08,099 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,099 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:08,105 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:08,120 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:08,134 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:08,134 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,137 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:08,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,151 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,153 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:08,154 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:08,154 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:08,154 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,154 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:08,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:08,173 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:08,189 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:08,189 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,192 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:08,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,201 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,205 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,208 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:08,208 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:08,208 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:08,209 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,209 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:08,219 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:08,234 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:08,249 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:08,249 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,252 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:08,257 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,261 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,266 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,268 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:08,269 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:08,269 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:08,269 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,269 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:08,279 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:08,280 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:08,295 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:08,295 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:08,298 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:08,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,307 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,312 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 08:52:08,314 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 08:52:08,315 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:08,315 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:08,315 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:08,315 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:08,334 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:08,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:08,375 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:08,376 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:08,377 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:08,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:08,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:08,947 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:08,948 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:08,949 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:08,949 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:08,949 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:08,949 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:08,949 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:08,988 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:09,027 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,027 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:09,028 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:09,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:09,032 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:09,034 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:09,036 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:09,038 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:09,038 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:09,038 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:09,038 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:09,040 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:09,045 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:09,085 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:09,085 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:09,086 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:09,087 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,089 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,090 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:09,090 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:09,090 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 24), torch.int64', '23')
2023-10-31 08:52:09,090 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:09,091 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:09,093 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:09,109 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:09,111 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 24), torch.int64', '23')
2023-10-31 08:52:09,112 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:09,112 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 24), torch.int64', '23'), {})
2023-10-31 08:52:09,114 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,115 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,117 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:09,119 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:09,119 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,119 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,119 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:09,121 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:09,134 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:09,148 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,148 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,151 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,155 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,160 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,166 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,167 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:09,167 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,167 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,167 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:09,172 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:09,185 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:09,198 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,198 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,201 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,206 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,215 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,217 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,218 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:09,218 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,218 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,218 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:09,223 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:09,236 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:09,250 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,250 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,253 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,263 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,267 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,270 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,270 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:09,270 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,270 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,271 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:09,275 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:09,288 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:09,301 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,302 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,304 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,309 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,314 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,319 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,321 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,321 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:09,322 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,322 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,322 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:09,327 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:09,340 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:09,353 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,354 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,356 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,361 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,365 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,372 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,373 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:09,373 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,373 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,373 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:09,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:09,391 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:09,404 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,405 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,407 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,412 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,417 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,424 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,424 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:09,424 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,425 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,425 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:09,430 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:09,443 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:09,456 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,456 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,459 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,474 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,476 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,477 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:09,477 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,477 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,477 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:09,482 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:09,497 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:09,510 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,510 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,513 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,518 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,527 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,529 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,530 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:09,530 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,530 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,530 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:09,536 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:09,551 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:09,565 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,565 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,568 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,573 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,582 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,585 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,585 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:09,585 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,585 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,586 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:09,590 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:09,605 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:09,621 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,621 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,624 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,629 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,641 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,641 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:09,641 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,642 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,642 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:09,650 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:09,664 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:09,680 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,680 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,683 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,699 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,700 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:09,700 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,700 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,700 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:09,704 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:09,705 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:09,721 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,721 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,724 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:09,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,732 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,737 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 08:52:09,739 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 08:52:09,740 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:09,740 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,740 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:09,740 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:09,747 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:09,787 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:09,788 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,788 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:09,789 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:09,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,793 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:09,794 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:09,794 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,794 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:09,794 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:09,795 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:09,833 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:09,873 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,873 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:09,874 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:09,876 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:09,878 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:09,880 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:09,882 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:09,883 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:09,884 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:09,884 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:09,884 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:09,885 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:09,890 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:09,931 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:09,931 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:09,932 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:09,933 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,935 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,936 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:09,936 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:09,936 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 25), torch.int64', '24')
2023-10-31 08:52:09,936 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:09,936 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:09,939 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:09,955 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:09,957 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 25), torch.int64', '24')
2023-10-31 08:52:09,958 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:09,958 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 25), torch.int64', '24'), {})
2023-10-31 08:52:09,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,961 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,962 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:09,963 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:09,965 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:09,965 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:09,965 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,965 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:09,969 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:09,982 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:09,995 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:09,996 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:09,998 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,003 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,007 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,016 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,017 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:10,017 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,017 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,017 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:10,021 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:10,037 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:10,053 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,053 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,056 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,061 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,066 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,074 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,074 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:10,074 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,074 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,074 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:10,080 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:10,097 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:10,113 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,113 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,116 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,121 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,127 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,135 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,136 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:10,136 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,136 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,136 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:10,140 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:10,156 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:10,172 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,172 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,175 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,179 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,184 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,192 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,192 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:10,192 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,193 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,193 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:10,197 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:10,213 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:10,228 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,229 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,233 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,238 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,243 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,248 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,251 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,251 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:10,251 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,252 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,252 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:10,255 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:10,270 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:10,284 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,284 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,287 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,292 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,296 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,300 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,303 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,303 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:10,303 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,303 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,304 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:10,307 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:10,322 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:10,336 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,337 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,343 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,348 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,352 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,357 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,359 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,360 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:10,360 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,360 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,360 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:10,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:10,383 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:10,397 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,398 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,401 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,409 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,415 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,424 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,425 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:10,425 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,425 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,425 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:10,435 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:10,464 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:10,480 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,480 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,483 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,503 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,506 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,507 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:10,507 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,507 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,507 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:10,516 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:10,532 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:10,548 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,548 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,551 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,557 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,562 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,580 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,580 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:10,581 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,581 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,581 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:10,597 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:10,614 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:10,633 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,633 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,638 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,650 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,659 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,660 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:10,660 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,660 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,660 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:10,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:10,670 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:10,688 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,689 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,694 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:10,701 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,707 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,714 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 08:52:10,718 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 08:52:10,718 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:10,718 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,718 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:10,719 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:10,739 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:10,784 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:10,785 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,785 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:10,787 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:10,788 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,792 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:10,793 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:10,793 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,793 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:10,794 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:10,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:10,835 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:10,877 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:10,877 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:10,878 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:10,881 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:10,883 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:10,885 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:10,887 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:10,889 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:10,889 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:10,889 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:10,890 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:10,890 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:10,896 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:10,939 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:10,939 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:10,940 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:10,942 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,943 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,945 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:10,946 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:10,946 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 26), torch.int64', '25')
2023-10-31 08:52:10,946 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:10,946 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:10,947 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:10,965 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:10,968 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 26), torch.int64', '25')
2023-10-31 08:52:10,968 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:10,969 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 26), torch.int64', '25'), {})
2023-10-31 08:52:10,971 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,974 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:10,975 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:10,977 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:10,977 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:10,978 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:10,978 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:10,982 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:10,997 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:11,011 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,011 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,015 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,026 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,034 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,035 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:11,035 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,035 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,035 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:11,043 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:11,058 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:11,072 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,072 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,077 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,082 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,087 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,093 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,096 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,096 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:11,096 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,097 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,097 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:11,107 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:11,121 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:11,135 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,136 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,139 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,145 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,156 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,158 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,159 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:11,159 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,159 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,159 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:11,167 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:11,181 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:11,195 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,195 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,199 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,205 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,218 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,219 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:11,219 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,219 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,219 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:11,229 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:11,243 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:11,257 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,258 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,262 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,268 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,279 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,282 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,282 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:11,282 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,282 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,283 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:11,291 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:11,304 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:11,318 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,318 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,322 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,327 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,338 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,341 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,341 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:11,342 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,342 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,342 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:11,349 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:11,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:11,377 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,377 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,381 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,386 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,392 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,397 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,401 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,401 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:11,401 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,401 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,401 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:11,416 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:11,432 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:11,448 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,448 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,453 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,473 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,477 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,477 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:11,477 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,477 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,477 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:11,492 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:11,510 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:11,526 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,526 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,530 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,536 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,549 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,552 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,553 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:11,553 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,553 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,553 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:11,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:11,579 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:11,602 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,602 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,607 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,613 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,619 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,625 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,628 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,628 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:11,628 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,628 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,629 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:11,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:11,659 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:11,676 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,676 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,680 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,690 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,699 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,699 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:11,699 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,699 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,700 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:11,708 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:11,709 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:11,726 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,727 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:11,731 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:11,737 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,748 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 08:52:11,750 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 08:52:11,751 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:11,751 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,751 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:11,752 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:11,767 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:11,808 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:11,810 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,810 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:11,811 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:11,812 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:11,814 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:11,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:11,816 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:11,816 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:11,816 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:11,817 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:11,817 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:11,817 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:11,857 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:11,911 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:11,911 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:11,914 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:11,916 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:11,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:11,921 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:11,923 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:11,925 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:11,925 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:11,926 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:11,926 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:11,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:11,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:11,976 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:11,976 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:11,978 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:11,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:11,980 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:11,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:11,983 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:11,984 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:11,984 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 27), torch.int64', '26')
2023-10-31 08:52:11,984 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:11,984 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:11,985 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:12,005 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:12,008 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 27), torch.int64', '26')
2023-10-31 08:52:12,008 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:12,009 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 27), torch.int64', '26'), {})
2023-10-31 08:52:12,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,014 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,015 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:12,018 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:12,018 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,018 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,019 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:12,023 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:12,038 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:12,053 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,053 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,057 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,074 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,077 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,077 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:12,078 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,078 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,078 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:12,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:12,102 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:12,117 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,118 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,121 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,127 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,138 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,141 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,142 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:12,142 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,142 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,142 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:12,152 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:12,166 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:12,181 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,181 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,185 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,190 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,196 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,205 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,205 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:12,205 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,205 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,206 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:12,214 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:12,230 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:12,244 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,244 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,248 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,253 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,259 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,268 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,268 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:12,268 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,268 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,269 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:12,278 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:12,293 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:12,307 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,307 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,311 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,316 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,323 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,329 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,332 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,332 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:12,332 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,332 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,333 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:12,341 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:12,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:12,369 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,369 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,373 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,383 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,392 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,392 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:12,392 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,392 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,392 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:12,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:12,418 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:12,432 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,433 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,436 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,442 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,453 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,455 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,456 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:12,456 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,456 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,456 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:12,464 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:12,479 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:12,494 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,494 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,498 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,517 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,520 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,520 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:12,520 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,520 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,521 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:12,530 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:12,546 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:12,562 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,562 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,565 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,576 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,582 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,585 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,585 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:12,585 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,585 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,585 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:12,591 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:12,607 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:12,623 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,623 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,627 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,633 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,646 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,647 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:12,647 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,647 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,647 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:12,659 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:12,674 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:12,690 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,690 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,694 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,699 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,705 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,711 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,714 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,714 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:12,714 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,714 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,715 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:12,720 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:12,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:12,737 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,737 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:12,741 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:12,747 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,752 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,758 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 08:52:12,761 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 08:52:12,761 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:12,761 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,761 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:12,761 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:12,777 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:12,820 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:12,822 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,822 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:12,823 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:12,825 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,828 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,829 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:12,829 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:12,829 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:12,829 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:12,829 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:12,830 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:12,869 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:12,910 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:12,910 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:12,912 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:12,915 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:12,918 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:12,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:12,922 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:12,923 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:12,924 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:12,924 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:12,924 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:12,925 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:12,930 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:12,971 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:12,972 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:12,973 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:12,974 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,975 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:12,977 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:12,977 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:12,977 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 28), torch.int64', '27')
2023-10-31 08:52:12,977 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:12,978 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:12,979 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:12,996 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:12,999 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 28), torch.int64', '27')
2023-10-31 08:52:12,999 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:13,000 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 28), torch.int64', '27'), {})
2023-10-31 08:52:13,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,002 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,004 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,005 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:13,007 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:13,007 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,007 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,008 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:13,012 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:13,025 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:13,038 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,039 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,042 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,047 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,057 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,059 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,060 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:13,060 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,060 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,060 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:13,068 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:13,081 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:13,097 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,097 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,100 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,106 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,119 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,119 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:13,119 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,120 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,120 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:13,129 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:13,142 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:13,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,157 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,160 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,165 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,169 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,174 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,177 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,177 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:13,177 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,178 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,178 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:13,186 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:13,199 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:13,213 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,213 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,217 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,229 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,235 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,237 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,238 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:13,238 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,238 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,238 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:13,248 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:13,262 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:13,276 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,276 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,280 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,293 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,301 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,302 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:13,302 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,302 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,302 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:13,310 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:13,324 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:13,339 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,339 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,342 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,348 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,362 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,363 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:13,363 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,363 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,363 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:13,373 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:13,387 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:13,401 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,401 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,405 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,412 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,419 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,424 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,427 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,428 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:13,428 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,428 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,428 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:13,437 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:13,452 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:13,466 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,466 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,470 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,482 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,490 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,491 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:13,491 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,491 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,491 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:13,501 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:13,517 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:13,532 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,532 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,536 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,548 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,556 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,556 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:13,556 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,557 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,557 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:13,565 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:13,581 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:13,597 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,597 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,601 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,618 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,621 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,622 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:13,622 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,622 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,622 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:13,642 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:13,657 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:13,673 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,673 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,677 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,683 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,689 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,697 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,697 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:13,698 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,698 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,698 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:13,706 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:13,707 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:13,723 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,723 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,727 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:13,733 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,738 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,744 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 08:52:13,747 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 08:52:13,747 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:13,747 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,747 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:13,747 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:13,763 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:13,804 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:13,805 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,805 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:13,806 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:13,808 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,809 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,811 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,812 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:13,812 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:13,812 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,812 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:13,812 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:13,813 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:13,853 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:13,905 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:13,905 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:13,906 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:13,909 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:13,911 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:13,913 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:13,915 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:13,917 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:13,917 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:13,917 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:13,917 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:13,918 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:13,924 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:13,964 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:13,964 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:13,965 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:13,966 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,967 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,968 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,969 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:13,970 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:13,970 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 29), torch.int64', '28')
2023-10-31 08:52:13,970 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:13,970 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:13,971 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:13,988 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:13,991 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 29), torch.int64', '28')
2023-10-31 08:52:13,991 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:13,992 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 29), torch.int64', '28'), {})
2023-10-31 08:52:13,993 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,996 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:13,997 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:13,999 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:13,999 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:13,999 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:13,999 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:14,004 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:14,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:14,030 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,030 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,033 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,048 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,051 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,051 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:14,052 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,052 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,052 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:14,060 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:14,073 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:14,087 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,087 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,090 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,095 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,100 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,105 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,107 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,108 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:14,108 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,108 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,108 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:14,114 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:14,128 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:14,141 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,142 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,145 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,154 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,159 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,162 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,162 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:14,162 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,163 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,163 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:14,167 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:14,180 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:14,194 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,194 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,197 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,203 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,208 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,213 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,216 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,216 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:14,216 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,216 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,216 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:14,226 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:14,239 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:14,253 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,253 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,256 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,261 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,266 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,271 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,274 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,274 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:14,274 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,274 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,275 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:14,279 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:14,292 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:14,306 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,306 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,309 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,314 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,319 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,324 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,326 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,327 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:14,327 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,327 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,327 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:14,333 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:14,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:14,361 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,361 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,364 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,374 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,380 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,382 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,383 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:14,383 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,383 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,383 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:14,388 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:14,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:14,418 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,418 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,421 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,427 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,437 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,440 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,440 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:14,440 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,441 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,441 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:14,450 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:14,467 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:14,481 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,482 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,485 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,494 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,499 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,502 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,503 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:14,503 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,503 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,503 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:14,511 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:14,526 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:14,541 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,542 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,545 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,561 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,562 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:14,562 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,562 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,562 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:14,573 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:14,588 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:14,603 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,604 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,606 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,611 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,616 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,620 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,623 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,623 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:14,624 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,624 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,624 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:14,628 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:14,629 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:14,644 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,644 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,647 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,652 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,664 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 08:52:14,667 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 08:52:14,668 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:14,668 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,668 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:14,668 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:14,682 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:14,722 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:14,723 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,724 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:14,724 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:14,726 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,729 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:14,729 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:14,729 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,729 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:14,729 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:14,730 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:14,768 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:14,808 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,809 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:14,810 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:14,812 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:14,814 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:14,816 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:14,818 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:14,819 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:14,819 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:14,819 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:14,820 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:14,820 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:14,826 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:14,868 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:14,868 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:14,870 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:14,871 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,874 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:14,874 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:14,875 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 30), torch.int64', '29')
2023-10-31 08:52:14,875 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:14,875 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:14,877 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:14,894 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:14,897 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 30), torch.int64', '29')
2023-10-31 08:52:14,897 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:14,898 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 30), torch.int64', '29'), {})
2023-10-31 08:52:14,899 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,901 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,902 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:14,903 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:14,905 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:14,905 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,905 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,906 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:14,910 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:14,923 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:14,937 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,937 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,940 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:14,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:14,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:14,956 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:14,958 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:14,959 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:14,959 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:14,959 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,959 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:14,967 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:14,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:14,994 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:14,994 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:14,997 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,003 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,007 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,015 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,016 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:15,016 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,016 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,016 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:15,025 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:15,039 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:15,053 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,053 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,056 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,061 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,066 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,074 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,074 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:15,074 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,074 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,074 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:15,082 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:15,097 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:15,110 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,110 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,114 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,119 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,124 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,129 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,132 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,132 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:15,133 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,133 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,133 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:15,142 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:15,156 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:15,169 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,170 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,173 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,183 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,191 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,191 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:15,191 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,191 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,191 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:15,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:15,213 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:15,227 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,227 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,230 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,235 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,240 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,248 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,248 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:15,248 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,248 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,248 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:15,258 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:15,271 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:15,285 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,285 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,288 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,293 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,298 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,306 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,306 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:15,306 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,306 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,306 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:15,314 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:15,329 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:15,343 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,343 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,346 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,351 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,356 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,361 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,364 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,364 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:15,364 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,365 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,365 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:15,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:15,390 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:15,404 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,405 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,408 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,414 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,419 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,424 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,427 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,427 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:15,427 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,427 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,427 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:15,435 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:15,451 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:15,466 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,466 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,469 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,475 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,479 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,484 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,487 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,487 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:15,488 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,488 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,488 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:15,503 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:15,518 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:15,533 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,534 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,537 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,547 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,552 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,555 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,555 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:15,555 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,555 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,555 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:15,563 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:15,564 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:15,579 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,579 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,582 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,587 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,592 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,597 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 08:52:15,600 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 08:52:15,600 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:15,600 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,600 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:15,600 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:15,614 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:15,653 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:15,654 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,655 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:15,655 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:15,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,658 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,659 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,659 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:15,660 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:15,660 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,660 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:15,660 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:15,661 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:15,699 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:15,738 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,738 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:15,739 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:15,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:15,744 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:15,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:15,748 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:15,749 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:15,750 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:15,750 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:15,750 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:15,752 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:15,756 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:15,797 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:15,797 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:15,798 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:15,799 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,800 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,802 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,802 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:15,803 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:15,803 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 31), torch.int64', '30')
2023-10-31 08:52:15,803 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:15,803 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:15,804 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:15,822 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:15,824 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 31), torch.int64', '30')
2023-10-31 08:52:15,824 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:15,825 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 31), torch.int64', '30'), {})
2023-10-31 08:52:15,827 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,828 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:15,830 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:15,832 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:15,832 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,833 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,833 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:15,835 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:15,848 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:15,861 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,861 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,864 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,869 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,882 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:15,882 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:15,882 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,882 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,883 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:15,887 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:15,900 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:15,914 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,914 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,917 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,927 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,932 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,935 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:15,935 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:15,935 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,935 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,935 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:15,941 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:15,954 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:15,967 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:15,968 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,971 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:15,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,980 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:15,988 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:15,988 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:15,988 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:15,989 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:15,989 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:15,997 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:16,010 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:16,023 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,024 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,026 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,032 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,037 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,042 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,044 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,045 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:16,045 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,045 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,045 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:16,051 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:16,064 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:16,078 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,078 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,081 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,086 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,098 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,099 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:16,099 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,099 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,099 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:16,107 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:16,120 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:16,134 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,134 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,137 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,147 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,154 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,155 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:16,155 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,155 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,155 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:16,161 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:16,175 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:16,188 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,189 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,191 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,207 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,209 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,210 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:16,210 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,210 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,210 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:16,216 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:16,230 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:16,244 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,244 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,247 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,252 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,257 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,262 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,265 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,265 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:16,265 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,265 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,266 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:16,274 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:16,290 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:16,305 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,305 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,308 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,318 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,323 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,326 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,326 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:16,327 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,327 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,327 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:16,332 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:16,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:16,362 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,363 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,366 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,371 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,376 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,384 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,384 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:16,384 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,384 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,384 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:16,397 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:16,413 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:16,429 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,429 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,432 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,437 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,442 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,450 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,450 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:16,451 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,451 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,451 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:16,456 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:16,457 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:16,472 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,473 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,476 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,481 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,486 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,491 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 08:52:16,494 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 08:52:16,494 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:16,494 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,494 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:16,494 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:16,503 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:16,544 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:16,546 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,546 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:16,547 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:16,548 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,551 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,552 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:16,552 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:16,552 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,552 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:16,552 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:16,553 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:16,591 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:16,631 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,631 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:16,632 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:16,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:16,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:16,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:16,640 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:16,641 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:16,642 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:16,642 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:16,642 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:16,643 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:16,648 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:16,688 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:16,688 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:16,689 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:16,690 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,691 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,693 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:16,694 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:16,694 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 32), torch.int64', '31')
2023-10-31 08:52:16,694 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:16,694 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:16,695 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:16,713 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:16,715 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 32), torch.int64', '31')
2023-10-31 08:52:16,715 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:16,716 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 32), torch.int64', '31'), {})
2023-10-31 08:52:16,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,720 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:16,721 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:16,723 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:16,724 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,724 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,724 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:16,726 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:16,742 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:16,755 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,756 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,759 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,764 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,769 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,773 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,776 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:16,776 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:16,777 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,777 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,777 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:16,785 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:16,798 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:16,812 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,812 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,815 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,820 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,824 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,832 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:16,832 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:16,832 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,832 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,833 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:16,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:16,855 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:16,869 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,869 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,872 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,884 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,888 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,891 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:16,891 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:16,892 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,892 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,892 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:16,900 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:16,913 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:16,926 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,926 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,929 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,939 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,946 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:16,947 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:16,947 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:16,947 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,947 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:16,957 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:16,970 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:16,985 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:16,986 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:16,989 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:16,995 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:16,999 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,004 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,007 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,007 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:17,008 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,008 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,008 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:17,016 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:17,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:17,043 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,043 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,046 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,051 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,055 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,060 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,063 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,063 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:17,064 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,064 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,064 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:17,073 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:17,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:17,100 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,100 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,103 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,108 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,112 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,120 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,120 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:17,120 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,120 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,121 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:17,128 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:17,143 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:17,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,156 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,159 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,169 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,174 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,177 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,177 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:17,177 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,177 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,177 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:17,187 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:17,202 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:17,217 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,217 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,220 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,225 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,229 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,237 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,237 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:17,238 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,238 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,238 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:17,246 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:17,261 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:17,277 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,278 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,281 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,286 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,291 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,296 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,298 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,299 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:17,299 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,299 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,299 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:17,314 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:17,328 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:17,344 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,344 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,347 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,352 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,356 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,362 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,365 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,365 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:17,365 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,365 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,365 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:17,373 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:17,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:17,389 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,389 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,392 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,398 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,402 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,407 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 08:52:17,410 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 08:52:17,410 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:17,410 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,411 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:17,411 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:17,426 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:17,465 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:17,466 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,467 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:17,467 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:17,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,470 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,472 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:17,472 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:17,472 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,473 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:17,473 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:17,473 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:17,512 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:17,551 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,551 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:17,552 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:17,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:17,557 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:17,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:17,561 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:17,562 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:17,562 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:17,562 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:17,563 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:17,564 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:17,569 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:17,610 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:17,610 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:17,611 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:17,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,613 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,614 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,615 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:17,615 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:17,615 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 33), torch.int64', '32')
2023-10-31 08:52:17,615 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:17,616 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:17,617 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:17,634 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:17,637 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 33), torch.int64', '32')
2023-10-31 08:52:17,637 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:17,638 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 33), torch.int64', '32'), {})
2023-10-31 08:52:17,639 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,641 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,642 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:17,643 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:17,645 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:17,645 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,645 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,646 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:17,650 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:17,663 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:17,676 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,676 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,680 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,690 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,697 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:17,698 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:17,698 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,698 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,698 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:17,703 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:17,716 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:17,729 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,730 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,733 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,738 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,747 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,750 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:17,750 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:17,751 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,751 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,751 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:17,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:17,771 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:17,785 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,785 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,788 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,794 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,799 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,804 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,807 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:17,807 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:17,808 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,808 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,808 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:17,817 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:17,830 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:17,843 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,843 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,846 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,865 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:17,865 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:17,865 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,865 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,865 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:17,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:17,888 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:17,902 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,902 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,905 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,910 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,915 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,923 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:17,923 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:17,924 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,924 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,924 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:17,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:17,945 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:17,958 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:17,958 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,961 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:17,966 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,971 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:17,979 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:17,979 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:17,979 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:17,979 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:17,979 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:17,989 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:18,002 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:18,015 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,016 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,019 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,024 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,029 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,034 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,036 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:18,037 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:18,037 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,037 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,037 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:18,045 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:18,059 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:18,072 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,073 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,076 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,086 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,093 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:18,093 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:18,094 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,094 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,094 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:18,101 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:18,117 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:18,131 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,132 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,134 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,153 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:18,154 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:18,154 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,154 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,154 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:18,163 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:18,178 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:18,194 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,194 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,197 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,207 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,212 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,214 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:18,214 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:18,215 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,215 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,215 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:18,223 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:18,238 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:18,253 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,253 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,256 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,261 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,266 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,271 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,273 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:18,274 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:18,274 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,274 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,274 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:18,279 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:18,280 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:18,295 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,296 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,299 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,308 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 08:52:18,316 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 08:52:18,316 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:18,316 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,317 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:18,317 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:18,324 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:18,364 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:18,365 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,365 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:18,366 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:18,367 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,370 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:18,371 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:18,371 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,371 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:18,371 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:18,372 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:18,410 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:18,450 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,450 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:18,451 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:18,453 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:18,455 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:18,457 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:18,459 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:18,461 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:18,461 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:18,461 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:18,461 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:18,463 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:18,468 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:18,508 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:18,509 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:18,509 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:18,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,513 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,514 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:18,514 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:18,514 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 34), torch.int64', '33')
2023-10-31 08:52:18,514 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:18,514 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:18,515 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:18,533 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:18,536 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 34), torch.int64', '33')
2023-10-31 08:52:18,536 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:18,537 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 34), torch.int64', '33'), {})
2023-10-31 08:52:18,538 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,541 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:18,542 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:18,544 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:18,544 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,544 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,544 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:18,549 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:18,562 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:18,575 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,575 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,578 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,583 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,588 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,593 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,596 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,596 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:18,596 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,596 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,597 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:18,601 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:18,615 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:18,628 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,628 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,631 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,642 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,649 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,650 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:18,650 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,650 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,650 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:18,655 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:18,668 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:18,682 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,682 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,685 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,691 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,700 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,703 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,703 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:18,703 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,703 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,704 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:18,708 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:18,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:18,735 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,735 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,738 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,743 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,748 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,753 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,755 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,756 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:18,756 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,756 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,756 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:18,765 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:18,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:18,791 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,792 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,795 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,800 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,810 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,812 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,813 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:18,813 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,813 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,813 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:18,818 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:18,831 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:18,845 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,845 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,848 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,853 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,863 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,866 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,866 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:18,866 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,867 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,867 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:18,873 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:18,888 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:18,901 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,901 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,904 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,909 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,914 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,922 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,922 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:18,922 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,922 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,922 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:18,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:18,941 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:18,955 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:18,955 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,958 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:18,963 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,968 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,973 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:18,975 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:18,976 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:18,976 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:18,976 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:18,976 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:18,982 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:18,997 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:19,011 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,012 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,015 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,025 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,032 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:19,033 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:19,033 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,033 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,033 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:19,038 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:19,053 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:19,068 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,068 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,071 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,077 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,086 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,089 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:19,089 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:19,090 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,090 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,090 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:19,098 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:19,113 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:19,129 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,129 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,132 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,138 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,147 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,150 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:19,150 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:19,150 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,151 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,151 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:19,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:19,159 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:19,175 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,175 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,178 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,183 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 08:52:19,196 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 08:52:19,196 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:19,196 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,196 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:19,196 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:19,205 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:19,244 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:19,245 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,246 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:19,246 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:19,248 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,249 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,250 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,251 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:19,251 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:19,251 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,251 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:19,251 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:19,252 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:19,290 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:19,330 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,330 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:19,331 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:19,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:19,335 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:19,337 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:19,339 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:19,340 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:19,341 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:19,341 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:19,341 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:19,343 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:19,348 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:19,388 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:19,388 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:19,389 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:19,390 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,391 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,392 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,393 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:19,394 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:19,394 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 35), torch.int64', '34')
2023-10-31 08:52:19,394 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:19,394 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:19,395 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:19,416 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:19,419 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 35), torch.int64', '34')
2023-10-31 08:52:19,419 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:19,420 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 35), torch.int64', '34'), {})
2023-10-31 08:52:19,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,422 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,424 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:19,425 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:19,427 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:19,427 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,427 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,428 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:19,432 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:19,446 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:19,460 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,460 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,463 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,473 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,478 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,481 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,481 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:19,481 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,481 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,482 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:19,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:19,503 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:19,516 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,517 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,520 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,525 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,534 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,537 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,537 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:19,538 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,538 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,538 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:19,547 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:19,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:19,574 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,574 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,577 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,583 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,587 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,592 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,595 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,595 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:19,596 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,596 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,596 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:19,604 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:19,617 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:19,630 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,630 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,633 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,651 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,651 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:19,651 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,652 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,652 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:19,661 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:19,674 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:19,688 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,688 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,691 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,701 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,706 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,712 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,712 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:19,712 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,712 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,713 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:19,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:19,734 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:19,748 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,748 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,751 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,756 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,761 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,766 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,769 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,769 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:19,769 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,769 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,770 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:19,779 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:19,793 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:19,806 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,806 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,810 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,820 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,829 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,829 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:19,829 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,829 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,829 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:19,837 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:19,852 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:19,865 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,866 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,869 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,884 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,887 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,887 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:19,887 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,887 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,887 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:19,897 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:19,912 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:19,927 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,927 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,930 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,935 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,940 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,945 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:19,948 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:19,948 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:19,948 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:19,948 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,949 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:19,956 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:19,972 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:19,987 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:19,987 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:19,990 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:19,996 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,000 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,005 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,008 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:20,008 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:20,008 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,009 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,009 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:20,024 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:20,041 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:20,056 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,056 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,059 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,078 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:20,079 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:20,079 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,079 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,079 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:20,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:20,088 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:20,103 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,103 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,106 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,112 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 08:52:20,124 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 08:52:20,125 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:20,125 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,125 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:20,125 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:20,144 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:20,184 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:20,185 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,185 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:20,186 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:20,187 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,190 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,190 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:20,191 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:20,191 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,191 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:20,191 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:20,192 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:20,230 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:20,270 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,270 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:20,271 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:20,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:20,275 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:20,277 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:20,279 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:20,281 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:20,281 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:20,281 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:20,282 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:20,282 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:20,288 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:20,328 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:20,328 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:20,329 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:20,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,332 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,334 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:20,334 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:20,334 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 36), torch.int64', '35')
2023-10-31 08:52:20,334 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:20,335 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:20,337 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:20,354 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:20,357 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 36), torch.int64', '35')
2023-10-31 08:52:20,357 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:20,358 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 36), torch.int64', '35'), {})
2023-10-31 08:52:20,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,361 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,362 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:20,363 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:20,365 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:20,365 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,366 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,366 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:20,370 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:20,383 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:20,396 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,397 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,400 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,405 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,410 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,416 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,419 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,419 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:20,419 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,419 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,419 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:20,427 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:20,441 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:20,454 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,454 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,457 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,462 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,474 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,475 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:20,475 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,475 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,475 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:20,481 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:20,494 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:20,509 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,509 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,512 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,517 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,527 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,529 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,530 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:20,530 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,530 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,530 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:20,538 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:20,551 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:20,565 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,565 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,568 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,573 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,583 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,585 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,585 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:20,586 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,586 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,586 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:20,592 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:20,605 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:20,618 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,619 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,622 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,627 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,632 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,637 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,639 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,640 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:20,640 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,640 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,640 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:20,649 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:20,662 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:20,675 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,675 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,678 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,683 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,693 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,695 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,696 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:20,696 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,696 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,696 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:20,702 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:20,715 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:20,728 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,728 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,731 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,736 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,741 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,748 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,749 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:20,749 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,749 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,749 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:20,754 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:20,768 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:20,782 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,782 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,785 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,795 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,800 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,802 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,803 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:20,803 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,803 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,803 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:20,811 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:20,826 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:20,841 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,841 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,844 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,854 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,859 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,861 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,862 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:20,862 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,862 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,862 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:20,869 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:20,884 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:20,899 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,899 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,902 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,907 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,917 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,919 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,920 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:20,920 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,920 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,920 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:20,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:20,946 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:20,962 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:20,962 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,965 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:20,970 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,974 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:20,982 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:20,982 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:20,982 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:20,982 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:20,983 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:20,988 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:20,989 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:21,004 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,004 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,007 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:21,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:21,022 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 08:52:21,024 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 08:52:21,025 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:21,025 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,025 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:21,025 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:21,032 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:21,072 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:21,073 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,073 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:21,074 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:21,075 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,077 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,078 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:21,078 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:21,078 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,078 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:21,078 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:21,079 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:21,117 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:21,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,157 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:21,158 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:21,160 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:21,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:21,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:21,166 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:21,167 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:21,168 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:21,168 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:21,168 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:21,169 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:21,174 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:21,215 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:21,215 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:21,216 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:21,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,218 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,219 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,220 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:21,221 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:21,221 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 37), torch.int64', '36')
2023-10-31 08:52:21,221 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:21,221 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:21,223 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:21,240 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:21,242 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 37), torch.int64', '36')
2023-10-31 08:52:21,242 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:21,243 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 37), torch.int64', '36'), {})
2023-10-31 08:52:21,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,246 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,248 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,248 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:21,250 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:21,251 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,251 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,251 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:21,253 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:21,267 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:21,280 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,281 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,284 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,289 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,294 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,302 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,302 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:21,302 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,302 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,302 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:21,307 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:21,321 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:21,334 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,334 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,337 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,347 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,352 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,355 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,355 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:21,356 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,356 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,356 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:21,361 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:21,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:21,388 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,388 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,391 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,397 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,406 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,409 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,409 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:21,409 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,410 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,410 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:21,416 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:21,429 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:21,442 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,443 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,446 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,451 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,456 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,461 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,464 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,464 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:21,464 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,464 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,464 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:21,470 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:21,484 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:21,497 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,497 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,500 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,510 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,518 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,518 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:21,519 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,519 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,519 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:21,526 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:21,540 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:21,554 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,554 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,557 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,563 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,568 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,573 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,576 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,576 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:21,576 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,576 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,577 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:21,582 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:21,596 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:21,609 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,609 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,612 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,618 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,623 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,628 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,631 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,631 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:21,631 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,631 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,632 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:21,636 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:21,651 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:21,665 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,665 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,668 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,679 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,684 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,687 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,687 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:21,687 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,687 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,687 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:21,693 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:21,709 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:21,724 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,724 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,727 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,732 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,737 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,744 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,745 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:21,745 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,745 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,745 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:21,750 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:21,765 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:21,780 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,781 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,783 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,789 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,798 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,801 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,801 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:21,801 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,801 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,802 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:21,815 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:21,829 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:21,844 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,845 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,848 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,853 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,865 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,865 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:21,865 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,865 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,865 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:21,870 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:21,872 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:21,889 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,889 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:21,892 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:21,897 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,902 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,907 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 08:52:21,910 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 08:52:21,910 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:21,910 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,910 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:21,910 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:21,918 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:21,957 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:21,958 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:21,958 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:21,959 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:21,961 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,962 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,963 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:21,964 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:21,964 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:21,964 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:21,964 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:21,964 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:21,965 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:22,003 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:22,043 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,043 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:22,044 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:22,046 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:22,049 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:22,050 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:22,052 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:22,054 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 08:52:22,054 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 08:52:22,054 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:22,054 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 08:52:22,055 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:22,060 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:22,101 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 08:52:22,101 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:22,102 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 08:52:22,103 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,104 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,105 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,106 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:22,106 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 08:52:22,106 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 38), torch.int64', '37')
2023-10-31 08:52:22,106 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:22,107 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 08:52:22,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:22,125 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 08:52:22,128 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 38), torch.int64', '37')
2023-10-31 08:52:22,128 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:22,129 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 38), torch.int64', '37'), {})
2023-10-31 08:52:22,130 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,131 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,134 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:22,136 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 08:52:22,136 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,136 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,136 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 08:52:22,138 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:22,151 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 08:52:22,165 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,165 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,168 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,177 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,182 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,185 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,185 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 08:52:22,185 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,185 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,186 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 08:52:22,191 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:22,205 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 08:52:22,218 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,218 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,221 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,226 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,231 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,236 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,239 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,239 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 08:52:22,240 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,240 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,240 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 08:52:22,245 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:22,259 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 08:52:22,273 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,273 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,276 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,281 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,285 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,290 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,293 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,293 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 08:52:22,294 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,294 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,294 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 08:52:22,298 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:22,312 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 08:52:22,325 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,325 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,328 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,338 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,345 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,346 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 08:52:22,346 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,346 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,346 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 08:52:22,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:22,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 08:52:22,381 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,382 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,384 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,394 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,399 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,402 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,402 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 08:52:22,402 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,402 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,402 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 08:52:22,407 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:22,420 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 08:52:22,434 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,434 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,437 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,442 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,452 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,455 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,455 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 08:52:22,455 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,455 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,456 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 08:52:22,461 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:22,474 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 08:52:22,488 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,488 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,491 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,501 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,509 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,509 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 08:52:22,510 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,510 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,510 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 08:52:22,514 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:22,529 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 08:52:22,542 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,542 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,545 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,562 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,562 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 08:52:22,563 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,563 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,563 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 08:52:22,569 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:22,584 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 08:52:22,598 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,599 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,602 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,617 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,624 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,625 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 08:52:22,625 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,625 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,625 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 08:52:22,634 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:22,651 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 08:52:22,666 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,667 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,670 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,675 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,680 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,684 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,687 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,687 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 08:52:22,688 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,688 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,688 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 08:52:22,702 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:22,717 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 08:52:22,733 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,733 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,736 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,741 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,751 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,754 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,754 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 08:52:22,754 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,754 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,755 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 08:52:22,763 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:22,764 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 08:52:22,780 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,780 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 08:52:22,783 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 08:52:22,789 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,794 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,800 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 08:52:22,803 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 08:52:22,803 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 08:52:22,803 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,803 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:22,804 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 08:52:22,825 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:22,866 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 08:52:22,868 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,868 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:22,869 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:22,870 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 08:52:22,874 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 08:52:22,874 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 08:52:22,874 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 08:52:22,874 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 08:52:22,874 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 08:52:22,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 08:52:22,915 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 08:52:22,956 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 08:52:22,956 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 08:52:22,957 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 08:52:22,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:22,962 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:22,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 08:52:22,966 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 08:52:22,970 [test.py:45 in test_hf_gen] INFO - for i in range(10):                               
2023-10-31 08:52:22,970 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 08:52:22,970 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-31 08:52:22,970 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 08:52:22,970 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 08:52:22,970 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 08:52:22,970 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-31 08:52:22,971 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 08:52:23,000 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-31 08:52:23,000 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-31 08:52:23,001 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-31 08:52:23,002 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-31 08:52:23,002 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-31 08:52:23,002 [wrapper.py:88 in layer_reset] DEBUG - lm_head from flexgen to old.
