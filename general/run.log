2023-10-16 07:15:34,123 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp5j8srrn8
2023-10-16 07:15:34,124 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp5j8srrn8/_remote_module_non_scriptable.py
2023-10-16 07:15:34,560 [model.py:62 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-6.7b
2023-10-16 07:15:34,562 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-16 07:15:34,622 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/config.json HTTP/1.1" 200 0
2023-10-16 07:15:36,113 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-16 07:15:36,401 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-16 07:15:36,402 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-16 07:15:36,402 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-16 07:15:36,402 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-16 07:15:37,412 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/config.json HTTP/1.1" 200 0
2023-10-16 07:15:37,679 [model.py:170 in download] INFO - downloading from hugging face...
2023-10-16 07:15:37,725 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/config.json HTTP/1.1" 200 0
2023-10-16 07:17:38,353 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/generation_config.json HTTP/1.1" 200 0
2023-10-16 07:17:38,402 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/config.json HTTP/1.1" 200 0
2023-10-16 07:17:38,655 [model.py:188 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-6.7b'
2023-10-16 07:17:38,673 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 6452559872
2023-10-16 07:17:38,673 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 6444163072
2023-10-16 07:17:38,674 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 6444154880
2023-10-16 07:17:38,675 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.16147563 0.83852437], size_todo: 6242775040
2023-10-16 07:17:38,676 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.21755148 0.78244852], size_todo: 6041395200
2023-10-16 07:17:38,677 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.24603266 0.75396734], size_todo: 5840015360
2023-10-16 07:17:38,678 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.26326591 0.73673409], size_todo: 5638635520
2023-10-16 07:17:38,679 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.         0.27481561 0.72518439], size_todo: 5437255680
2023-10-16 07:17:38,680 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.         0.28309541 0.71690459], size_todo: 5235875840
2023-10-16 07:17:38,681 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.28932175 0.71067825], size_todo: 5034496000
2023-10-16 07:17:38,682 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.29417427 0.70582573], size_todo: 4833116160
2023-10-16 07:17:38,683 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.29806248 0.70193752], size_todo: 4631736320
2023-10-16 07:17:38,684 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.30124785 0.69875215], size_todo: 4430356480
2023-10-16 07:17:38,685 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.30390516 0.69609484], size_todo: 4228976640
2023-10-16 07:17:38,686 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30615565 0.69384435], size_todo: 4027596800
2023-10-16 07:17:38,686 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.12, [0.         0.30808612 0.69191388], size_todo: 3826216960
2023-10-16 07:17:38,687 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.13, [0.         0.30976029 0.69023971], size_todo: 3624837120
2023-10-16 07:17:38,688 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.14, [0.         0.31122602 0.68877398], size_todo: 3423457280
2023-10-16 07:17:38,689 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.15, [0.         0.30763775 0.69236225], size_todo: 3222077440
2023-10-16 07:17:38,690 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.16, [0.         0.30905871 0.69094129], size_todo: 3020697600
2023-10-16 07:17:38,691 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.17, [0.         0.30596056 0.69403944], size_todo: 2819317760
2023-10-16 07:17:38,692 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.18, [0.         0.30732347 0.69267653], size_todo: 2617937920
2023-10-16 07:17:38,693 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.19, [0.         0.30460186 0.69539814], size_todo: 2416558080
2023-10-16 07:17:38,694 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.20, [0.         0.30590281 0.69409719], size_todo: 2215178240
2023-10-16 07:17:38,695 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.21, [0.        0.3034788 0.6965212], size_todo: 2013798400
2023-10-16 07:17:38,696 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.22, [0.         0.30471829 0.69528171], size_todo: 1812418560
2023-10-16 07:17:38,697 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.23, [0.         0.30253497 0.69746503], size_todo: 1611038720
2023-10-16 07:17:38,698 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.24, [0.         0.30371556 0.69628444], size_todo: 1409658880
2023-10-16 07:17:38,699 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.25, [0.         0.30173063 0.69826937], size_todo: 1208279040
2023-10-16 07:17:38,699 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.26, [0.         0.30285575 0.69714425], size_todo: 1006899200
2023-10-16 07:17:38,701 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.27, [0.         0.30103699 0.69896301], size_todo: 805519360
2023-10-16 07:17:38,702 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.28, [0.         0.30211033 0.69788967], size_todo: 604139520
2023-10-16 07:17:38,703 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.29, [0.         0.30043267 0.69956733], size_todo: 402759680
2023-10-16 07:17:38,704 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.30, [0.         0.30145791 0.69854209], size_todo: 201379840
2023-10-16 07:17:38,705 [model.py:140 in get_policy_weight_map] DEBUG - model.decoder.layers.31, [0.         0.29990145 0.70009855], size_todo: 0
2023-10-16 07:17:38,705 [model.py:140 in get_policy_weight_map] DEBUG - lm_head, [0.         0.29990145 0.70009855], size_todo: 0
2023-10-16 07:17:38,706 [model.py:144 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-16 07:17:38,711 [model.py:150 in get_policy_weight_map] INFO - CausalLM facebook/opt-6.7b is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 3.72 GiB (29.99%), Disk Mem 8.68 Gib (70.01%)
2023-10-16 07:17:38,712 [model.py:247 in init_all_weights] DEBUG - init all weights...
2023-10-16 07:17:38,760 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-16 07:17:38,760 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-16 07:17:38,760 [forward.py:48 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-16 07:17:38,760 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-16 07:17:38,760 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-16 07:17:38,761 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-16 07:17:38,761 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-16 07:17:38,761 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-16 07:17:38,761 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-16 07:17:38,761 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-16 07:17:38,761 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-16 07:17:38,762 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-16 07:17:38,762 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-16 07:17:38,762 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-16 07:17:38,762 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-16 07:17:38,762 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.12 to test forward
2023-10-16 07:17:38,762 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.13 to test forward
2023-10-16 07:17:38,762 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.14 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.15 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.16 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.17 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.18 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.19 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.20 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.21 to test forward
2023-10-16 07:17:38,763 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.22 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.23 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.24 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.25 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.26 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.27 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.28 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.29 to test forward
2023-10-16 07:17:38,764 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.30 to test forward
2023-10-16 07:17:38,765 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.31 to test forward
2023-10-16 07:17:38,765 [forward.py:48 in to_test_forward] DEBUG - lm_head to test forward
2023-10-16 07:17:38,827 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-16 07:17:38,828 [_api.py:172 in acquire] DEBUG - Attempting to acquire lock 139996334709808 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/27c24ca9d908d0b678b20c698aeb9e950c44d865.lock
2023-10-16 07:17:38,829 [_api.py:176 in acquire] DEBUG - Lock 139996334709808 acquired on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/27c24ca9d908d0b678b20c698aeb9e950c44d865.lock
2023-10-16 07:17:38,870 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "GET /facebook/opt-6.7b/resolve/main/tokenizer_config.json HTTP/1.1" 200 685
2023-10-16 07:17:38,873 [_api.py:209 in release] DEBUG - Attempting to release lock 139996334709808 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/27c24ca9d908d0b678b20c698aeb9e950c44d865.lock
2023-10-16 07:17:38,873 [_api.py:212 in release] DEBUG - Lock 139996334709808 released on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/27c24ca9d908d0b678b20c698aeb9e950c44d865.lock
2023-10-16 07:17:39,086 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/vocab.json HTTP/1.1" 200 0
2023-10-16 07:17:39,087 [_api.py:172 in acquire] DEBUG - Attempting to acquire lock 139996334717200 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/0a39732b2d8be8e493cab3da68b68cc3e28221de.lock
2023-10-16 07:17:39,087 [_api.py:176 in acquire] DEBUG - Lock 139996334717200 acquired on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/0a39732b2d8be8e493cab3da68b68cc3e28221de.lock
2023-10-16 07:17:39,180 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "GET /facebook/opt-6.7b/resolve/main/vocab.json HTTP/1.1" 200 898822
2023-10-16 07:17:39,276 [_api.py:209 in release] DEBUG - Attempting to release lock 139996334717200 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/0a39732b2d8be8e493cab3da68b68cc3e28221de.lock
2023-10-16 07:17:39,276 [_api.py:212 in release] DEBUG - Lock 139996334717200 released on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/0a39732b2d8be8e493cab3da68b68cc3e28221de.lock
2023-10-16 07:17:39,320 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/merges.txt HTTP/1.1" 200 0
2023-10-16 07:17:39,321 [_api.py:172 in acquire] DEBUG - Attempting to acquire lock 139996334718064 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2023-10-16 07:17:39,321 [_api.py:176 in acquire] DEBUG - Lock 139996334718064 acquired on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2023-10-16 07:17:39,366 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "GET /facebook/opt-6.7b/resolve/main/merges.txt HTTP/1.1" 200 456318
2023-10-16 07:17:39,381 [_api.py:209 in release] DEBUG - Attempting to release lock 139996334718064 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2023-10-16 07:17:39,381 [_api.py:212 in release] DEBUG - Lock 139996334718064 released on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2023-10-16 07:17:39,426 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/tokenizer.json HTTP/1.1" 404 0
2023-10-16 07:17:39,470 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/added_tokens.json HTTP/1.1" 404 0
2023-10-16 07:17:39,511 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/special_tokens_map.json HTTP/1.1" 200 0
2023-10-16 07:17:39,512 [_api.py:172 in acquire] DEBUG - Attempting to acquire lock 139996334710240 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/5dfa36546b8eddce0e04df3133c30df43fcc3828.lock
2023-10-16 07:17:39,512 [_api.py:176 in acquire] DEBUG - Lock 139996334710240 acquired on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/5dfa36546b8eddce0e04df3133c30df43fcc3828.lock
2023-10-16 07:17:39,555 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "GET /facebook/opt-6.7b/resolve/main/special_tokens_map.json HTTP/1.1" 200 441
2023-10-16 07:17:39,557 [_api.py:209 in release] DEBUG - Attempting to release lock 139996334710240 on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/5dfa36546b8eddce0e04df3133c30df43fcc3828.lock
2023-10-16 07:17:39,557 [_api.py:212 in release] DEBUG - Lock 139996334710240 released on /home/fsuser/.cache/huggingface/hub/models--facebook--opt-6.7b/blobs/5dfa36546b8eddce0e04df3133c30df43fcc3828.lock
2023-10-16 07:17:39,739 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:17:39,743 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:17:39,744 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:17:39,746 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:17:39,747 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:17:40,692 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:17:40,713 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:17:41,621 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:17:41,638 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:17:42,519 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:17:42,534 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:17:43,431 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:17:43,447 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:17:44,331 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:17:44,350 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:17:45,304 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:17:45,320 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:17:46,238 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:17:46,254 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:17:47,281 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:17:47,296 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:17:48,226 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:17:48,246 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:17:49,174 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:17:49,190 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:17:50,093 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:17:50,110 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:17:51,002 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:17:51,022 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:17:51,946 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:17:51,962 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:17:52,856 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:17:52,872 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:17:53,778 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:17:53,794 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:17:54,680 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:17:54,705 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:17:55,598 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:17:55,616 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:17:56,510 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:17:56,529 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:17:57,410 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:17:57,429 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:17:58,335 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:17:58,353 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:17:59,231 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:17:59,248 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:18:00,200 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:18:00,223 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:18:01,159 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:18:01,179 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:18:02,279 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:18:02,299 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:18:03,284 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:18:03,303 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:18:04,254 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:18:04,275 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:18:05,175 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:18:05,190 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:18:06,099 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:18:06,118 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:18:07,028 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:18:07,045 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:18:07,979 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:18:08,002 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:18:08,958 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:18:08,979 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:18:09,880 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:18:09,898 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:18:09,901 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:18:09,901 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:18:10,699 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:18:10,729 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-16 07:18:10,730 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-16 07:18:10,739 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-16 07:18:10,739 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-16 07:18:10,739 [forward.py:28 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-16 07:18:10,739 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-16 07:18:10,739 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-16 07:18:10,739 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-16 07:18:10,739 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-16 07:18:10,740 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.12 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.13 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.14 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.15 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.16 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.17 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.18 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.19 from test to old.
2023-10-16 07:18:10,741 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.20 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.21 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.22 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.23 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.24 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.25 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.26 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.27 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.28 from test to old.
2023-10-16 07:18:10,742 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.29 from test to old.
2023-10-16 07:18:10,743 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.30 from test to old.
2023-10-16 07:18:10,743 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.31 from test to old.
2023-10-16 07:18:10,743 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-16 07:18:10,743 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-16 07:18:10,743 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-16 07:18:10,743 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-16 07:18:10,743 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-16 07:18:10,743 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-16 07:18:10,743 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-16 07:18:10,744 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.12 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.13 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.14 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.15 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.16 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.17 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.18 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.19 to flexgen forward
2023-10-16 07:18:10,745 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.20 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.21 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.22 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.23 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.24 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.25 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.26 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.27 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.28 to flexgen forward
2023-10-16 07:18:10,746 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.29 to flexgen forward
2023-10-16 07:18:10,747 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.30 to flexgen forward
2023-10-16 07:18:10,747 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.31 to flexgen forward
2023-10-16 07:18:10,747 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-16 07:18:10,747 [forward.py:127 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-16 07:18:10,796 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-6.7b/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-16 07:18:10,969 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:18:10,970 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:18:10,971 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])",)
2023-10-16 07:18:10,971 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:18:10,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,976 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])
2023-10-16 07:18:10,976 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:18:10,977 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:18:10,977 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:18:10,992 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])", "<class 'int'>: 0")
2023-10-16 07:18:10,992 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:18:10,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:10,997 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])
2023-10-16 07:18:10,997 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:18:11,001 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:18:11,004 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:18:11,014 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:11,014 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:11,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:11,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:11,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:11,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:12,000 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:12,000 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:18:12,017 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:18:12,020 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:18:12,029 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:12,029 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:12,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:12,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:12,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:12,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:12,845 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:12,845 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:18:12,862 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:18:12,865 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:18:12,874 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:12,874 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:13,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:13,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:13,738 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:13,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:13,795 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:13,795 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:18:13,811 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:18:13,815 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:18:13,823 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:13,824 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:14,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:14,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:14,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:14,672 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:14,673 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:14,673 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:18:14,691 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:18:14,694 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:18:14,703 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:14,703 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:15,306 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:15,425 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:15,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:15,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:15,534 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:15,534 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:18:15,551 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:18:15,554 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:18:15,564 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:15,564 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:16,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:16,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:16,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:16,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:16,383 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:16,384 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:18:16,399 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:18:16,403 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:18:16,413 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:16,413 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:17,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:17,179 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:17,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:17,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:17,292 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:17,292 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:18:17,314 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:18:17,317 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:18:17,329 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:17,329 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:17,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:18,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:18,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:18,149 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:18,149 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:18,149 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:18:18,168 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:18:18,172 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:18:18,183 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:18,184 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:18,817 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:18,925 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:18,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:19,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:19,034 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:19,034 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:18:19,051 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:18:19,054 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:18:19,064 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:19,065 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:19,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:19,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:19,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:19,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:19,912 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:19,912 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:18:19,929 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:18:19,932 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:18:19,942 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:19,942 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:20,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:20,606 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:20,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:20,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:20,720 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:20,721 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:18:20,738 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:18:20,741 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:18:20,752 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:20,753 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:21,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:21,440 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:21,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:21,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:21,579 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:21,579 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:18:21,602 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:18:21,606 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:18:21,619 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:21,620 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:22,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:22,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:22,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:22,532 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:22,532 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:22,532 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:18:22,558 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:18:22,562 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:18:22,571 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:22,571 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:23,201 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:23,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:23,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:23,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:23,395 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:23,396 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:18:23,412 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:18:23,416 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:18:23,424 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:23,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:24,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:24,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:24,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:24,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:24,303 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:24,303 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:18:24,320 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:18:24,324 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:18:24,332 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:24,333 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:24,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:25,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:25,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:25,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:25,217 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:25,217 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:18:25,236 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:18:25,239 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:18:25,247 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:25,247 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:25,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:25,938 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:26,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:26,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:26,128 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:26,128 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:18:26,144 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:18:26,148 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:18:26,156 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:26,156 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:26,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:26,929 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:27,008 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:27,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:27,101 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:27,101 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:18:27,129 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:18:27,132 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:18:27,139 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:27,139 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:27,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:27,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:27,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:28,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:28,050 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:28,050 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:18:28,067 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:18:28,071 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:18:28,081 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:28,082 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:28,777 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:28,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:28,890 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:28,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:28,946 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:28,946 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:18:28,967 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:18:28,970 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:18:28,979 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:28,979 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:29,603 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:29,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:29,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:29,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:29,834 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:29,834 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:18:29,852 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:18:29,855 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:18:29,865 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:29,865 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:30,536 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:30,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:30,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:30,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:30,776 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:30,777 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:18:30,805 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:18:30,809 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:18:30,818 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:30,818 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:31,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:31,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:31,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:31,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:31,706 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:31,706 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:18:31,723 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:18:31,727 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:18:31,737 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:31,737 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:32,451 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:32,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:32,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:32,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:32,640 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:32,641 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:18:32,665 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:18:32,668 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:18:32,681 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:32,681 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:33,308 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:33,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:33,466 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:33,532 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:33,532 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:33,533 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:18:33,549 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:18:33,553 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:18:33,562 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:33,563 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:34,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:34,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:34,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:34,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:34,454 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:34,454 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:18:34,479 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:18:34,482 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:18:34,490 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:34,490 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:35,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:35,183 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:35,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:35,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:35,297 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:35,298 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:18:35,313 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:18:35,317 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:18:35,327 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:35,327 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:36,009 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:36,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:36,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:36,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:36,252 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:36,252 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:18:36,271 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:18:36,274 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:18:36,283 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:36,283 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:36,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:36,934 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:36,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:37,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:37,042 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:37,042 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:18:37,061 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:18:37,065 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:18:37,075 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:37,076 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:37,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:37,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:37,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:38,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:38,007 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:38,007 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:18:38,027 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:18:38,031 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:18:38,040 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:38,040 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:38,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:38,702 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:38,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:38,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:38,832 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:38,833 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:18:38,850 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:18:38,853 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:18:38,854 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:38,854 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:39,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:39,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:39,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:39,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 9, 128])"))
2023-10-16 07:18:39,728 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"))
2023-10-16 07:18:39,728 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:18:39,748 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:18:39,749 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:18:39,749 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:39,749 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:18:39,751 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:39,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:39,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:39,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 4096])
2023-10-16 07:18:39,755 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])
2023-10-16 07:18:39,755 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:18:39,756 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:18:39,756 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:18:39,757 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 4096])",)
2023-10-16 07:18:39,757 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:18:40,593 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-16 07:18:40,672 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-16 07:18:40,751 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-16 07:18:40,831 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-16 07:18:40,835 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 9, 50272])
2023-10-16 07:18:40,835 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:18:40,859 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:18:40,860 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:18:40,860 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:18:40,861 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:18:40,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,863 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,865 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:18:40,866 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:18:40,866 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:18:40,866 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:18:40,879 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 10])", "<class 'int'>: 9")
2023-10-16 07:18:40,879 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:18:40,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:18:40,883 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:18:40,883 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:18:40,886 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:18:40,889 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:18:40,899 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:40,899 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:41,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:41,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:41,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:41,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:41,524 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:41,524 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:18:41,544 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:18:41,547 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:18:41,555 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:41,555 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:42,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,171 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:42,171 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:18:42,186 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:18:42,189 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:18:42,197 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:42,197 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:42,758 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:42,882 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:42,882 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:18:42,899 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:18:42,902 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:18:42,910 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:42,911 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:43,418 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:43,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:43,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:43,574 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:43,574 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:43,574 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:18:43,600 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:18:43,607 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:18:43,624 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:43,624 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:44,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,196 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,280 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:44,280 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:18:44,297 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:18:44,300 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:18:44,311 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:44,311 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:44,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:44,958 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:44,958 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:18:44,975 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:18:44,979 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:18:44,989 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:44,989 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:45,600 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:45,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:45,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:45,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:45,729 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:45,729 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:18:45,747 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:18:45,751 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:18:45,760 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:45,760 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:46,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:46,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:46,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:46,382 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:46,383 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:46,383 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:18:46,401 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:18:46,405 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:18:46,414 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:46,415 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:46,921 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:46,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:46,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:47,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:47,038 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:47,038 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:18:47,063 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:18:47,066 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:18:47,075 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:47,075 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:47,636 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:47,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:47,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:47,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:47,743 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:47,743 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:18:47,771 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:18:47,774 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:18:47,787 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:47,787 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:48,295 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:48,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:48,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:48,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:48,421 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:48,421 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:18:48,439 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:18:48,443 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:18:48,454 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:48,454 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:48,961 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:48,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:49,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:49,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:49,067 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:49,067 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:18:49,085 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:18:49,088 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:18:49,097 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:49,097 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:49,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:49,632 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:49,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:49,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:49,700 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:49,701 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:18:49,717 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:18:49,721 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:18:49,730 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:49,730 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:50,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:50,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:50,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:50,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:50,351 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:50,351 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:18:50,368 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:18:50,371 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:18:50,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:50,379 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:50,895 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:50,933 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:50,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:51,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:51,013 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:51,013 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:18:51,032 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:18:51,035 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:18:51,045 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:51,045 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:51,637 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:51,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:51,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:51,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:51,756 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:51,756 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:18:51,776 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:18:51,779 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:18:51,786 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:51,787 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:52,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:52,322 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:52,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:52,399 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:52,400 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:52,400 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:18:52,418 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:18:52,421 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:18:52,431 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:52,431 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:52,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,078 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,119 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:53,119 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:18:53,139 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:18:53,142 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:18:53,149 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:53,150 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:53,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:53,775 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:53,775 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:18:53,798 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:18:53,801 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:18:53,815 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:53,816 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:54,393 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:54,432 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:54,473 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:54,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:54,510 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:54,511 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:18:54,533 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:18:54,537 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:18:54,545 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:54,545 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:55,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,174 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,174 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:55,174 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:18:55,191 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:18:55,194 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:18:55,207 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:55,207 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:55,805 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,890 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,933 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:55,933 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:55,933 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:18:55,954 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:18:55,957 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:18:55,965 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:55,965 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:56,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:56,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:56,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:56,601 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:56,601 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:56,601 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:18:56,627 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:18:56,631 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:18:56,642 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:56,642 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:57,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,253 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,344 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,344 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:57,345 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:18:57,370 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:18:57,373 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:18:57,383 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:57,383 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:57,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,961 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:57,998 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:57,999 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:18:58,019 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:18:58,022 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:18:58,032 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:58,032 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:58,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:58,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:58,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:58,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:58,723 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:58,723 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:18:58,744 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:18:58,748 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:18:58,755 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:58,755 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:18:59,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:59,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:59,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:59,388 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:18:59,389 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:18:59,389 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:18:59,409 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:18:59,413 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:18:59,423 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:18:59,423 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:00,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,079 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,112 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:19:00,112 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:19:00,134 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:19:00,137 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:19:00,146 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:00,147 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:00,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,669 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:00,740 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:19:00,740 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:19:00,758 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:19:00,762 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:19:00,774 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:00,775 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:01,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:01,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:01,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:01,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:01,579 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:19:01,579 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:19:01,599 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:19:01,603 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:19:01,611 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:01,612 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:02,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,175 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,254 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,255 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:19:02,255 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:19:02,272 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:19:02,275 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:19:02,276 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:02,276 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:02,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,895 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 10, 128])"))
2023-10-16 07:19:02,964 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"))
2023-10-16 07:19:02,964 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:19:02,983 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:19:02,984 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:19:02,984 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:02,985 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:02,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:02,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:02,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:02,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:02,989 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:02,989 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:19:02,989 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:19:02,990 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:19:02,990 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:02,990 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:03,734 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:03,759 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:03,784 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:03,810 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:03,812 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:19:03,812 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:19:03,842 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:19:03,843 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:19:03,843 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:19:03,843 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:03,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,846 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:03,846 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:19:03,847 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:19:03,847 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:19:03,852 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 11])", "<class 'int'>: 10")
2023-10-16 07:19:03,853 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:03,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:03,856 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:03,857 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:19:03,859 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:19:03,863 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:19:03,866 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:03,866 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:04,588 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:04,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:04,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:04,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:04,690 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:04,690 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:19:04,711 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:19:04,715 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:19:04,719 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:04,719 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:05,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:05,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:05,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:05,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:05,573 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:05,573 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:19:05,590 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:19:05,594 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:19:05,597 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:05,598 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:06,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:06,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:06,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:06,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:06,453 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:06,453 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:19:06,472 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:19:06,475 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:19:06,478 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:06,479 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:07,247 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:07,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:07,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:07,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:07,350 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:07,350 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:19:07,369 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:19:07,373 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:19:07,379 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:07,379 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:08,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:08,153 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:08,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:08,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:08,223 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:08,224 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:19:08,242 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:19:08,245 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:19:08,255 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:08,255 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:09,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,182 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:09,183 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:19:09,203 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:19:09,207 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:19:09,216 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:09,216 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:09,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,831 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,903 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:09,903 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:09,903 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:19:09,920 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:19:09,924 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:19:09,933 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:09,933 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:10,455 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:10,490 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:10,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:10,559 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:10,560 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:10,560 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:19:10,577 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:19:10,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:19:10,591 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:10,591 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:11,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,230 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:11,230 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:19:11,249 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:19:11,252 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:19:11,262 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:11,262 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:11,757 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:11,867 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:11,867 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:19:11,885 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:19:11,888 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:19:11,899 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:11,899 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:12,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:12,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:12,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:12,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:12,513 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:12,514 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:19:12,531 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:19:12,534 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:19:12,546 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:12,546 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:13,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,084 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,171 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:13,171 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:19:13,188 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:19:13,192 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:19:13,201 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:13,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:13,738 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,770 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:13,834 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:13,834 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:19:13,852 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:19:13,855 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:19:13,864 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:13,864 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:14,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:14,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:14,436 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:14,473 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:14,473 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:14,473 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:19:14,492 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:19:14,496 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:19:14,503 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:14,504 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:15,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,083 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,119 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:15,120 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:19:15,149 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:19:15,152 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:19:15,161 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:15,161 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:15,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:15,808 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:15,808 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:19:15,829 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:19:15,833 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:19:15,841 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:15,841 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:16,374 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:16,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:16,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:16,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:16,480 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:16,480 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:19:16,499 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:19:16,503 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:19:16,513 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:16,513 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:17,048 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,080 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,148 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,148 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:17,149 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:19:17,168 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:19:17,172 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:19:17,179 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:17,179 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:17,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:17,778 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:17,778 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:19:17,797 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:19:17,801 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:19:17,811 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:17,811 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:18,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:18,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:18,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:18,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:18,460 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:18,460 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:19:18,481 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:19:18,484 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:19:18,492 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:18,492 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:19,041 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,077 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,155 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,155 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:19,156 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:19:19,173 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:19:19,176 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:19:19,186 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:19,186 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:19,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,809 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:19,886 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:19,886 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:19:19,907 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:19:19,911 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:19:19,920 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:19,920 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:20,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:20,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:20,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:20,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:20,517 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:20,517 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:19:20,538 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:19:20,543 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:19:20,552 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:20,553 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:21,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,208 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,250 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:21,250 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:19:21,282 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:19:21,286 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:19:21,296 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:21,297 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:21,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,926 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:21,926 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:21,926 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:19:21,947 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:19:21,951 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:19:21,961 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:21,961 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:22,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:22,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:22,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:22,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:22,632 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:22,632 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:19:22,655 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:19:22,659 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:19:22,667 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:22,668 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:23,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:23,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:23,251 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:23,291 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:23,291 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:23,291 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:19:23,311 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:19:23,315 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:19:23,325 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:23,325 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:23,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:23,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:23,973 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:24,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:24,004 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:24,004 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:19:24,029 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:19:24,032 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:19:24,041 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:24,041 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:24,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:24,574 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:24,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:24,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:24,647 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:24,648 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:19:24,666 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:19:24,670 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:19:24,680 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:24,681 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:25,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:25,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:25,334 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:25,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:25,368 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:25,368 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:19:25,397 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:19:25,400 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:19:25,410 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:25,411 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:25,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:25,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:25,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:26,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:26,027 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:26,027 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:19:26,046 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:19:26,050 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:19:26,051 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:26,051 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:26,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:26,641 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:26,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:26,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 11, 128])"))
2023-10-16 07:19:26,722 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"))
2023-10-16 07:19:26,722 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:19:26,744 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:19:26,745 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:19:26,746 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:26,746 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:26,747 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:26,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:26,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:26,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:26,749 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:26,750 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:19:26,750 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:19:26,750 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:19:26,751 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:26,751 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:27,475 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:27,498 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:27,522 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:27,545 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:27,546 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:19:27,547 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:19:27,576 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:19:27,577 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:19:27,577 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:19:27,578 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:27,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,580 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,580 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:27,580 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:19:27,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:19:27,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:19:27,593 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 12])", "<class 'int'>: 11")
2023-10-16 07:19:27,594 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:27,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:27,598 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:27,598 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:19:27,601 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:19:27,604 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:19:27,617 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:27,617 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:28,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,210 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:28,210 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:19:28,236 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:19:28,241 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:19:28,254 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:28,254 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:28,757 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:28,877 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:28,877 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:19:28,900 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:19:28,904 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:19:28,921 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:28,921 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:29,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:29,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:29,473 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:29,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:29,506 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:29,506 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:19:29,523 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:19:29,526 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:19:29,537 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:29,538 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:30,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,080 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,115 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,151 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:30,152 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:19:30,172 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:19:30,175 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:19:30,187 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:30,187 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:30,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,813 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:30,814 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:30,814 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:19:30,836 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:19:30,840 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:19:30,850 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:30,850 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:31,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:31,402 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:31,436 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:31,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:31,471 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:31,471 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:19:31,493 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:19:31,497 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:19:31,506 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:31,507 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:32,002 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,074 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,112 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:32,112 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:19:32,131 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:19:32,135 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:19:32,145 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:32,145 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:32,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,837 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:32,838 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:32,838 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:19:32,857 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:19:32,860 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:19:32,870 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:32,870 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:33,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:33,425 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:33,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:33,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:33,497 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:33,498 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:19:33,517 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:19:33,520 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:19:33,531 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:33,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:34,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,084 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,119 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:34,120 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:19:34,139 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:19:34,143 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:19:34,154 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:34,154 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:34,670 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,732 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:34,775 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:34,776 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:19:34,796 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:19:34,799 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:19:34,810 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:34,811 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:35,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:35,334 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:35,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:35,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:35,407 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:35,408 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:19:35,427 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:19:35,430 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:19:35,438 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:35,439 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:35,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,002 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,033 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,066 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,067 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:36,067 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:19:36,086 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:19:36,090 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:19:36,099 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:36,099 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:36,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:36,700 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:36,701 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:19:36,723 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:19:36,726 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:19:36,734 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:36,734 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:37,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,285 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,347 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:37,347 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:19:37,367 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:19:37,370 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:19:37,379 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:37,379 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:37,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:37,616 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:37,616 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:19:37,649 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:19:37,653 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:19:37,660 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:37,660 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:38,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,137 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,177 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:38,178 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:19:38,197 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:19:38,200 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:19:38,209 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:38,209 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:38,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:38,873 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:38,873 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:19:38,897 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:19:38,901 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:19:38,908 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:38,909 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:39,406 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:39,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:39,486 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:39,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:39,520 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:39,520 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:19:39,542 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:19:39,545 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:19:39,554 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:39,555 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:40,080 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,113 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,181 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:40,181 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:19:40,205 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:19:40,209 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:19:40,217 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:40,217 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:40,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,816 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:40,816 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:40,816 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:19:40,835 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:19:40,838 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:19:40,842 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:40,843 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:41,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:41,450 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:41,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:41,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:41,518 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:41,518 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:19:41,539 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:19:41,542 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:19:41,546 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:41,546 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:42,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:42,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:42,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:42,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:42,414 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:42,414 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:19:42,442 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:19:42,445 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:19:42,449 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:42,449 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:43,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:43,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:43,278 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:43,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:43,316 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:43,316 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:19:43,337 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:19:43,340 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:19:43,344 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:43,344 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:44,083 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,200 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:44,200 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:19:44,219 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:19:44,222 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:19:44,226 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:44,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:44,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:44,838 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:44,839 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:19:44,873 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:19:44,876 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:19:44,880 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:44,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:45,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:45,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:45,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:45,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:45,281 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:45,281 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:19:45,324 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:19:45,327 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:19:45,331 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:45,331 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:46,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:46,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:46,076 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:46,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:46,120 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:46,120 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:19:46,148 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:19:46,152 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:19:46,156 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:46,156 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:46,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:46,953 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:46,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:47,023 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:47,024 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:47,024 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:19:47,044 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:19:47,048 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:19:47,052 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:47,052 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:47,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:47,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:47,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:47,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:47,899 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:47,899 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:19:47,921 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:19:47,925 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:19:47,928 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:47,928 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:48,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:48,731 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:48,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:48,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:48,811 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:48,811 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:19:48,830 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:19:48,834 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:19:48,835 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:48,835 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:49,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:49,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:49,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:49,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 12, 128])"))
2023-10-16 07:19:49,699 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"))
2023-10-16 07:19:49,699 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:19:49,722 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:19:49,723 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:19:49,723 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:49,724 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:49,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:49,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:49,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:49,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:49,728 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:49,728 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:19:49,729 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:19:49,729 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:19:49,730 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:49,730 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:50,407 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:50,438 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:50,469 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:50,501 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:19:50,506 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:19:50,507 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:19:50,544 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:19:50,545 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:19:50,545 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:19:50,545 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:50,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,548 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:50,548 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:19:50,549 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:19:50,549 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:19:50,553 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 13])", "<class 'int'>: 12")
2023-10-16 07:19:50,553 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:19:50,554 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:19:50,556 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:19:50,557 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:19:50,559 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:19:50,563 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:19:50,566 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:50,567 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:51,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,332 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:51,333 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:19:51,358 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:19:51,361 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:19:51,366 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:51,366 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:51,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,747 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:51,815 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:51,816 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:19:51,855 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:19:51,860 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:19:51,864 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:51,864 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:52,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,254 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,294 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:52,294 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:19:52,337 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:19:52,340 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:19:52,344 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:52,344 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:52,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,673 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,745 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:52,746 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:52,746 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:19:52,789 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:19:52,793 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:19:52,797 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:52,797 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:53,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,221 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:53,221 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:19:53,264 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:19:53,268 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:19:53,272 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:53,272 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:53,851 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:53,954 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:53,954 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:19:53,983 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:19:53,987 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:19:53,990 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:53,990 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:54,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:54,760 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:54,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:54,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:54,842 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:54,842 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:19:54,862 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:19:54,866 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:19:54,870 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:54,870 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:55,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:55,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:55,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:55,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:55,709 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:55,709 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:19:55,728 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:19:55,732 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:19:55,735 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:55,735 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:56,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:56,508 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:56,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:56,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:56,573 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:56,573 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:19:56,592 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:19:56,596 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:19:56,600 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:56,600 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:57,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:57,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:57,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:57,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:57,413 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:57,414 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:19:57,435 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:19:57,439 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:19:57,442 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:57,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:58,217 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,344 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,344 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:58,345 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:19:58,365 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:19:58,369 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:19:58,372 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:58,373 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:58,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:58,923 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:58,923 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:19:58,956 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:19:58,959 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:19:58,962 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:58,962 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:59,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,428 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:59,428 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:19:59,476 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:19:59,479 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:19:59,483 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:59,483 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:19:59,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,906 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:19:59,935 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:19:59,935 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:19:59,983 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:19:59,987 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:19:59,990 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:19:59,991 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:00,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:00,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:00,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:00,396 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:00,396 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:00,396 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:20:00,438 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:20:00,442 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:00,445 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:00,446 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:00,933 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:00,973 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,052 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:01,053 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:20:01,076 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:01,079 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:01,083 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:01,083 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:01,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,609 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,610 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:01,610 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:20:01,645 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:01,648 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:01,652 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:01,652 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:01,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:01,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:02,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:02,019 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:02,020 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:20:02,065 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:02,069 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:02,073 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:02,073 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:02,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:02,432 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:02,472 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:02,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:02,538 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:02,539 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:20:02,582 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:02,586 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:02,590 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:02,591 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:02,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:02,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:03,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:03,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:03,097 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:03,097 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:20:03,158 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:03,164 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:03,172 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:03,173 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:03,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:03,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:03,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:03,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:03,820 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:03,820 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:20:03,857 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:03,863 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:03,875 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:03,876 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:04,634 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:04,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:04,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:04,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:04,737 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:04,737 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:20:04,759 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:04,763 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:04,773 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:04,773 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:05,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:05,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:05,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:05,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:05,440 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:05,440 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:20:05,459 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:05,463 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:05,475 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:05,475 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:06,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,113 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,191 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:06,191 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:20:06,212 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:06,216 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:06,226 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:06,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:06,757 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:06,871 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:06,871 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:20:06,891 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:06,895 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:06,907 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:06,908 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:07,219 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,256 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,341 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:07,341 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:20:07,366 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:07,369 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:20:07,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:07,378 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:07,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,555 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:07,555 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:20:07,575 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:20:07,578 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:20:07,589 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:07,590 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:07,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,771 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:07,771 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:20:07,813 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:20:07,817 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:20:07,826 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:07,826 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:07,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,958 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:07,989 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:07,989 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:20:08,031 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:20:08,035 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:20:08,048 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:08,048 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:08,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,209 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,209 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:08,209 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:20:08,258 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:20:08,261 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:20:08,271 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:08,271 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:08,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,863 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:08,946 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:08,946 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:20:08,965 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:20:08,968 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:20:08,969 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:08,969 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:09,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:09,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:09,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:09,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 13, 128])"))
2023-10-16 07:20:09,709 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"))
2023-10-16 07:20:09,709 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:20:09,732 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:20:09,732 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:20:09,733 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:09,733 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:09,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:09,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:09,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:09,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:09,736 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:09,737 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:20:09,737 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:20:09,737 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:20:09,738 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:09,738 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:10,395 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:10,420 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:10,446 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:10,472 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:10,474 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:20:10,475 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:20:10,508 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:20:10,509 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:20:10,509 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:20:10,510 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:10,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,512 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,512 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,513 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:10,513 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:20:10,513 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:20:10,514 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:20:10,523 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 14])", "<class 'int'>: 13")
2023-10-16 07:20:10,523 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:10,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:10,527 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:10,527 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:20:10,530 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:20:10,534 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:20:10,544 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:10,544 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:10,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,076 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,077 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:11,077 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:20:11,099 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:20:11,103 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:20:11,117 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:11,118 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:11,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,364 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,399 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,399 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:11,399 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:20:11,419 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:20:11,422 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:20:11,433 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:11,433 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:11,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,586 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:11,586 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:20:11,606 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:20:11,610 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:20:11,620 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:11,621 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:11,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,760 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,794 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:11,794 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:20:11,814 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:20:11,817 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:20:11,828 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:11,829 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:11,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:11,993 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:12,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:12,030 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:12,031 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:20:12,053 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:20:12,057 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:20:12,068 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:12,068 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:12,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:12,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:12,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:12,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:12,513 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:12,513 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:20:12,535 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:20:12,539 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:20:12,551 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:12,551 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:13,080 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,149 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,181 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:13,181 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:20:13,201 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:20:13,205 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:20:13,215 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:13,215 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:13,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,770 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:13,847 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:13,847 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:20:13,865 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:20:13,869 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:20:13,880 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:13,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:14,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:14,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:14,537 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:14,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:14,577 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:14,577 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:20:14,600 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:20:14,606 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:20:14,618 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:14,618 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:15,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,259 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:15,259 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:20:15,278 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:20:15,282 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:20:15,293 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:15,293 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:15,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,956 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:15,956 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:15,956 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:20:15,976 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:20:15,980 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:20:15,991 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:15,991 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:16,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,276 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:16,276 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:20:16,297 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:20:16,300 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:20:16,312 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:16,312 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:16,371 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,482 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:16,482 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:20:16,502 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:20:16,505 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:20:16,523 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:16,523 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:16,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,650 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,687 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,723 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:16,723 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:20:16,744 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:20:16,747 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:20:16,756 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:16,756 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:16,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,962 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:16,963 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:16,963 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:20:16,982 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:20:16,985 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:16,997 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:16,997 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:17,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,644 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:17,645 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:20:17,665 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:17,669 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:17,677 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:17,677 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:17,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,896 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,973 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:17,973 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:17,973 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:20:17,992 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:17,995 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:18,007 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:18,007 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:18,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,209 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,273 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,340 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:18,340 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:20:18,363 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:18,366 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:18,376 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:18,376 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:18,496 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,593 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:18,593 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:20:18,613 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:18,617 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:18,629 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:18,629 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:18,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,962 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:18,962 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:18,962 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:20:18,992 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:18,998 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:19,013 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:19,013 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:19,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,287 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:19,287 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:20:19,307 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:19,311 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:19,316 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:19,316 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:19,677 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,711 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,746 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:19,782 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:19,782 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:20:19,805 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:19,808 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:19,812 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:19,813 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:20,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:20,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:20,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:20,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:20,543 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:20,544 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:20:20,564 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:20,568 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:20,572 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:20,572 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:21,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:21,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:21,466 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:21,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:21,501 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:21,501 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:20:21,526 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:21,530 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:21,534 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:21,534 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:22,295 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:22,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:22,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:22,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:22,395 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:22,395 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:20:22,415 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:22,419 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:22,423 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:22,423 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:23,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:23,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:23,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:23,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:23,256 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:23,256 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:20:23,278 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:23,282 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:20:23,285 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:23,285 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:24,066 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,168 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,168 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:24,169 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:20:24,194 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:20:24,197 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:20:24,201 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:24,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:24,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:24,662 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:24,662 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:20:24,684 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:20:24,687 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:20:24,691 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:24,691 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:25,015 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,141 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,141 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:25,141 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:20:25,162 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:20:25,166 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:20:25,178 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:25,179 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:25,483 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,570 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,611 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:25,611 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:20:25,632 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:20:25,636 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:20:25,644 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:25,645 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:25,725 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,762 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:25,841 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:25,841 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:20:25,879 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:20:25,882 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:20:25,883 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:25,883 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:26,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:26,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:26,096 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:26,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 14, 128])"))
2023-10-16 07:20:26,132 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"))
2023-10-16 07:20:26,132 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:20:26,180 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:20:26,180 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:20:26,181 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:26,181 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:26,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,183 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,185 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:26,185 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:20:26,185 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:20:26,186 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:20:26,186 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:26,186 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:26,379 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:26,405 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:26,431 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:26,455 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:26,459 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:20:26,459 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:20:26,513 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:20:26,514 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:20:26,514 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:20:26,514 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:26,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,517 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:26,518 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:20:26,518 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:20:26,519 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:20:26,523 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 15])", "<class 'int'>: 14")
2023-10-16 07:20:26,523 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:26,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:26,526 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:26,526 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:20:26,530 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:20:26,533 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:20:26,543 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:26,543 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:27,095 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:27,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:27,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:27,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:27,194 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:27,194 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:20:27,213 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:20:27,217 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:20:27,228 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:27,228 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:28,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,061 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,144 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,145 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:28,145 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:20:28,165 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:20:28,168 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:20:28,179 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:28,179 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:28,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,953 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:28,954 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:28,954 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:20:28,973 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:20:28,977 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:20:28,986 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:28,986 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:29,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:29,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:29,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:29,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:29,613 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:29,614 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:20:29,636 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:20:29,639 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:20:29,651 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:29,651 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:30,169 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,253 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,293 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:30,293 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:20:30,314 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:20:30,318 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:20:30,329 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:30,329 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:30,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,654 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,688 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,724 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:30,724 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:20:30,760 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:20:30,764 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:20:30,774 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:30,774 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:30,829 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:30,937 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:30,937 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:20:30,981 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:20:30,985 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:20:30,996 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:30,996 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:31,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,079 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,154 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:31,154 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:20:31,176 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:20:31,180 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:20:31,191 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:31,191 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:31,261 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,362 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:31,362 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:20:31,381 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:20:31,385 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:20:31,395 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:31,396 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:31,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,688 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,756 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:31,756 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:20:31,777 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:20:31,780 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:20:31,791 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:31,791 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:31,932 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:31,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:32,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:32,061 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:32,062 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:32,062 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:20:32,081 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:20:32,084 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:20:32,096 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:32,096 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:32,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:32,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:32,574 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:32,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:32,617 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:32,617 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:20:32,636 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:20:32,640 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:20:32,651 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:32,651 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:33,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,284 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:33,284 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:20:33,304 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:20:33,308 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:20:33,319 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:33,319 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:33,819 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:33,918 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:33,918 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:20:33,939 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:20:33,943 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:20:33,952 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:33,953 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:34,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:34,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:34,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:34,580 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:34,581 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:34,581 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:20:34,602 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:20:34,606 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:34,619 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:34,619 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:35,012 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,126 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:35,126 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:20:35,149 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:35,152 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:35,161 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:35,161 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:35,583 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,620 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:35,692 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:35,692 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:20:35,718 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:35,722 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:35,729 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:35,730 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:36,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:36,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:36,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:36,431 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:36,432 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:36,432 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:20:36,455 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:36,459 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:36,463 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:36,463 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:37,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:37,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:37,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:37,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:37,260 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:37,261 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:20:37,284 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:37,287 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:37,291 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:37,291 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:38,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,152 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:38,152 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:20:38,178 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:38,181 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:38,185 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:38,185 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:38,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,842 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:38,905 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:38,905 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:20:38,934 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:38,937 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:38,947 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:38,947 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:39,261 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,386 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,387 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:39,387 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:20:39,434 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:39,438 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:39,447 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:39,447 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:39,680 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:39,793 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:39,793 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:20:39,835 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:39,839 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:39,850 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:39,851 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:40,044 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,158 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:40,158 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:20:40,209 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:40,213 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:40,223 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:40,223 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:40,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,747 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:40,747 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:40,747 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:20:40,767 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:40,770 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:40,782 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:40,782 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:41,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:41,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:41,314 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:41,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:41,347 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:41,348 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:20:41,371 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:41,374 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:20:41,384 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:41,385 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:41,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:41,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:41,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:42,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:42,020 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:42,020 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:20:42,041 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:20:42,044 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:20:42,056 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:42,056 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:42,652 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:42,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:42,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:42,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:42,750 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:42,750 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:20:42,775 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:20:42,779 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:20:42,789 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:42,789 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:43,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:43,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:43,359 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:43,392 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:43,392 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:43,393 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:20:43,411 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:20:43,414 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:20:43,425 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:43,425 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:43,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,112 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:44,112 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:20:44,142 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:20:44,148 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:20:44,157 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:44,158 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:44,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,278 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,372 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:44,372 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:20:44,396 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:20:44,400 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:20:44,400 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:44,400 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:44,529 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 14, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 15, 128])"))
2023-10-16 07:20:44,616 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"))
2023-10-16 07:20:44,617 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:20:44,639 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:20:44,640 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:20:44,640 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:44,640 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:44,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:44,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:44,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:44,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:44,644 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:44,644 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:20:44,645 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:20:44,645 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:20:44,646 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:44,646 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:44,992 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:45,014 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:45,036 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:45,057 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:20:45,061 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:20:45,062 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:20:45,093 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:20:45,094 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:20:45,095 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:20:45,095 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:45,096 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,096 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,098 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,098 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:45,098 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:20:45,098 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:20:45,099 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:20:45,110 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 16])", "<class 'int'>: 15")
2023-10-16 07:20:45,110 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:20:45,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,113 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:20:45,114 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:20:45,114 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:20:45,117 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:20:45,121 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:20:45,132 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:45,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:45,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:45,625 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:45,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:45,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:45,697 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:45,697 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:20:45,721 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:20:45,724 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:20:45,737 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:45,737 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:46,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,267 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,336 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,336 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:46,337 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:20:46,358 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:20:46,362 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:20:46,373 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:46,373 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:46,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:46,997 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:46,997 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:20:47,018 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:20:47,021 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:20:47,032 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:47,032 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:47,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:47,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:47,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:47,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:47,629 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:47,629 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:20:47,653 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:20:47,657 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:20:47,669 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:47,669 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:48,167 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,198 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,299 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,299 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:48,299 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:20:48,322 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:20:48,325 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:20:48,335 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:48,335 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:48,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,677 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:48,677 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:20:48,701 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:20:48,705 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:20:48,716 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:48,716 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:48,772 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,872 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:48,872 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:48,873 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:20:48,892 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:20:48,896 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:20:48,906 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:48,906 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:49,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:49,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:49,424 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:49,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:49,460 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:49,460 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:20:49,481 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:20:49,485 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:20:49,495 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:49,496 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:50,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,096 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,192 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:50,192 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:20:50,216 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:20:50,220 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:20:50,232 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:50,232 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:50,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:50,744 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:50,745 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:20:50,780 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:20:50,785 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:20:50,799 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:50,799 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:51,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:51,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:51,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:51,435 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:51,436 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:51,436 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:20:51,457 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:20:51,460 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:20:51,471 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:51,471 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:51,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,118 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:52,118 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:20:52,139 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:20:52,144 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:20:52,157 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:52,157 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:52,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,759 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:52,797 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:52,798 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:20:52,826 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:20:52,832 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:20:52,846 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:52,846 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:53,337 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:53,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:53,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:53,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:53,446 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:53,447 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:20:53,466 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:20:53,470 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:20:53,478 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:53,478 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:53,968 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,009 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,087 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:54,087 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:20:54,108 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:20:54,112 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:54,124 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:54,124 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:54,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:54,604 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:54,604 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:20:54,642 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:20:54,645 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:54,655 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:54,655 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:54,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,120 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:55,120 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:20:55,155 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:20:55,158 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:55,175 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:55,175 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:55,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:55,585 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:55,585 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:20:55,621 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:20:55,625 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:55,635 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:55,635 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:56,187 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:56,222 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:56,257 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:56,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:56,290 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:56,290 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:20:56,310 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:20:56,313 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:56,326 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:56,326 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:56,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,095 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,095 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:57,096 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:20:57,120 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:20:57,124 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:57,134 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:57,135 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:57,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,608 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,609 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:57,609 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:20:57,644 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:20:57,647 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:57,653 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:57,653 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:57,751 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:57,865 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:57,865 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:20:57,888 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:20:57,891 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:57,896 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:57,896 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:58,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,364 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,402 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,435 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,435 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:58,436 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:20:58,456 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:20:58,459 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:58,463 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:58,464 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:58,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:58,891 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:58,892 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:20:58,915 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:20:58,919 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:58,925 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:58,925 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:59,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,401 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:59,401 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:20:59,445 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:20:59,449 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:59,453 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:59,454 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:20:59,759 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:20:59,862 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:20:59,862 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:20:59,910 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:20:59,913 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:20:59,916 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:20:59,916 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:00,247 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:00,289 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:00,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:00,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:00,370 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:21:00,370 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:21:00,416 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:21:00,420 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:00,425 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:00,425 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:01,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:01,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:01,232 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:01,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:01,270 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:21:01,270 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:21:01,299 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:01,302 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:01,306 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:01,306 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:02,029 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:02,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:02,108 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:02,150 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:02,151 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:21:02,151 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:21:02,174 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:02,177 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:02,187 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:02,188 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:02,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:02,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:03,028 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:03,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:03,069 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:21:03,069 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:21:03,096 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:03,099 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:03,107 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:03,108 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:03,609 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:03,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:03,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:03,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:03,715 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:21:03,715 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:21:03,738 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:03,741 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:03,742 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:03,742 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:04,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:04,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:04,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:04,489 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 15, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 16, 128])"))
2023-10-16 07:21:04,490 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"))
2023-10-16 07:21:04,490 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:21:04,517 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:04,518 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:04,518 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:04,518 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:04,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:04,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:04,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:04,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:04,525 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:04,525 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:21:04,526 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:04,526 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:04,527 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:04,527 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:04,970 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:04,996 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:05,022 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:05,048 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:05,052 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:21:05,053 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:21:05,107 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:05,107 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:05,108 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:21:05,108 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:05,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,111 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:05,111 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:21:05,111 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:05,112 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:05,115 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 17])", "<class 'int'>: 16")
2023-10-16 07:21:05,116 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:05,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:05,119 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:05,119 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:21:05,122 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:05,126 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:05,130 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:05,130 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:05,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:05,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:05,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:05,588 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:05,588 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:05,588 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:21:05,631 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:05,635 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:05,645 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:05,645 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:06,183 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,247 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,283 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:06,283 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:21:06,317 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:06,321 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:06,331 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:06,332 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:06,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,654 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,731 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,731 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:06,731 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:21:06,770 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:06,773 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:06,783 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:06,783 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:06,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,900 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:06,937 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:06,937 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:21:06,980 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:06,984 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:06,995 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:06,995 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:07,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,133 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:07,133 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:21:07,178 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:07,182 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:07,192 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:07,192 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:07,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:07,635 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:07,635 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:21:07,667 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:07,671 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:07,682 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:07,682 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:08,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,256 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,334 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,334 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:08,335 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:21:08,356 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:08,360 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:21:08,371 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:08,371 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:08,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,612 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:08,612 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:21:08,654 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:21:08,658 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:21:08,669 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:08,670 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:08,716 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,822 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:08,822 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:21:08,865 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:21:08,869 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:21:08,879 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:08,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:08,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:08,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,050 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:09,050 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:21:09,085 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:21:09,088 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:21:09,099 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:09,100 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:09,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,264 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,306 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,307 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:09,307 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:21:09,348 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:21:09,351 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:21:09,362 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:09,362 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:09,786 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:09,907 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:09,907 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:21:09,934 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:21:09,938 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:21:09,948 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:09,949 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:10,455 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:10,494 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:10,537 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:10,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:10,576 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:10,576 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:21:10,602 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:21:10,605 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:21:10,617 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:10,617 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:11,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,227 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:11,228 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:21:11,249 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:21:11,253 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:21:11,262 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:11,262 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:11,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,837 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:11,916 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:11,916 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:21:11,937 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:21:11,941 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:21:11,953 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:11,954 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:12,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,153 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,187 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,223 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:12,224 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:21:12,258 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:21:12,262 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:21:12,271 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:12,271 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:12,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,571 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,601 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,601 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:12,601 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:21:12,634 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:21:12,637 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:21:12,648 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:12,648 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:12,948 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:12,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,061 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:13,061 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:21:13,090 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:21:13,094 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:21:13,102 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:13,102 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:13,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,460 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,523 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:13,523 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:21:13,555 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:21:13,559 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:21:13,569 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:13,569 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:13,759 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:13,867 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:13,867 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:21:13,912 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:21:13,915 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:21:13,926 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:13,926 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:14,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:14,483 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:14,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:14,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:14,561 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:14,561 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:21:14,584 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:21:14,587 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:21:14,602 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:14,602 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:15,187 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,310 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,311 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:15,311 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:21:15,335 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:21:15,339 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:21:15,348 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:15,348 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:15,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:15,923 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:15,923 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:21:15,945 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:21:15,948 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:21:15,961 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:15,961 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:16,651 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,752 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:16,752 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:21:16,777 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:21:16,781 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:21:16,791 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:16,792 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:16,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:16,954 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:16,954 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:21:16,975 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:21:16,979 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:21:16,991 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:16,991 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:17,080 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,144 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,177 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:17,177 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:21:17,200 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:21:17,204 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:21:17,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:17,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:17,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,310 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,382 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:17,382 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:21:17,404 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:21:17,408 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:17,422 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:17,422 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:17,787 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:17,890 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:17,890 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:21:17,914 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:17,918 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:17,928 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:17,929 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:18,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:18,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:18,337 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:18,380 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:18,380 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:18,380 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:21:18,400 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:18,404 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:18,414 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:18,415 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:18,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:18,977 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:19,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:19,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:19,050 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:19,051 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:21:19,079 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:19,083 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:19,091 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:19,092 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:19,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:19,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:19,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:19,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:19,701 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:19,701 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:21:19,720 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:19,724 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:19,724 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:19,725 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:20,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:20,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:20,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:20,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 16, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 17, 128])"))
2023-10-16 07:21:20,416 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"))
2023-10-16 07:21:20,417 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:21:20,444 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:20,445 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:20,446 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:20,446 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:20,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:20,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:20,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:20,449 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:20,449 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:20,449 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:21:20,450 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:20,450 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:20,450 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:20,451 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:20,889 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:20,914 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:20,937 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:20,960 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:20,963 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:21:20,963 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:21:20,995 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:20,996 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:20,996 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:21:20,996 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:20,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:20,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:20,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:21,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:21,000 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:21,000 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:21:21,001 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:21,001 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:21,015 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 18])", "<class 'int'>: 17")
2023-10-16 07:21:21,015 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:21,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:21,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:21,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:21,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:21,019 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:21,019 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:21:21,022 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:21,025 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:21,038 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:21,038 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:21,166 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,269 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:21,269 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:21:21,289 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:21,293 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:21,300 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:21,301 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:21,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:21,721 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:21,721 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:21:21,743 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:21,748 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:21,751 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:21,752 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:22,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,088 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,164 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:22,164 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:21:22,183 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:22,187 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:22,190 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:22,190 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:22,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,593 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:22,594 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:21:22,624 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:22,627 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:22,631 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:22,631 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:22,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:22,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:23,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:23,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:23,059 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:23,059 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:21:23,078 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:23,082 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:23,085 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:23,085 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:23,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:23,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:23,747 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:23,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:23,790 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:23,790 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:21:23,812 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:23,815 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:23,819 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:23,819 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:24,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:24,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:24,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:24,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:24,689 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:24,690 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:21:24,713 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:24,717 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:21:24,721 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:24,721 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:25,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,139 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,187 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,226 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:25,226 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:21:25,250 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:21:25,254 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:21:25,258 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:25,258 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:25,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:25,667 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:25,667 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:21:25,688 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:21:25,692 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:21:25,695 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:25,695 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:26,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,293 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:26,293 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:21:26,314 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:21:26,318 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:21:26,321 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:26,321 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:26,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:26,793 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:26,793 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:21:26,815 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:21:26,819 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:21:26,824 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:26,825 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:27,137 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,219 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,252 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:27,252 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:21:27,292 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:21:27,296 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:21:27,300 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:27,300 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:27,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:27,719 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:27,719 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:21:27,769 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:21:27,773 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:21:27,776 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:27,776 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:28,115 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,155 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,232 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,232 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:28,232 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:21:28,277 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:21:28,280 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:21:28,283 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:28,284 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:28,603 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:28,726 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:28,727 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:21:28,772 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:21:28,776 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:21:28,779 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:28,779 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:29,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:29,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:29,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:29,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:29,401 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:29,401 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:21:29,426 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:21:29,430 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:21:29,433 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:29,433 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:29,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:29,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,015 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,056 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,056 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:30,056 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:21:30,081 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:21:30,085 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:21:30,094 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:30,094 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:30,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,657 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,732 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,732 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:30,732 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:21:30,771 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:21:30,774 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:21:30,785 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:30,785 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:30,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:30,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,029 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,064 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:31,064 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:21:31,096 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:21:31,099 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:21:31,112 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:31,112 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:31,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,328 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:31,329 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:21:31,358 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:21:31,361 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:21:31,370 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:31,371 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:31,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,828 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:31,828 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:31,828 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:21:31,862 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:21:31,865 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:21:31,877 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:31,877 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:32,470 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:32,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:32,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:32,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:32,569 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:32,569 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:21:32,594 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:21:32,598 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:21:32,610 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:32,610 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:33,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,219 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,220 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:33,220 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:21:33,246 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:21:33,249 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:21:33,262 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:33,262 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:33,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,904 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,978 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:33,978 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:33,978 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:21:34,013 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:21:34,016 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:21:34,026 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:34,026 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:34,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:34,607 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:34,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:34,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:34,685 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:34,685 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:21:34,713 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:21:34,718 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:21:34,732 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:34,732 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:35,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:35,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:35,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:35,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:35,482 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:35,482 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:21:35,507 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:21:35,511 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:21:35,521 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:35,522 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:36,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,146 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:36,147 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:21:36,168 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:21:36,172 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:36,184 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:36,185 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:36,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,380 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,458 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:36,458 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:21:36,504 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:36,508 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:36,519 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:36,519 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:36,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,733 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,734 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:36,734 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:21:36,781 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:36,786 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:36,795 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:36,796 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:36,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:36,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,010 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:37,010 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:21:37,059 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:37,063 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:37,071 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:37,072 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:37,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,486 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,523 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:37,523 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:21:37,567 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:37,571 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:37,572 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:37,572 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:37,766 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,809 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 17, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 18, 128])"))
2023-10-16 07:21:37,889 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"))
2023-10-16 07:21:37,889 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:21:37,937 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:37,938 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:37,938 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:37,938 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:37,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:37,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:37,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:37,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:37,943 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:37,943 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:21:37,943 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:37,944 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:37,944 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:37,944 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:38,311 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:38,334 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:38,357 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:38,382 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:38,386 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:21:38,386 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:21:38,444 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:38,445 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:38,445 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:21:38,445 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:38,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,448 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:38,448 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:21:38,449 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:38,449 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:38,462 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 19])", "<class 'int'>: 18")
2023-10-16 07:21:38,462 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:38,463 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:38,465 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:38,465 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:21:38,468 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:38,472 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:38,486 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:38,486 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:38,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,078 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,079 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:39,079 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:21:39,102 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:39,106 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:39,118 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:39,119 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:39,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:39,545 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:39,545 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:21:39,577 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:39,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:39,592 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:39,593 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:40,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,087 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:40,087 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:21:40,115 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:40,119 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:40,132 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:40,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:40,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,679 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,762 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:40,763 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:40,763 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:21:40,795 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:40,799 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:40,815 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:40,816 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:41,336 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:41,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:41,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:41,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:41,457 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:41,457 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:21:41,479 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:41,483 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:41,494 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:41,495 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:41,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,033 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,115 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:42,115 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:21:42,136 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:42,140 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:42,152 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:42,152 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:42,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:42,762 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:42,762 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:21:42,784 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:42,788 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:21:42,800 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:42,800 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:43,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:43,345 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:43,380 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:43,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:43,416 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:43,416 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:21:43,438 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:21:43,445 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:21:43,463 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:43,464 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:43,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,005 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,073 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,074 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:44,074 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:21:44,097 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:21:44,103 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:21:44,121 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:44,121 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:44,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:44,698 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:44,698 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:21:44,720 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:21:44,724 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:21:44,738 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:44,738 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:45,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,346 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:45,347 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:21:45,370 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:21:45,374 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:21:45,386 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:45,386 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:45,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,683 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,684 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:45,684 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:21:45,706 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:21:45,710 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:21:45,723 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:45,723 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:45,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,837 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,872 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:45,872 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:45,872 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:21:45,894 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:21:45,897 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:21:45,909 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:45,909 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:45,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,029 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,061 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,094 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,094 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:46,094 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:21:46,116 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:21:46,119 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:21:46,129 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:46,129 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:46,201 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,278 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,309 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:46,309 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:21:46,328 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:21:46,332 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:21:46,343 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:46,344 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:46,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,903 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,934 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,973 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:46,973 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:46,974 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:21:47,000 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:21:47,004 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:21:47,018 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:47,018 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:47,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:47,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:47,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:47,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:47,562 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:47,563 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:21:47,584 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:21:47,588 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:21:47,595 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:47,595 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:48,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,066 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,096 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,096 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:48,096 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:21:48,124 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:21:48,127 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:21:48,131 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:48,131 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:48,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,626 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,704 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:48,704 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:48,704 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:21:48,727 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:21:48,731 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:21:48,737 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:48,738 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:49,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:49,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:49,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:49,493 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:49,494 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:49,494 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:21:49,519 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:21:49,523 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:21:49,527 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:49,527 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:49,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,104 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:50,104 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:21:50,131 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:21:50,135 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:21:50,146 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:50,146 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:50,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,529 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,529 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:50,529 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:21:50,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:21:50,585 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:21:50,598 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:50,598 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:50,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,716 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,781 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:50,781 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:21:50,834 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:21:50,838 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:21:50,853 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:50,853 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:50,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:50,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:51,000 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:51,000 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:21:51,054 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:21:51,058 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:21:51,069 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:51,069 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:51,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:51,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:51,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:51,687 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:51,688 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:51,688 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:21:51,711 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:21:51,715 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:21:51,728 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:51,728 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:52,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:52,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:52,373 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:52,406 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:52,406 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:52,406 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:21:52,430 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:21:52,433 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:21:52,442 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:52,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:52,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:52,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,042 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:53,042 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:21:53,063 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:21:53,066 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:53,070 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:53,070 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:53,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,167 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,201 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,235 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:53,235 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:21:53,261 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:21:53,264 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:53,274 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:53,274 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:53,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,632 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,664 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,664 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:53,664 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:21:53,686 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:21:53,689 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:53,701 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:53,701 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:53,737 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,858 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,859 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:53,859 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:21:53,882 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:21:53,886 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:53,897 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:53,897 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:53,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:53,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:54,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:54,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:54,043 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:54,043 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:21:54,062 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:21:54,065 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:54,066 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:54,066 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:54,115 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:54,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:54,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:54,224 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 18, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 19, 128])"))
2023-10-16 07:21:54,224 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"))
2023-10-16 07:21:54,224 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:21:54,255 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:21:54,255 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:54,256 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:54,256 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:54,257 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,260 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:54,260 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:21:54,260 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:21:54,261 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:54,261 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:54,261 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:54,500 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:54,530 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:54,561 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:54,591 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:21:54,593 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:21:54,593 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:21:54,625 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:21:54,625 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:54,626 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:21:54,626 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:54,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,629 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:54,629 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:21:54,630 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:21:54,630 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:54,642 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 20])", "<class 'int'>: 19")
2023-10-16 07:21:54,642 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:21:54,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,646 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:21:54,647 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:21:54,647 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:21:54,650 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:21:54,653 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:54,670 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:54,670 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:55,078 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,195 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:21:55,195 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:21:55,219 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:21:55,222 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:55,235 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:55,235 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:55,534 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,600 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,632 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:55,632 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:21:55,632 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:21:55,653 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:21:55,656 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:55,667 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:55,667 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:56,193 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:56,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:56,267 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:56,306 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:56,306 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:21:56,307 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:21:56,338 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:21:56,343 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:56,355 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:56,355 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:57,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:57,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:57,166 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:57,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:57,204 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:21:57,204 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:21:57,236 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:21:57,239 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:57,248 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:57,248 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:58,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,177 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:21:58,177 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:21:58,199 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:21:58,202 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:58,214 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:58,214 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:58,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,760 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,842 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:58,843 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:21:58,843 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:21:58,863 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:21:58,866 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:58,876 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:58,876 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:21:59,277 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:59,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:59,359 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:59,393 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:21:59,394 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:21:59,394 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:21:59,413 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:21:59,416 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:21:59,427 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:21:59,427 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:00,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:00,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:00,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:00,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:00,246 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:00,246 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:22:00,267 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:22:00,270 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:22:00,273 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:00,274 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:01,089 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,215 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:01,215 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:22:01,236 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:22:01,239 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:22:01,243 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:01,243 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:01,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,932 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:01,966 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:01,966 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:22:01,987 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:22:01,990 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:22:01,994 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:01,994 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:02,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:02,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:02,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:02,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:02,919 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:02,919 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:22:02,941 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:22:02,945 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:22:02,949 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:02,949 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:03,641 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:03,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:03,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:03,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:03,737 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:03,737 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:22:03,764 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:22:03,767 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:22:03,771 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:03,771 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:04,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:04,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:04,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:04,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:04,649 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:04,649 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:22:04,672 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:22:04,676 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:22:04,685 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:04,685 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:05,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:05,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:05,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:05,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:05,577 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:05,577 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:22:05,599 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:22:05,603 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:22:05,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:05,613 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:06,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,436 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,473 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,510 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:06,510 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:22:06,539 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:22:06,542 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:22:06,555 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:06,555 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:06,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,813 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:06,813 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:06,813 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:22:06,865 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:22:06,869 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:22:06,879 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:06,879 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:07,095 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,190 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:07,190 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:22:07,227 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:22:07,231 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:22:07,244 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:07,244 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:07,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:07,772 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:07,772 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:22:07,806 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:22:07,809 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:22:07,819 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:07,820 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:08,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:08,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:08,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:08,388 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:08,388 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:08,389 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:22:08,413 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:22:08,417 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:22:08,429 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:08,429 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:09,008 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,127 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:09,127 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:22:09,152 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:22:09,156 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:22:09,166 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:09,166 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:09,620 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,687 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,722 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:09,722 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:22:09,745 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:22:09,749 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:22:09,762 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:09,763 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:09,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:09,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,021 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,021 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:10,022 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:22:10,047 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:22:10,051 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:22:10,061 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:10,062 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:10,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,365 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:10,365 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:22:10,388 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:22:10,392 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:22:10,405 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:10,405 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:10,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:10,660 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:10,660 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:22:10,687 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:22:10,692 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:22:10,701 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:10,702 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:11,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:11,257 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:11,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:11,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:11,332 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:11,333 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:22:11,364 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:22:11,367 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:22:11,383 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:11,383 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:12,028 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,148 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,148 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:12,148 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:22:12,173 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:22:12,176 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:22:12,186 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:12,187 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:12,702 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:12,823 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:12,823 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:22:12,845 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:22:12,849 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:22:12,861 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:12,861 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:13,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,430 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:13,430 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:22:13,453 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:22:13,457 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:22:13,467 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:13,468 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:13,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:13,994 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:13,994 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:22:14,020 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:22:14,023 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:22:14,031 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:14,031 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:14,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:14,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:14,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:14,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:14,914 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:14,914 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:22:14,940 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:22:14,943 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:22:14,947 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:14,948 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:15,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:15,759 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:15,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:15,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:15,830 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:15,830 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:22:15,852 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:22:15,856 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:22:15,856 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:15,856 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:16,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:16,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:16,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:16,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 19, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 20, 128])"))
2023-10-16 07:22:16,775 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"))
2023-10-16 07:22:16,775 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:22:16,805 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:22:16,806 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:22:16,806 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:16,806 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:16,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:16,809 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:16,810 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:16,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:16,811 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:16,811 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:22:16,811 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:22:16,812 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:22:16,812 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:16,812 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:17,621 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:17,650 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:17,679 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:17,707 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:17,714 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:22:17,714 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:22:17,752 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:22:17,753 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:22:17,754 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:22:17,754 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:17,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,756 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:17,757 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:22:17,757 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:22:17,757 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:22:17,761 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 21])", "<class 'int'>: 20")
2023-10-16 07:22:17,761 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:17,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,764 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,764 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:17,765 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:17,765 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:22:17,768 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:22:17,772 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:22:17,775 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:17,775 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:18,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:18,534 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:18,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:18,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:18,597 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:18,597 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:22:18,620 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:22:18,623 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:22:18,631 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:18,631 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:19,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:19,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:19,371 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:19,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:19,410 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:19,410 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:22:19,436 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:22:19,439 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:22:19,451 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:19,451 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:20,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,089 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,161 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:20,161 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:22:20,183 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:22:20,186 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:22:20,199 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:20,199 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:20,702 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:20,822 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:20,822 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:22:20,845 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:22:20,849 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:22:20,860 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:20,861 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:21,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,222 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,223 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:21,223 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:22:21,262 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:22:21,265 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:22:21,279 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:21,279 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:21,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,769 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:21,809 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:21,809 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:22:21,837 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:22:21,840 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:22:21,851 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:21,851 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:22,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,518 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,557 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:22,557 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:22:22,590 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:22:22,593 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:22:22,610 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:22,610 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:22,760 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:22,882 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:22,882 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:22:22,924 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:22:22,928 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:22:22,941 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:22,941 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:22,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,098 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:23,098 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:22:23,145 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:22:23,149 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:22:23,160 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:23,161 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:23,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,463 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,542 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:23,542 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:22:23,577 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:22:23,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:22:23,593 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:23,593 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:23,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,835 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:23,835 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:23,835 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:22:23,877 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:22:23,881 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:22:23,893 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:23,894 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:24,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:24,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:24,371 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:24,411 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:24,411 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:24,412 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:22:24,433 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:22:24,437 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:22:24,449 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:24,449 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:24,948 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:24,981 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:25,016 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:25,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:25,049 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:25,049 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:22:25,071 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:22:25,074 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:22:25,086 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:25,087 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:25,571 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:25,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:25,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:25,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:25,690 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:25,690 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:22:25,720 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:22:25,726 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:22:25,736 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:25,736 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:26,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,314 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,354 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:26,354 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:22:26,375 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:22:26,379 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:22:26,391 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:26,391 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:26,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,649 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:26,649 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:22:26,673 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:22:26,677 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:22:26,686 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:26,686 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:26,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:26,931 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:26,931 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:22:26,964 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:22:26,967 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:22:26,984 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:26,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:27,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,382 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,415 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:27,415 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:22:27,442 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:22:27,446 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:22:27,456 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:27,456 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:27,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,788 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,900 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:27,901 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:27,901 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:22:27,935 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:22:27,938 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:22:27,949 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:27,949 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:28,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,197 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,232 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,233 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:28,233 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:22:28,288 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:22:28,293 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:22:28,304 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:28,304 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:28,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:28,924 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:28,924 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:22:28,954 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:22:28,957 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:22:28,962 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:28,963 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:29,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:29,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:29,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:29,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:29,771 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:29,771 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:22:29,797 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:22:29,801 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:22:29,804 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:29,805 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:30,662 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:30,702 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:30,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:30,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:30,779 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:30,779 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:22:30,802 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:22:30,806 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:22:30,810 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:30,810 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:31,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:31,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:31,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:31,688 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:31,689 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:31,689 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:22:31,721 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:22:31,726 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:22:31,731 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:31,732 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:32,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,086 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,157 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:32,157 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:22:32,203 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:22:32,207 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:22:32,219 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:32,219 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:32,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,483 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,554 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,555 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:32,555 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:22:32,605 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:22:32,609 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:22:32,620 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:32,620 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:32,688 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,769 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,805 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:32,806 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:32,806 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:22:32,852 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:22:32,855 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:22:32,859 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:32,859 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:33,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:33,440 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:33,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:33,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:33,520 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:33,520 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:22:33,545 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:22:33,549 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:22:33,559 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:33,560 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:34,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:34,251 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:34,289 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:34,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:34,330 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:34,331 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:22:34,354 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:22:34,358 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:22:34,370 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:34,370 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:34,886 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:34,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:34,966 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,005 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:35,005 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:22:35,042 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:22:35,046 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:22:35,058 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:35,058 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:35,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,221 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:35,222 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:22:35,269 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:22:35,274 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:22:35,275 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:35,275 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:35,327 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 20, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 21, 128])"))
2023-10-16 07:22:35,447 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"))
2023-10-16 07:22:35,447 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:22:35,499 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:22:35,500 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:22:35,500 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:35,501 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:35,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,504 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,506 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:35,506 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:22:35,506 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:22:35,507 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:22:35,508 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:35,508 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:35,736 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:35,765 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:35,794 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:35,824 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:35,827 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:22:35,828 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:22:35,884 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:22:35,885 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:22:35,886 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:22:35,886 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:35,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,889 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:35,889 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:22:35,890 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:22:35,890 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:22:35,904 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 22])", "<class 'int'>: 21")
2023-10-16 07:22:35,905 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:35,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,909 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:35,909 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:35,909 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:22:35,912 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:22:35,916 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:22:35,930 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:35,930 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:36,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,334 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,405 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:36,405 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:22:36,426 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:22:36,430 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:22:36,438 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:36,438 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:36,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,711 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:36,799 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:36,799 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:22:36,821 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:22:36,825 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:22:36,828 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:36,828 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:37,359 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:37,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:37,431 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:37,466 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:37,466 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:37,466 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:22:37,485 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:22:37,489 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:22:37,492 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:37,492 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:38,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,302 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:38,302 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:22:38,325 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:22:38,329 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:22:38,332 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:38,332 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:38,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:38,932 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:38,932 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:22:38,963 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:22:38,967 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:22:38,971 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:38,971 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:39,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,449 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:39,449 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:22:39,485 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:22:39,489 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:22:39,493 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:39,493 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:39,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,825 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,900 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:39,900 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:39,900 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:22:39,947 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:22:39,950 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:22:39,954 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:39,954 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:40,273 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,385 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,385 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:40,385 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:22:40,412 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:22:40,415 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:22:40,418 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:40,419 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:40,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,787 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,825 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:40,826 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:40,826 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:22:40,856 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:22:40,860 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:22:40,863 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:40,864 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:41,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,282 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:41,282 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:22:41,316 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:22:41,319 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:22:41,323 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:41,323 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:41,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,747 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,786 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:41,824 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:41,824 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:22:41,846 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:22:41,849 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:22:41,853 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:41,853 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:42,580 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:42,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:42,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:42,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:42,682 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:42,682 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:22:42,707 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:22:42,711 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:22:42,715 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:42,715 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:43,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:43,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:43,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:43,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:43,516 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:43,516 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:22:43,540 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:22:43,543 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:22:43,546 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:43,547 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:44,291 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:44,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:44,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:44,435 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:44,436 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:44,436 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:22:44,467 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:22:44,473 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:22:44,487 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:44,487 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:45,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:45,264 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:45,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:45,337 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:45,338 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:45,338 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:22:45,361 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:22:45,364 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:22:45,377 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:45,377 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:45,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:45,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:46,015 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:46,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:46,054 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:46,054 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:22:46,079 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:22:46,082 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:22:46,092 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:46,092 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:46,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:46,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:46,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:46,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:46,659 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:46,659 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:22:46,682 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:22:46,686 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:22:46,700 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:46,700 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:47,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,326 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:47,326 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:22:47,352 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:22:47,355 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:22:47,365 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:47,366 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:47,759 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:47,884 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:47,885 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:22:47,914 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:22:47,921 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:22:47,937 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:47,937 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:48,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,086 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,159 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:48,160 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:22:48,190 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:22:48,194 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:22:48,205 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:48,205 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:48,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,512 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:48,558 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:48,559 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:22:48,593 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:22:48,596 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:22:48,608 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:48,608 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:49,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,255 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:49,255 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:22:49,282 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:22:49,286 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:22:49,296 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:49,296 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:49,762 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,825 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:49,856 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:49,856 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:22:49,880 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:22:49,884 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:22:49,898 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:49,898 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:50,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,595 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:50,596 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:22:50,621 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:22:50,625 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:22:50,633 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:50,633 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:50,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:50,815 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:50,815 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:22:50,837 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:22:50,840 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:22:50,852 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:50,852 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:51,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,108 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,108 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:51,108 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:22:51,134 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:22:51,138 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:22:51,149 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:51,149 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:51,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,278 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,316 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:51,316 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:22:51,339 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:22:51,342 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:22:51,355 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:51,355 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:51,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,992 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:51,993 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:51,993 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:22:52,018 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:22:52,022 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:22:52,031 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:52,032 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:52,431 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:52,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:52,494 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:52,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:52,530 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:52,531 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:22:52,554 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:22:52,558 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:22:52,569 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:52,569 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:53,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,390 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:53,391 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:22:53,417 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:22:53,420 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:22:53,431 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:53,432 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:53,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,817 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,897 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:53,897 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:53,897 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:22:53,920 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:22:53,923 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:22:53,924 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:53,924 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:53,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:54,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:54,053 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:54,091 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 21, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 22, 128])"))
2023-10-16 07:22:54,091 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"))
2023-10-16 07:22:54,091 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:22:54,117 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:22:54,118 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:22:54,119 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:54,119 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:54,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,123 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:54,123 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:22:54,123 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:22:54,124 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:22:54,124 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:54,124 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:54,448 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:54,478 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:54,508 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:54,538 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:22:54,542 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:22:54,542 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:22:54,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:22:54,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:22:54,582 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:22:54,582 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:54,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,583 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,584 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:54,584 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:22:54,585 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:22:54,585 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:22:54,589 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 23])", "<class 'int'>: 22")
2023-10-16 07:22:54,589 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:22:54,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:22:54,593 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:22:54,593 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:22:54,596 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:22:54,600 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:22:54,603 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:54,603 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:55,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:55,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:55,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:55,425 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:55,425 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:55,425 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:22:55,447 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:22:55,450 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:22:55,458 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:55,459 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:56,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,213 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:56,213 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:22:56,238 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:22:56,242 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:22:56,256 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:56,256 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:56,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,581 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,654 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:56,654 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:56,654 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:22:56,695 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:22:56,699 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:22:56,712 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:56,713 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:57,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,178 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,250 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:57,250 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:22:57,276 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:22:57,279 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:22:57,291 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:57,291 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:57,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,906 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:57,906 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:57,906 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:22:57,931 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:22:57,934 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:22:57,947 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:57,947 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:58,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,430 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:58,430 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:22:58,452 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:22:58,455 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:22:58,468 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:58,469 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:58,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:58,948 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:58,948 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:22:58,973 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:22:58,977 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:22:58,992 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:58,992 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:22:59,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:59,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:59,528 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:59,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:22:59,570 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:22:59,570 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:22:59,593 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:22:59,597 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:22:59,611 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:22:59,611 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:00,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,250 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:00,250 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:23:00,274 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:23:00,278 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:00,293 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:00,293 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:00,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,651 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,687 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:00,724 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:00,725 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:23:00,748 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:00,752 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:00,767 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:00,767 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:01,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:01,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:01,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:01,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:01,402 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:01,402 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:23:01,423 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:01,426 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:01,439 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:01,439 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:01,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:01,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:01,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:02,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:02,020 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:02,020 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:23:02,043 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:02,046 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:02,059 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:02,059 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:02,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:02,552 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:02,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:02,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:02,624 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:02,624 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:23:02,646 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:02,650 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:02,662 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:02,662 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:03,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,222 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,262 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,262 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:03,262 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:23:03,286 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:03,290 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:03,294 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:03,295 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:03,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:03,563 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:03,563 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:23:03,606 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:03,610 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:23:03,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:03,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:04,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:04,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:04,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:04,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:04,453 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:04,453 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:23:04,478 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:23:04,482 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:23:04,485 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:04,485 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:05,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:05,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:05,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:05,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:05,361 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:05,361 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:23:05,391 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:23:05,395 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:23:05,407 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:05,408 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:06,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:06,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:06,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:06,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:06,275 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:06,275 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:23:06,301 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:23:06,304 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:23:06,314 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:06,314 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:06,982 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,093 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:07,093 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:23:07,118 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:23:07,121 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:23:07,132 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:07,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:07,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:07,753 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:07,753 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:23:07,791 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:23:07,795 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:23:07,808 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:07,808 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:08,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,263 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,263 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:08,264 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:23:08,288 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:23:08,293 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:23:08,313 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:08,313 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:08,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,607 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:08,682 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:08,682 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:23:08,735 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:23:08,738 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:23:08,749 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:08,749 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:08,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,084 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,085 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:09,085 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:23:09,131 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:23:09,135 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:23:09,147 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:09,148 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:09,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,266 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,308 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:09,308 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:23:09,362 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:23:09,366 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:23:09,376 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:09,376 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:09,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:09,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:10,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:10,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:10,068 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:10,068 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:23:10,099 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:23:10,104 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:23:10,112 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:10,112 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:10,758 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:10,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:10,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:10,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:10,868 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:10,868 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:23:10,905 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:23:10,908 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:23:10,918 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:10,918 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:11,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:11,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:11,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:11,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:11,821 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:11,821 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:23:11,850 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:23:11,853 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:23:11,865 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:11,866 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:12,278 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,404 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:12,404 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:23:12,458 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:23:12,461 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:23:12,472 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:12,472 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:12,634 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,673 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,764 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:12,764 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:12,764 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:23:12,812 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:23:12,816 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:23:12,831 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:12,831 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:13,084 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,200 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:13,200 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:23:13,251 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:23:13,255 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:23:13,265 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:13,265 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:13,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:13,913 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:13,913 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:23:13,940 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:23:13,943 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:23:13,944 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:13,944 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:14,537 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:14,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:14,608 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:14,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 22, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 23, 128])"))
2023-10-16 07:23:14,645 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"))
2023-10-16 07:23:14,645 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:23:14,672 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:23:14,673 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:23:14,674 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:14,674 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:14,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:14,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:14,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:14,677 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:14,677 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:14,677 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:23:14,678 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:23:14,678 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:23:14,678 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:14,678 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:15,195 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:15,222 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:15,249 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:15,276 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:15,279 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:23:15,280 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:23:15,347 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:23:15,347 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:23:15,348 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:23:15,348 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:15,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,351 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:15,351 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:23:15,351 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:23:15,352 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:23:15,364 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 24])", "<class 'int'>: 23")
2023-10-16 07:23:15,364 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:15,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,368 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:15,368 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:15,368 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:23:15,371 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:23:15,374 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:23:15,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:15,378 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:15,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:15,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:15,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:15,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:15,915 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:15,915 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:23:15,946 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:23:15,950 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:23:15,957 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:15,957 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:16,512 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,632 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,633 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:16,633 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:23:16,661 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:23:16,664 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:23:16,668 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:16,668 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:16,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,925 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,961 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:16,997 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:16,997 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:23:17,026 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:23:17,030 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:23:17,033 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:17,034 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:17,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:17,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:17,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:17,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:17,654 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:17,654 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:23:17,684 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:23:17,687 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:23:17,691 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:17,691 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:18,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:18,345 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:18,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:18,418 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:18,419 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:18,419 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:23:18,452 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:23:18,455 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:23:18,459 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:18,459 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:19,246 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:19,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:19,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:19,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:19,376 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:19,376 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:23:19,399 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:23:19,403 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:23:19,410 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:19,410 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:20,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:20,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:20,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:20,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:20,274 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:20,275 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:23:20,299 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:23:20,302 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:23:20,315 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:20,315 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:20,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:20,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:20,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,012 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,013 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:21,013 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:23:21,049 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:23:21,053 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:23:21,066 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:21,066 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:21,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,472 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,472 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:21,472 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:23:21,508 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:23:21,512 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:21,525 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:21,525 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:21,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,816 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,896 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:21,896 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:21,896 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:23:21,933 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:21,936 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:21,950 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:21,950 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:21,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,103 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:22,103 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:23:22,148 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:22,152 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:22,157 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:22,157 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:22,248 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,351 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:22,351 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:23:22,395 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:22,399 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:22,402 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:22,402 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:22,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,893 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:22,894 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:22,894 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:23:22,940 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:22,944 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:22,947 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:22,947 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:23,334 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,447 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:23,447 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:23:23,491 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:23,495 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:23,505 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:23,505 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:23,829 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:23,951 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:23,952 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:23:23,978 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:23,982 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:23:23,995 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:23,995 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:24,075 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,197 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,198 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:24,198 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:23:24,256 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:23:24,259 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:23:24,269 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:24,269 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:24,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,489 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,489 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:24,489 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:23:24,535 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:23:24,539 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:23:24,552 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:24,553 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:24,938 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:24,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:25,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:25,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:25,043 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:25,043 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:23:25,077 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:23:25,080 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:23:25,088 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:25,088 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:25,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:25,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:25,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:25,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:25,821 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:25,821 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:23:25,843 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:23:25,846 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:23:25,852 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:25,852 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:26,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:26,559 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:26,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:26,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:26,628 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:26,628 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:23:26,654 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:23:26,657 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:23:26,668 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:26,668 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:27,382 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,458 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,497 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:27,497 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:23:27,520 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:23:27,524 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:23:27,537 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:27,538 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:27,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,732 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,732 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:27,733 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:23:27,760 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:23:27,764 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:23:27,775 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:27,775 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:27,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,893 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,928 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:27,928 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:27,928 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:23:27,952 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:23:27,957 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:23:27,971 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:27,971 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:28,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,194 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:28,195 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:23:28,221 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:23:28,225 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:23:28,234 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:28,234 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:28,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,766 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:28,839 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:28,839 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:23:28,863 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:23:28,867 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:23:28,880 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:28,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:29,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:29,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:29,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:29,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:29,596 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:29,597 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:23:29,622 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:23:29,625 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:23:29,636 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:29,636 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:30,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,267 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,267 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:30,268 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:23:30,290 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:23:30,293 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:23:30,307 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:30,307 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:30,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,388 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,460 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,461 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:30,461 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:23:30,487 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:23:30,490 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:23:30,494 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:30,494 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:30,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:30,655 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:30,655 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:23:30,680 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:23:30,684 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:23:30,688 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:30,688 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:31,078 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:31,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:31,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:31,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:31,195 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:31,195 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:23:31,220 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:23:31,223 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:23:31,233 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:31,234 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:31,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,015 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,104 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:32,104 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:23:32,128 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:23:32,131 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:23:32,132 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:32,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:32,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 23, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 24, 128])"))
2023-10-16 07:23:32,806 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"))
2023-10-16 07:23:32,806 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:23:32,829 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:23:32,830 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:23:32,830 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:32,831 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:32,831 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:32,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:32,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:32,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:32,834 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:32,834 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:23:32,834 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:23:32,835 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:23:32,835 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:32,835 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:33,350 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:33,377 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:33,404 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:33,431 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:33,434 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:23:33,435 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:23:33,474 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:23:33,475 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:23:33,475 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:23:33,475 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:33,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,478 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:33,478 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:23:33,479 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:23:33,479 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:23:33,483 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 25])", "<class 'int'>: 24")
2023-10-16 07:23:33,483 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:33,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,486 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:33,487 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:33,487 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:23:33,490 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:23:33,493 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:23:33,507 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:33,507 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:34,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,193 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,261 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:34,261 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:23:34,284 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:23:34,287 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:23:34,297 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:34,298 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:34,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,731 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,807 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:34,807 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:34,807 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:23:34,831 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:23:34,835 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:23:34,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:34,848 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:35,338 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,448 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:35,449 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:23:35,472 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:23:35,475 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:23:35,488 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:35,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:35,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,977 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:35,977 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:35,977 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:23:36,001 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:23:36,005 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:23:36,018 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:36,018 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:36,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,253 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,288 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,288 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:36,289 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:23:36,322 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:23:36,325 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:23:36,338 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:36,339 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:36,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,858 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,929 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:36,930 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:36,930 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:23:36,953 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:23:36,956 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:23:36,969 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:36,969 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:37,455 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:37,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:37,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:37,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:37,585 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:37,585 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:23:37,609 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:23:37,613 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:23:37,625 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:37,625 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:37,983 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,093 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:38,093 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:23:38,118 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:23:38,122 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:23:38,134 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:38,134 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:38,449 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,558 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:38,559 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:23:38,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:23:38,585 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:38,597 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:38,597 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:38,787 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:38,900 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:38,900 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:23:38,924 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:38,927 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:38,940 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:38,940 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:38,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,107 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:39,108 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:23:39,130 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:39,134 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:39,148 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:39,148 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:39,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,359 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,360 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:39,360 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:23:39,382 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:39,385 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:39,399 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:39,399 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:39,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,664 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:39,664 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:23:39,687 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:39,691 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:39,705 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:39,705 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:39,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,904 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:39,986 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:39,986 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:23:40,008 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:40,011 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:40,022 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:40,022 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:40,442 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,483 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,566 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:40,566 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:23:40,596 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:40,600 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:23:40,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:40,613 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:40,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:40,849 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:40,849 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:23:40,874 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:23:40,877 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:23:40,888 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:40,888 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:41,002 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,109 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:41,109 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:23:41,132 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:23:41,136 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:23:41,149 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:41,149 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:41,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,598 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:41,598 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:23:41,634 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:23:41,637 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:23:41,652 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:41,652 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:41,835 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:41,950 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:41,950 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:23:41,972 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:23:41,976 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:23:41,990 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:41,990 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:42,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:42,512 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:42,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:42,588 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:42,588 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:42,589 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:23:42,616 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:23:42,620 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:23:42,632 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:42,632 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:43,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,245 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:43,245 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:23:43,269 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:23:43,272 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:23:43,284 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:43,284 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:43,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:43,995 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:43,995 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:23:44,021 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:23:44,024 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:23:44,029 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:44,029 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:44,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:44,608 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:44,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:44,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:44,682 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:44,683 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:23:44,706 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:23:44,710 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:23:44,714 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:44,714 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:45,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:45,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:45,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:45,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:45,665 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:45,666 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:23:45,689 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:23:45,693 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:23:45,699 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:45,699 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:46,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:46,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:46,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:46,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:46,535 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:46,535 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:23:46,560 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:23:46,563 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:23:46,575 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:46,576 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:47,266 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:47,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:47,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:47,373 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:47,373 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:47,374 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:23:47,399 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:23:47,403 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:23:47,406 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:47,406 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:47,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:47,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:47,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:48,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:48,025 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:48,025 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:23:48,046 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:23:48,050 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:23:48,054 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:48,054 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:48,789 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:48,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:48,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:48,898 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:48,899 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:48,899 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:23:48,931 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:23:48,937 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:23:48,952 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:48,953 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:49,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:49,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:49,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:49,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:49,795 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:49,795 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:23:49,818 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:23:49,821 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:23:49,834 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:49,834 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:50,371 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,481 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:50,481 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:23:50,514 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:23:50,518 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:23:50,521 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:50,521 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:50,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,650 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:50,686 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:50,686 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:23:50,731 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:23:50,735 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:23:50,735 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:50,736 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:50,982 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:51,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:51,056 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:51,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 24, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 25, 128])"))
2023-10-16 07:23:51,093 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"))
2023-10-16 07:23:51,093 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:23:51,144 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:23:51,144 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:23:51,145 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:51,145 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:51,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,148 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,149 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,150 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,150 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:51,150 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:23:51,150 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:23:51,151 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:23:51,151 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:51,151 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:51,418 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:51,450 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:51,482 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:51,515 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:23:51,526 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:23:51,526 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:23:51,593 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:23:51,594 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:23:51,595 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:23:51,595 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:51,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,597 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:51,597 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:23:51,598 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:23:51,598 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:23:51,611 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 26])", "<class 'int'>: 25")
2023-10-16 07:23:51,611 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:23:51,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:23:51,614 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:23:51,614 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:23:51,617 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:23:51,620 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:23:51,633 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:51,634 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:51,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:51,787 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:51,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:51,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:51,858 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:51,858 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:23:51,910 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:23:51,914 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:23:51,927 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:51,927 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:52,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,202 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,248 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,248 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:52,249 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:23:52,287 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:23:52,291 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:23:52,304 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:52,304 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:52,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:52,921 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:52,921 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:23:52,944 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:23:52,947 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:23:52,959 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:52,959 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:53,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,452 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:53,452 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:23:53,495 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:23:53,499 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:23:53,516 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:53,516 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:53,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:53,866 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:53,866 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:23:53,896 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:23:53,900 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:23:53,914 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:53,914 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:53,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,124 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:54,124 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:23:54,170 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:23:54,173 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:23:54,186 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:54,186 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:54,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,295 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,388 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:54,388 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:23:54,427 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:23:54,430 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:23:54,443 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:54,444 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:54,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:54,882 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:54,882 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:23:54,906 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:23:54,910 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:23:54,923 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:54,923 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:55,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,449 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:55,449 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:23:55,469 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:23:55,473 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:55,484 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:55,484 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:55,807 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:55,927 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:55,927 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:23:55,949 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:23:55,953 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:55,967 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:55,967 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:56,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:56,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:56,537 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:56,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:56,580 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:56,580 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:23:56,604 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:23:56,608 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:56,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:56,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:57,099 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:57,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:57,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:57,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:57,203 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:57,203 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:23:57,229 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:23:57,233 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:57,237 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:57,237 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:57,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:57,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:57,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:58,033 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:58,033 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:58,033 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:23:58,065 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:23:58,069 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:58,073 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:58,074 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:58,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:58,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:58,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:58,981 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:58,982 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:58,982 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:23:59,008 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:23:59,012 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:59,023 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:59,023 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:23:59,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:59,859 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:59,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:59,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:23:59,940 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:23:59,940 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:23:59,965 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:23:59,969 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:23:59,982 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:23:59,982 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:00,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:00,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:00,646 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:00,686 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:00,686 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:00,686 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:24:00,712 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:24:00,715 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:00,728 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:00,728 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:01,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:01,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:01,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:01,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:01,350 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:01,350 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:24:01,372 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:01,376 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:01,384 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:01,385 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:01,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,021 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,104 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:02,105 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:24:02,131 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:02,135 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:02,143 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:02,143 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:02,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,819 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,860 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:02,860 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:02,860 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:24:02,889 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:02,892 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:24:02,898 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:02,898 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:03,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,179 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,263 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,263 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:03,263 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:24:03,317 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:24:03,321 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:24:03,332 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:03,333 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:03,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:03,793 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:03,793 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:24:03,839 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:24:03,842 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:24:03,854 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:03,854 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:04,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:04,441 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:04,475 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:04,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:04,509 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:04,509 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:24:04,539 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:24:04,542 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:24:04,552 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:04,553 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:05,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,129 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,196 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,196 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:05,196 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:24:05,220 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:24:05,223 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:24:05,235 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:05,236 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:05,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:05,902 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:05,902 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:24:05,929 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:24:05,932 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:24:05,943 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:05,944 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:06,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:06,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:06,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:06,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:06,545 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:06,546 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:24:06,569 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:24:06,573 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:24:06,580 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:06,581 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:07,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:07,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:07,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:07,257 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:07,258 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:07,258 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:24:07,289 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:24:07,292 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:24:07,303 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:07,303 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:07,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,015 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,100 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:08,100 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:24:08,124 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:24:08,127 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:24:08,140 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:08,140 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:08,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,289 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,324 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,325 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:08,325 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:24:08,377 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:24:08,381 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:24:08,385 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:08,385 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:08,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,475 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,552 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,553 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:08,553 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:24:08,605 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:24:08,609 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:24:08,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:08,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:08,858 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:08,989 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:08,990 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:24:09,049 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:24:09,053 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:24:09,063 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:09,063 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:09,392 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,451 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,533 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:09,533 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:24:09,556 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:24:09,559 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:24:09,560 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:09,560 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:09,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 25, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 26, 128])"))
2023-10-16 07:24:09,749 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"))
2023-10-16 07:24:09,749 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:24:09,776 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:24:09,777 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:24:09,778 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:09,778 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:09,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,781 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:09,782 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:24:09,782 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:24:09,783 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:24:09,783 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:09,783 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:09,825 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:09,857 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:09,888 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:09,916 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:09,919 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:24:09,919 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:24:09,984 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:24:09,984 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:24:09,985 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:24:09,985 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:09,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,988 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:09,988 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:24:09,989 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:24:09,989 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:24:09,993 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 27])", "<class 'int'>: 26")
2023-10-16 07:24:09,993 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:09,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:09,996 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:09,996 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:24:09,999 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:24:10,002 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:24:10,017 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:10,018 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:10,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,429 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:10,429 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:24:10,456 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:24:10,460 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:24:10,472 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:10,472 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:10,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,686 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,686 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:10,687 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:24:10,716 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:24:10,720 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:24:10,732 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:10,733 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:10,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:10,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,054 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:11,054 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:24:11,100 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:24:11,104 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:24:11,117 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:11,117 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:11,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,396 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,466 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,467 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:11,467 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:24:11,498 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:24:11,503 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:24:11,521 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:11,521 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:11,937 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:11,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,061 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:12,061 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:24:12,083 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:24:12,087 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:24:12,105 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:12,105 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:12,168 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,247 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,282 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:12,282 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:24:12,306 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:24:12,309 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:24:12,321 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:12,322 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:12,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,679 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:12,724 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:12,725 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:24:12,749 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:24:12,752 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:24:12,756 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:12,756 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:13,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:13,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:13,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:13,193 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:13,193 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:13,193 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:24:13,227 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:24:13,231 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:24:13,234 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:13,235 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:13,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:13,948 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:13,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:14,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:14,030 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:14,030 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:24:14,058 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:24:14,061 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:24:14,064 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:14,065 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:14,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:14,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:14,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:14,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:14,914 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:14,914 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:24:14,939 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:24:14,942 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:24:14,945 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:14,945 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:15,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:15,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:15,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:15,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:15,798 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:15,799 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:24:15,822 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:24:15,826 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:24:15,833 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:15,833 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:16,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:16,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:16,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:16,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:16,617 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:16,617 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:24:16,643 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:24:16,647 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:24:16,659 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:16,659 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:17,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,037 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,105 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,105 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:17,105 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:24:17,146 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:24:17,150 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:24:17,164 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:17,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:17,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,382 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:17,382 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:24:17,428 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:24:17,432 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:24:17,442 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:17,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:17,860 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:17,979 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:17,979 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:24:18,007 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:24:18,011 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:24:18,024 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:18,024 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:18,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,281 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:18,281 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:24:18,325 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:24:18,328 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:18,339 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:18,340 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:18,467 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,582 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:18,582 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:24:18,603 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:18,607 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:18,620 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:18,620 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:18,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:18,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,023 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,063 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:19,063 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:24:19,090 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:19,094 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:19,106 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:19,106 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:19,193 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,264 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,300 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:19,301 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:24:19,322 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:19,326 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:24:19,340 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:19,340 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:19,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,504 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,505 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:19,505 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:24:19,529 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:24:19,532 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:24:19,542 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:19,543 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:19,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,764 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:19,801 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:19,801 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:24:19,825 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:24:19,828 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:24:19,839 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:19,839 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:20,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:20,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:20,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:20,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:20,522 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:20,523 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:24:20,548 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:24:20,552 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:24:20,556 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:20,557 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:21,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,201 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,242 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:21,242 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:24:21,274 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:24:21,277 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:24:21,282 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:21,282 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:21,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,680 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:21,715 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:21,715 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:24:21,743 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:24:21,746 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:24:21,752 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:21,752 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:22,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,263 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,264 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:22,264 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:24:22,309 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:24:22,312 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:24:22,326 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:22,326 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:22,577 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,611 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:22,685 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:22,685 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:24:22,721 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:24:22,725 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:24:22,730 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:22,731 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:23,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,083 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,154 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:23,154 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:24:23,177 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:24:23,181 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:24:23,184 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:23,184 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:23,386 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,502 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:23,502 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:24:23,525 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:24:23,529 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:24:23,540 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:23,540 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:23,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,915 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:23,952 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:23,952 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:24:23,976 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:24:23,980 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:24:23,994 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:23,994 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:24,253 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:24,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:24,338 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:24,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:24,377 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:24,377 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:24:24,404 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:24:24,408 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:24:24,412 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:24,412 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:24,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:24,953 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:24,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:25,023 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:25,024 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:25,024 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:24:25,048 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:24:25,051 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:24:25,051 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:25,052 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:25,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:25,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:25,874 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:25,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 26, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 27, 128])"))
2023-10-16 07:24:25,913 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"))
2023-10-16 07:24:25,913 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:24:25,939 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:24:25,940 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:24:25,940 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:25,940 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:25,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:25,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:25,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:25,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:25,943 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:25,943 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:24:25,944 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:24:25,944 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:24:25,944 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:25,945 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:26,459 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:26,491 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:26,523 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:26,556 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:26,557 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:24:26,557 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:24:26,595 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:24:26,595 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:24:26,596 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:24:26,596 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:26,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,599 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:26,599 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:24:26,600 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:24:26,600 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:24:26,615 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 28])", "<class 'int'>: 27")
2023-10-16 07:24:26,615 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:26,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:26,619 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:26,619 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:24:26,624 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:24:26,630 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:24:26,649 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:26,650 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:27,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,169 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,169 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:27,169 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:24:27,194 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:24:27,197 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:24:27,211 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:27,211 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:27,536 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,650 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:27,650 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:24:27,673 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:24:27,676 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:24:27,687 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:27,687 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:27,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:27,850 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:27,851 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:24:27,874 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:24:27,877 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:24:27,880 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:27,881 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:28,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:28,374 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:28,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:28,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:28,457 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:28,457 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:24:28,481 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:24:28,484 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:24:28,487 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:28,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:29,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:29,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:29,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:29,399 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:29,399 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:29,399 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:24:29,423 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:24:29,426 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:24:29,432 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:29,432 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:30,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:30,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:30,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:30,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:30,341 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:30,342 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:24:30,365 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:24:30,369 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:24:30,373 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:30,373 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:31,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:31,169 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:31,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:31,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:31,244 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:31,244 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:24:31,269 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:24:31,273 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:24:31,288 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:31,288 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:31,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,074 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,075 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:32,075 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:24:32,099 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:24:32,102 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:24:32,117 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:32,118 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:32,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:32,660 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:32,660 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:24:32,693 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:24:32,697 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:24:32,710 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:32,710 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:33,219 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,256 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,332 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:33,332 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:24:33,357 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:24:33,360 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:24:33,374 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:33,374 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:33,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,937 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:33,975 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:33,975 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:24:33,998 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:24:34,002 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:24:34,011 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:34,011 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:34,475 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:34,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:34,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:34,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:34,598 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:34,598 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:24:34,624 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:24:34,628 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:24:34,632 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:34,633 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:35,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,091 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,128 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:35,129 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:24:35,151 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:24:35,154 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:24:35,167 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:35,167 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:35,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,641 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:35,641 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:24:35,666 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:24:35,670 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:24:35,674 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:35,675 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:35,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,863 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:35,863 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:35,863 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:24:35,895 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:24:35,898 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:24:35,901 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:35,902 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:36,320 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:36,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:36,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:36,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:36,447 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:36,447 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:24:36,478 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:24:36,482 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:36,485 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:36,485 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:37,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,331 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:37,331 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:24:37,354 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:37,358 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:37,361 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:37,362 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:37,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,926 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:37,999 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:37,999 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:24:38,029 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:38,033 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:38,036 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:38,036 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:38,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:38,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:38,833 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:38,874 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:38,875 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:38,875 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:24:38,899 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:38,904 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:24:38,908 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:38,909 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:39,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:39,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:39,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:39,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:39,776 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:39,776 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:24:39,802 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:24:39,806 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:24:39,809 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:39,810 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:40,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:40,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:40,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:40,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:40,640 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:40,640 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:24:40,666 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:24:40,669 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:24:40,674 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:40,675 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:40,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:40,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:41,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:41,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:41,059 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:41,059 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:24:41,110 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:24:41,114 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:24:41,125 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:41,125 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:41,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:41,424 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:41,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:41,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:41,507 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:41,507 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:24:41,556 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:24:41,559 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:24:41,574 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:41,574 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:42,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,151 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:42,151 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:24:42,176 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:24:42,179 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:24:42,187 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:42,187 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:42,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,433 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,479 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:42,479 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:24:42,504 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:24:42,507 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:24:42,510 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:42,511 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:42,937 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:42,978 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:43,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:43,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:43,059 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:43,059 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:24:43,083 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:24:43,086 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:24:43,096 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:43,096 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:43,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:43,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:43,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:43,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:43,592 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:43,592 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:24:43,612 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:24:43,615 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:24:43,629 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:43,629 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:44,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:44,358 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:44,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:44,455 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:44,455 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:44,456 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:24:44,481 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:24:44,485 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:24:44,489 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:44,489 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:44,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:45,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:45,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:45,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:45,109 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:45,109 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:24:45,134 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:24:45,138 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:24:45,142 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:45,142 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:45,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:45,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:45,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:46,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:46,031 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:46,031 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:24:46,057 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:24:46,061 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:24:46,072 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:46,073 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:46,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:46,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:46,921 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:46,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:46,964 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:46,964 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:24:46,988 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:24:46,991 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:24:46,992 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:46,992 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:47,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:47,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:47,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:47,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 27, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 28, 128])"))
2023-10-16 07:24:47,668 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"))
2023-10-16 07:24:47,668 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:24:47,694 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:24:47,695 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:24:47,696 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:47,696 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:47,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:47,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:47,699 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:47,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:47,700 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:47,700 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:24:47,700 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:24:47,701 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:24:47,701 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:47,701 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:48,409 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:48,438 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:48,465 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:48,491 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:24:48,495 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:24:48,496 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:24:48,546 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:24:48,546 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:24:48,547 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:24:48,547 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:48,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,550 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,550 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:48,550 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:24:48,550 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:24:48,551 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:24:48,554 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 29])", "<class 'int'>: 28")
2023-10-16 07:24:48,554 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:24:48,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:24:48,558 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:24:48,558 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:24:48,561 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:24:48,565 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:24:48,580 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:48,581 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:49,306 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:49,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:49,393 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:49,437 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:49,437 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:49,437 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:24:49,461 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:24:49,464 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:24:49,474 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:49,474 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:49,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,061 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,098 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:50,098 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:24:50,119 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:24:50,123 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:24:50,136 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:50,136 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:50,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:50,871 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:50,871 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:24:50,904 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:24:50,907 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:24:50,921 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:50,921 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:51,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,345 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,385 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,424 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,425 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:51,425 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:24:51,457 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:24:51,461 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:24:51,474 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:51,474 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:51,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,732 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:51,775 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:51,776 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:24:51,821 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:24:51,825 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:24:51,837 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:51,838 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:52,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:52,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:52,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:52,496 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:52,496 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:52,496 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:24:52,520 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:24:52,524 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:24:52,538 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:52,539 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:52,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:52,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:52,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:53,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:53,024 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:53,024 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:24:53,058 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:24:53,061 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:24:53,068 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:53,069 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:53,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:53,520 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:53,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:53,603 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:53,603 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:53,604 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:24:53,649 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:24:53,655 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:24:53,664 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:53,664 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:53,993 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,113 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,114 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:54,114 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:24:54,157 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:24:54,160 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:24:54,173 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:54,173 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:54,466 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,580 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:54,580 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:24:54,630 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:24:54,634 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:24:54,648 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:54,648 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:54,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,807 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,842 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:54,842 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:54,843 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:24:54,890 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:24:54,894 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:24:54,907 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:54,907 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:55,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,204 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:55,204 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:24:55,249 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:24:55,252 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:24:55,265 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:55,265 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:55,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:55,930 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:55,931 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:24:55,959 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:24:55,963 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:24:55,976 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:55,976 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:56,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:56,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:56,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:56,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:56,585 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:56,585 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:24:56,608 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:24:56,611 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:24:56,622 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:56,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:56,977 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,097 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:57,098 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:24:57,122 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:24:57,125 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:24:57,139 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:57,139 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:57,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:57,867 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:57,867 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:24:57,903 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:24:57,909 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:57,924 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:57,924 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:58,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:58,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:58,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:58,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:58,547 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:58,547 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:24:58,571 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:24:58,574 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:58,587 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:58,587 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:59,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,106 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,173 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,174 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:59,174 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:24:59,200 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:24:59,203 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:59,214 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:59,214 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:24:59,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:24:59,822 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:24:59,822 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:24:59,845 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:24:59,849 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:24:59,861 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:24:59,862 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:00,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:00,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:00,496 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:00,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:00,531 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:00,531 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:25:00,557 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:25:00,560 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:00,570 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:00,570 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:01,048 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,091 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,173 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,174 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:01,174 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:25:01,199 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:01,203 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:01,217 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:01,217 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:01,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,444 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:01,444 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:25:01,473 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:01,476 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:01,480 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:01,480 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:01,600 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,636 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,716 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:01,716 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:01,716 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:25:01,740 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:01,744 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:01,748 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:01,749 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:02,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:02,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:02,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:02,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:02,588 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:02,588 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:25:02,614 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:02,618 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:02,624 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:02,624 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:03,399 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:03,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:03,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:03,607 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:03,608 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:03,608 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:25:03,633 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:03,637 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:03,651 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:03,651 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:04,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,487 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:04,487 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:25:04,522 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:04,526 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:25:04,531 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:04,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:04,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,872 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,915 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:04,981 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:04,981 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:25:05,007 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:25:05,011 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:25:05,015 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:05,015 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:05,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,368 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:05,368 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:25:05,424 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:25:05,428 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:25:05,439 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:05,439 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:05,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,829 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:05,866 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:05,866 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:25:05,913 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:25:05,916 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:25:05,929 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:05,929 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:06,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:06,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:06,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:06,402 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:06,402 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:06,403 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:25:06,460 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:25:06,463 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:25:06,478 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:06,478 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:06,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,061 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,098 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:07,098 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:25:07,123 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:25:07,127 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:25:07,127 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:07,128 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:07,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,737 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 28, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 29, 128])"))
2023-10-16 07:25:07,813 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"))
2023-10-16 07:25:07,813 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:25:07,841 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:25:07,841 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:25:07,842 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:07,842 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:07,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:07,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:07,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:07,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:07,845 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:07,845 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:25:07,846 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:25:07,846 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:25:07,846 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:07,846 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:08,392 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:08,421 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:08,451 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:08,480 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:08,483 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:25:08,484 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:25:08,542 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:25:08,542 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:25:08,543 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:25:08,543 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:08,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,546 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:08,546 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:25:08,546 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:25:08,547 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:25:08,560 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 30])", "<class 'int'>: 29")
2023-10-16 07:25:08,560 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:08,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:08,564 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:08,564 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:25:08,567 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:25:08,570 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:25:08,587 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:08,587 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:09,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,076 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,115 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,115 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:09,115 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:25:09,143 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:25:09,146 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:25:09,160 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:09,160 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:09,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,691 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:09,691 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:25:09,754 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:25:09,758 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:25:09,771 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:09,771 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:09,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,859 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,895 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:09,930 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:09,930 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:25:09,977 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:25:09,981 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:25:09,994 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:09,994 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:10,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,483 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,484 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:10,484 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:25:10,512 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:25:10,515 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:25:10,529 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:10,529 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:10,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,796 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:10,797 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:10,797 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:25:10,821 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:25:10,824 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:25:10,838 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:10,838 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:11,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,448 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:11,448 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:25:11,472 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:25:11,475 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:25:11,487 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:11,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:11,813 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,878 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:11,986 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:11,986 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:25:12,009 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:25:12,012 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:25:12,022 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:12,022 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:12,105 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,229 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:12,229 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:25:12,266 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:25:12,269 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:25:12,282 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:12,283 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:12,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,560 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:12,560 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:25:12,589 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:25:12,593 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:25:12,597 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:12,597 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:12,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,733 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,770 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:12,770 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:12,770 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:25:12,797 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:25:12,800 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:25:12,804 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:12,804 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:13,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,277 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:13,277 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:25:13,301 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:25:13,304 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:25:13,318 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:13,318 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:13,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:13,834 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:13,834 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:25:13,860 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:25:13,864 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:25:13,877 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:13,877 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:13,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,074 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,111 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:14,111 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:25:14,149 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:25:14,153 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:25:14,161 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:14,162 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:14,695 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,814 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:14,814 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:14,814 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:25:14,840 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:25:14,843 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:25:14,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:14,848 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:15,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:15,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:15,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:15,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:15,542 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:15,542 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:25:15,571 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:25:15,574 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:25:15,578 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:15,578 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:16,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:16,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:16,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:16,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:16,333 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:16,333 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:25:16,367 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:25:16,370 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:25:16,374 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:16,375 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:17,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:17,149 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:17,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:17,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:17,228 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:17,228 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:25:17,251 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:25:17,255 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:25:17,263 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:17,263 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:17,937 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:17,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:18,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:18,048 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:18,048 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:18,048 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:25:18,080 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:25:18,085 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:25:18,091 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:18,091 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:18,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:18,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:18,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:18,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:18,818 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:18,818 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:25:18,844 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:25:18,848 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:25:18,858 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:18,859 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:19,392 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:19,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:19,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:19,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:19,502 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:19,503 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:25:19,540 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:25:19,543 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:19,546 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:19,547 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:20,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:20,174 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:20,208 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:20,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:20,244 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:20,244 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:25:20,275 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:20,279 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:20,293 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:20,293 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:21,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,129 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:21,129 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:25:21,157 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:21,161 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:21,171 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:21,172 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:21,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:21,908 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:21,908 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:25:21,937 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:21,941 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:21,955 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:21,955 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:22,357 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,398 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,480 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:22,481 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:25:22,518 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:22,524 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:22,538 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:22,538 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:22,693 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:22,818 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:22,818 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:25:22,865 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:22,868 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:22,881 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:22,882 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:23,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,142 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:23,142 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:25:23,183 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:23,187 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:25:23,198 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:23,199 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:23,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,377 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:23,377 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:25:23,417 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:25:23,421 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:25:23,435 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:23,435 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:23,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,577 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,621 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,621 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:23,622 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:25:23,647 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:25:23,651 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:25:23,662 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:23,663 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:23,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:23,830 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:23,830 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:25:23,854 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:25:23,858 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:25:23,871 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:23,871 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:24,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,113 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,194 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:24,194 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:25:24,221 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:25:24,225 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:25:24,236 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:24,236 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:24,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:24,863 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:24,863 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:25:24,886 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:25:24,889 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:25:24,890 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:24,890 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:25,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:25,358 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:25,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:25,431 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 29, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 30, 128])"))
2023-10-16 07:25:25,432 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"))
2023-10-16 07:25:25,432 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:25:25,457 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:25:25,458 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:25:25,458 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:25,459 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:25,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:25,460 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:25,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:25,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:25,462 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:25,462 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:25:25,463 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:25:25,463 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:25:25,463 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:25,464 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:25,935 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:25,964 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:25,990 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:26,020 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:26,022 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:25:26,023 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:25:26,061 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:25:26,062 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:25:26,062 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:25:26,062 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:26,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,065 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:26,065 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:25:26,066 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:25:26,066 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:25:26,079 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 31])", "<class 'int'>: 30")
2023-10-16 07:25:26,079 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:26,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,083 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:26,083 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:26,083 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:25:26,086 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:25:26,090 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:25:26,098 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:26,099 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:26,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:26,455 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:26,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:26,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:26,536 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:26,536 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:25:26,569 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:25:26,572 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:25:26,580 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:26,581 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:27,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,175 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,176 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:27,176 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:25:27,209 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:25:27,213 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:25:27,219 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:27,219 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:27,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,489 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:27,577 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:27,577 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:25:27,603 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:25:27,607 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:25:27,621 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:27,621 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:28,254 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:28,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:28,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:28,385 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:28,385 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:28,385 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:25:28,409 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:25:28,412 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:25:28,425 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:28,426 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:28,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:28,982 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:29,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:29,068 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:29,068 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:29,068 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:25:29,093 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:25:29,097 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:25:29,111 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:29,111 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:29,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:29,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:29,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:29,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:29,718 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:29,718 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:25:29,742 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:25:29,745 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:25:29,759 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:29,759 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:30,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,174 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,261 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:30,261 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:25:30,294 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:25:30,298 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:25:30,311 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:30,311 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:30,651 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,737 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,777 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:30,778 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:30,778 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:25:30,811 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:25:30,815 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:25:30,829 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:30,829 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:31,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:31,460 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:31,496 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:31,534 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:31,534 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:31,534 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:25:31,560 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:25:31,563 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:25:31,576 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:31,576 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:32,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,229 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:32,229 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:25:32,252 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:25:32,255 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:25:32,268 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:32,268 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:32,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:32,920 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:32,920 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:25:32,942 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:25:32,946 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:25:32,952 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:32,952 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:33,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:33,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:33,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:33,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:33,582 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:33,583 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:25:33,611 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:25:33,616 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:25:33,636 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:33,637 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:34,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,273 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,273 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:34,273 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:25:34,295 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:25:34,299 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:25:34,309 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:34,309 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:34,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,805 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:34,881 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:34,881 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:25:34,911 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:25:34,915 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:25:34,929 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:34,930 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:35,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:35,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:35,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:35,531 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:35,531 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:35,532 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:25:35,562 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:25:35,565 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:25:35,579 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:35,579 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:36,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,106 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,146 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:36,146 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:25:36,173 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:25:36,176 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:25:36,186 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:36,187 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:36,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,796 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:36,796 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:36,797 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:25:36,821 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:25:36,825 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:25:36,839 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:36,840 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:37,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,435 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,436 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:37,436 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:25:37,462 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:25:37,466 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:25:37,477 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:37,477 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:37,786 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:37,905 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:37,905 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:25:37,938 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:25:37,942 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:25:37,956 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:37,956 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:38,007 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,079 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,116 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,116 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:38,117 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:25:38,158 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:25:38,161 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:38,172 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:38,172 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:38,532 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:38,642 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:38,642 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:25:38,670 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:38,674 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:38,687 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:38,687 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:39,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,347 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:39,347 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:25:39,375 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:39,378 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:39,388 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:39,388 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:39,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,904 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:39,942 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:39,942 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:25:39,967 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:39,970 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:39,984 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:39,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:40,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,554 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,592 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:40,592 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:25:40,620 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:40,623 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:40,634 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:40,634 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:40,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,948 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:40,948 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:40,948 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:25:40,973 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:40,976 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:40,990 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:40,990 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:41,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,406 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:41,406 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:25:41,437 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:41,441 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:25:41,445 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:41,445 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:41,670 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:41,799 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:41,800 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:25:41,823 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:25:41,826 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:25:41,832 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:41,832 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:42,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:42,636 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:42,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:42,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:42,719 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:42,720 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:25:42,748 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:25:42,751 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:25:42,762 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:42,762 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:43,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:43,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:43,531 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:43,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:43,569 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:43,569 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:25:43,602 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:25:43,608 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:25:43,626 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:43,627 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:44,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,222 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,298 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:44,298 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:25:44,337 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:25:44,341 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:25:44,352 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:44,352 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:44,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,483 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,560 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:44,560 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:25:44,614 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:25:44,617 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:25:44,618 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:44,618 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:44,874 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,982 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 30, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 31, 128])"))
2023-10-16 07:25:44,982 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"))
2023-10-16 07:25:44,982 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:25:45,027 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:25:45,027 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:25:45,028 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:45,028 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:45,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,032 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:45,032 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:25:45,033 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:25:45,033 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:25:45,034 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:45,034 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:45,306 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:45,335 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:45,364 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:45,391 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:25:45,393 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:25:45,393 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:25:45,459 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:25:45,460 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:25:45,460 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:25:45,460 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:45,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,463 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,463 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:45,463 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:25:45,464 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:25:45,464 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:25:45,479 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 32])", "<class 'int'>: 31")
2023-10-16 07:25:45,479 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:25:45,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:25:45,482 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:25:45,482 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:25:45,485 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:25:45,489 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:25:45,503 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:45,503 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:45,625 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:45,662 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:45,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:45,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:45,735 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:45,735 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:25:45,773 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:25:45,776 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:25:45,791 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:45,791 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:46,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,089 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,126 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:46,126 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:25:46,153 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:25:46,157 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:25:46,171 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:46,171 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:46,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:46,868 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:46,869 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:25:46,892 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:25:46,895 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:25:46,908 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:46,908 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:47,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:47,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:47,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:47,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:47,489 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:47,489 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:25:47,515 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:25:47,518 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:25:47,531 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:47,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:48,056 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,159 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:48,159 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:25:48,185 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:25:48,188 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:25:48,202 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:48,202 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:48,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:48,826 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:48,826 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:25:48,851 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:25:48,855 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:25:48,869 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:48,870 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:49,137 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,259 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:49,259 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:25:49,286 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:25:49,290 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:25:49,302 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:49,302 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:49,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:49,969 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:49,969 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:25:49,994 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:25:49,998 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:25:50,010 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:50,010 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:50,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:50,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:50,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:50,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:50,622 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:50,623 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:25:50,647 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:25:50,651 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:25:50,661 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:50,661 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:51,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:51,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:51,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:51,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:51,360 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:51,361 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:25:51,385 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:25:51,389 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:25:51,397 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:51,397 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:52,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:52,187 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:52,232 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:52,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:52,274 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:52,274 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:25:52,306 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:25:52,309 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:25:52,322 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:52,322 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:53,068 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,141 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,179 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,179 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:53,179 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:25:53,205 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:25:53,209 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:25:53,222 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:53,223 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:53,650 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:53,776 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:53,776 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:25:53,806 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:25:53,809 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:25:53,823 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:53,823 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:53,966 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,079 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,080 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:54,080 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:25:54,124 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:25:54,127 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:25:54,139 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:54,139 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:54,310 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,450 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,451 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:54,451 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:25:54,495 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:25:54,499 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:25:54,513 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:54,513 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:54,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,925 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:54,925 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:54,926 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:25:54,975 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:25:54,979 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:25:54,989 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:54,990 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:55,041 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,078 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,156 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:55,156 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:25:55,205 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:25:55,209 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:25:55,222 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:55,222 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:55,345 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,460 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:55,460 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:25:55,510 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:25:55,513 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:25:55,525 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:55,525 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:55,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:55,722 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:55,722 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:25:55,759 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:25:55,763 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:25:55,776 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:55,776 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:56,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,129 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,169 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,170 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:56,170 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:25:56,196 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:25:56,200 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:56,211 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:56,211 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:56,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,520 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,521 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:56,521 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:25:56,552 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:25:56,555 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:56,569 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:56,569 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:56,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:56,794 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:56,794 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:25:56,820 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:25:56,824 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:56,837 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:56,838 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:56,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,028 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,101 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:57,101 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:25:57,131 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:25:57,135 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:57,149 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:57,149 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:57,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:57,891 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:57,891 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:25:57,919 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:25:57,922 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:57,933 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:57,934 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:58,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:58,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:58,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:58,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:58,579 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:58,579 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:25:58,604 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:25:58,608 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:58,623 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:58,623 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:59,165 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,277 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:59,277 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:25:59,304 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:25:59,308 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:25:59,319 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:25:59,319 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:25:59,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,906 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:25:59,975 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:25:59,976 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:25:59,999 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:26:00,003 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:00,015 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:00,016 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:00,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:00,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:00,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:00,968 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:00,968 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:26:00,968 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:26:00,998 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:01,002 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:01,016 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:01,016 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:01,570 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:01,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:01,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:01,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:01,697 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:26:01,697 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:26:01,728 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:01,731 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:01,737 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:01,737 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:02,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,253 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,339 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:26:02,340 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:26:02,372 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:02,375 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:26:02,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:02,379 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:02,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:02,823 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:26:02,823 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:26:02,847 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:26:02,851 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:26:02,851 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:02,852 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:03,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:03,320 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:03,357 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:03,394 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 31, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 32, 128])"))
2023-10-16 07:26:03,395 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"))
2023-10-16 07:26:03,395 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:26:03,423 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:26:03,423 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:26:03,424 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:03,424 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:03,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,429 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:03,429 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:26:03,429 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:26:03,430 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:26:03,430 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:03,430 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:03,641 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:03,667 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:03,694 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:03,722 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:03,725 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:26:03,726 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:26:03,770 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:26:03,771 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:26:03,771 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:26:03,771 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:03,772 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,774 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:03,774 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:26:03,775 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:26:03,775 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:26:03,778 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 33])", "<class 'int'>: 32")
2023-10-16 07:26:03,779 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:03,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:03,783 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:03,783 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:26:03,786 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:26:03,789 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:26:03,801 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:03,801 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:04,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:04,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:04,411 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:04,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:04,452 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:04,452 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:26:04,476 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:26:04,480 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:26:04,493 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:04,493 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:04,971 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,015 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,092 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:05,092 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:26:05,118 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:26:05,122 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:26:05,136 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:05,136 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:05,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,625 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,662 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:05,706 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:05,706 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:26:05,731 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:26:05,735 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:26:05,738 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:05,738 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:06,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,272 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:06,272 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:26:06,303 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:26:06,307 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:26:06,310 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:06,310 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:06,737 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:06,862 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:06,862 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:26:06,905 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:26:06,909 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:26:06,918 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:06,918 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:07,248 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,340 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:07,340 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:26:07,383 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:26:07,387 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:26:07,399 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:07,399 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:07,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:07,981 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:07,981 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:26:08,013 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:26:08,017 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:26:08,030 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:08,030 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:08,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:08,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:08,762 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:08,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:08,800 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:08,801 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:26:08,830 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:26:08,834 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:26:08,847 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:08,847 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:09,475 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:09,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:09,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:09,606 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:09,606 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:09,606 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:26:09,633 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:26:09,637 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:26:09,650 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:09,650 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:10,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,200 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,276 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:10,276 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:26:10,301 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:26:10,304 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:26:10,317 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:10,318 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:10,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:10,917 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:10,918 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:26:10,943 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:26:10,947 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:26:10,953 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:10,954 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:11,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:11,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:11,359 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:11,402 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:11,402 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:11,402 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:26:11,440 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:26:11,444 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:26:11,457 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:11,457 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:12,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,120 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:12,120 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:26:12,153 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:26:12,156 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:26:12,167 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:12,167 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:12,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,324 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,324 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:12,325 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:26:12,353 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:26:12,356 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:26:12,366 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:12,367 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:12,486 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,599 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:12,599 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:26:12,628 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:26:12,632 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:26:12,645 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:12,645 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:12,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:12,958 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,001 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:13,001 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:26:13,031 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:26:13,034 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:26:13,046 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:13,046 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:13,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,169 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,207 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:13,207 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:26:13,232 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:26:13,236 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:26:13,250 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:13,251 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:13,371 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,484 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:13,484 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:26:13,513 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:26:13,517 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:26:13,530 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:13,530 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:13,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:13,892 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:13,892 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:26:13,923 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:26:13,927 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:26:13,940 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:13,940 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:14,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:14,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:14,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:14,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:14,591 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:14,591 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:26:14,622 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:26:14,625 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:26:14,636 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:14,636 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:15,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,141 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,182 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:15,183 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:26:15,208 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:26:15,212 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:26:15,225 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:15,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:15,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:15,917 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:15,917 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:26:15,945 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:26:15,948 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:26:15,959 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:15,960 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:16,494 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,532 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,618 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:16,618 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:26:16,642 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:26:16,645 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:26:16,659 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:16,659 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:16,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:16,990 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:16,990 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:26:17,016 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:26:17,021 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:26:17,034 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:17,034 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:17,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:17,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:17,606 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:17,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:17,648 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:17,648 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:26:17,676 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:26:17,680 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:26:17,694 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:17,694 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:18,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:18,368 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:18,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:18,449 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:18,450 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:18,450 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:26:18,479 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:26:18,483 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:26:18,495 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:18,496 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:19,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:19,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:19,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:19,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:19,356 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:19,356 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:26:19,382 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:26:19,386 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:19,400 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:19,401 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:19,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,123 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:20,123 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:26:20,151 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:20,155 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:20,159 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:20,159 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:20,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,672 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:20,714 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:20,714 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:26:20,745 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:20,749 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:20,760 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:20,760 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:21,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:21,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:21,494 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:21,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:21,541 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:21,542 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:26:21,569 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:21,572 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:26:21,584 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:21,584 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:22,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:22,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:22,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:22,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:22,355 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:22,355 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:26:22,386 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:26:22,390 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:26:22,390 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:22,390 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:22,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:22,961 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:22,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:23,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 32, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 33, 128])"))
2023-10-16 07:26:23,039 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"))
2023-10-16 07:26:23,039 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:26:23,067 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:26:23,068 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:26:23,068 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:23,069 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:23,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,072 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:23,072 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:26:23,073 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:26:23,073 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:26:23,074 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:23,074 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:23,735 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:23,763 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:23,791 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:23,818 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:23,820 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:26:23,820 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:26:23,859 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:26:23,860 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:26:23,860 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:26:23,860 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:23,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,863 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,864 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:23,864 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:26:23,864 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:26:23,865 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:26:23,877 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 34])", "<class 'int'>: 33")
2023-10-16 07:26:23,878 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:23,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:23,882 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:23,882 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:26:23,884 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:26:23,888 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:26:23,893 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:23,893 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:24,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:24,417 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:24,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:24,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:24,503 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:24,504 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:26:24,528 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:26:24,532 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:26:24,546 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:24,546 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:25,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,160 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:25,160 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:26:25,197 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:26:25,201 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:26:25,215 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:25,215 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:25,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,392 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,393 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:25,393 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:26:25,438 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:26:25,442 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:26:25,455 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:25,455 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:25,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,620 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:25,620 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:26:25,660 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:26:25,664 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:26:25,678 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:25,678 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:25,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:25,855 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:25,856 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:26:25,892 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:26:25,896 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:26:25,913 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:25,914 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:25,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,110 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:26,110 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:26:26,132 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:26:26,136 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:26:26,150 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:26,151 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:26,490 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,532 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:26,625 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:26,625 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:26:26,647 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:26:26,651 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:26:26,664 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:26,664 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:27,105 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,215 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:27,215 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:26:27,240 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:26:27,244 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:26:27,258 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:27,259 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:27,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:27,914 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:27,914 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:26:27,938 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:26:27,942 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:26:27,955 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:27,955 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:28,489 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:28,528 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:28,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:28,607 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:28,607 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:28,607 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:26:28,631 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:26:28,635 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:26:28,649 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:28,649 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:29,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,266 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,308 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,309 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:29,309 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:26:29,333 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:26:29,337 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:26:29,351 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:29,351 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:29,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:29,803 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:29,803 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:26:29,828 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:26:29,832 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:26:29,846 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:29,846 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:30,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:30,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:30,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:30,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:30,408 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:30,408 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:26:30,433 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:26:30,436 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:26:30,442 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:30,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:30,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:30,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:30,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:31,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:31,036 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:31,036 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:26:31,061 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:26:31,065 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:26:31,069 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:31,069 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:31,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:31,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:32,028 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:32,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:32,071 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:32,071 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:26:32,096 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:26:32,100 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:26:32,103 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:32,104 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:32,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:32,893 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:32,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:32,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:32,964 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:32,964 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:26:32,996 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:26:33,000 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:26:33,004 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:33,004 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:33,828 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:33,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:33,909 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:33,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:33,952 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:33,953 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:26:33,977 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:26:33,980 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:26:33,984 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:33,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:34,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:34,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:34,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:34,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:34,812 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:34,812 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:26:34,841 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:26:34,845 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:26:34,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:34,848 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:35,606 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:35,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:35,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:35,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:35,721 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:35,721 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:26:35,750 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:26:35,754 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:26:35,760 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:35,760 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:36,514 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:36,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:36,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:36,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:36,685 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:36,685 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:26:36,714 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:26:36,718 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:26:36,722 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:36,723 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:37,382 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,458 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,496 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:37,496 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:26:37,528 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:26:37,531 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:26:37,535 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:37,535 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:37,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,909 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,953 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:37,954 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:37,954 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:26:38,019 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:26:38,023 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:26:38,035 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:38,035 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:38,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:38,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:38,514 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:38,553 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:38,554 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:38,554 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:26:38,600 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:26:38,604 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:26:38,619 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:38,619 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:39,358 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,483 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:39,483 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:26:39,513 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:26:39,517 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:26:39,527 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:39,527 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:39,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,909 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:39,946 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:39,946 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:26:39,995 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:26:39,999 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:26:40,012 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:40,012 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:40,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,374 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,458 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,458 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:40,458 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:26:40,512 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:26:40,516 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:26:40,528 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:40,528 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:40,581 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,700 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:40,700 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:26:40,749 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:26:40,753 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:40,766 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:40,766 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:40,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:40,924 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:40,924 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:26:40,980 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:40,984 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:40,995 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:40,995 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:41,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,391 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:41,391 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:26:41,433 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:41,437 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:41,451 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:41,451 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:41,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:41,930 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:41,930 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:26:41,971 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:41,975 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:26:41,985 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:41,985 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:42,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:42,514 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:42,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:42,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:42,580 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:42,580 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:26:42,604 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:26:42,608 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:26:42,609 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:42,609 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:43,088 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:43,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:43,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:43,192 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 33, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 34, 128])"))
2023-10-16 07:26:43,192 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"))
2023-10-16 07:26:43,192 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:26:43,227 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:26:43,227 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:26:43,228 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:43,228 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:43,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:43,230 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:43,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:43,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:43,232 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:43,232 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:26:43,232 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:26:43,233 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:26:43,233 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:43,233 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:43,992 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:44,021 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:44,048 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:44,075 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:26:44,077 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:26:44,077 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:26:44,116 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:26:44,117 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:26:44,117 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:26:44,117 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:44,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,120 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:44,120 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:26:44,121 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:26:44,121 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:26:44,135 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 35])", "<class 'int'>: 34")
2023-10-16 07:26:44,135 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:26:44,137 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,138 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,138 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,139 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:26:44,139 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:26:44,140 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:26:44,143 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:26:44,146 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:26:44,159 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:44,160 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:44,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:44,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:44,796 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:44,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:44,834 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:44,834 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:26:44,858 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:26:44,862 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:26:44,875 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:44,876 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:45,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,266 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,303 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:45,303 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:26:45,327 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:26:45,331 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:26:45,343 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:45,343 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:45,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,607 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:45,725 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:45,725 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:26:45,749 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:26:45,752 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:26:45,766 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:45,767 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:46,075 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,196 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,197 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:46,197 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:26:46,221 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:26:46,225 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:26:46,228 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:46,229 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:46,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:46,795 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:46,795 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:26:46,830 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:26:46,835 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:26:46,839 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:46,839 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:47,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:47,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:47,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:47,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:47,749 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:47,749 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:26:47,773 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:26:47,777 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:26:47,781 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:47,781 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:48,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:48,419 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:48,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:48,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:48,492 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:48,492 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:26:48,527 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:26:48,531 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:26:48,537 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:48,537 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:48,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,101 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:49,101 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:26:49,144 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:26:49,148 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:26:49,161 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:49,161 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:49,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,588 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,630 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:49,631 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:26:49,675 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:26:49,678 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:26:49,691 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:49,691 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:49,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:49,877 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:49,877 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:26:49,925 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:26:49,929 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:26:49,943 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:49,943 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:50,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,084 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,160 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:50,160 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:26:50,209 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:26:50,213 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:26:50,222 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:50,223 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:50,607 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:50,740 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:50,740 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:26:50,781 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:26:50,784 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:26:50,788 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:50,788 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:51,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:51,278 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:51,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:51,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:51,354 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:51,354 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:26:51,390 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:26:51,394 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:26:51,408 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:51,408 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:52,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,308 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:52,308 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:26:52,332 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:26:52,335 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:26:52,345 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:52,346 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:52,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,932 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:52,970 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:52,970 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:26:52,996 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:26:53,001 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:26:53,015 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:53,015 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:53,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:53,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:53,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:53,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:53,695 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:53,695 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:26:53,723 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:26:53,727 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:26:53,738 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:53,738 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:53,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:53,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:54,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:54,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:54,072 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:54,072 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:26:54,114 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:26:54,118 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:26:54,131 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:54,131 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:54,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:54,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:54,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:54,602 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:54,602 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:54,602 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:26:54,642 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:26:54,647 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:26:54,657 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:54,657 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:55,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:55,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:55,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:55,310 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:55,311 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:55,311 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:26:55,335 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:26:55,338 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:26:55,351 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:55,351 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:55,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:55,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,078 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,079 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:56,079 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:26:56,110 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:26:56,115 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:26:56,128 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:56,129 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:56,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,626 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,667 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:56,667 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:26:56,690 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:26:56,693 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:26:56,715 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:56,715 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:56,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:56,942 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:56,942 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:26:56,969 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:26:56,972 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:26:56,983 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:56,983 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:57,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,252 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:57,252 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:26:57,278 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:26:57,282 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:26:57,295 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:57,295 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:57,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:57,914 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:57,915 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:26:57,942 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:26:57,946 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:26:57,958 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:57,958 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:57,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,068 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,104 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:58,104 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:26:58,128 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:26:58,132 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:26:58,136 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:58,136 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:58,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,353 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:58,353 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:26:58,380 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:26:58,383 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:26:58,395 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:58,395 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:58,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,819 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:58,819 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:26:58,844 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:26:58,848 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:58,862 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:58,862 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:58,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:58,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,060 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:59,060 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:26:59,087 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:26:59,090 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:59,096 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:59,096 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:59,141 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,179 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,219 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,254 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,254 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:59,255 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:26:59,278 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:26:59,281 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:59,287 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:59,287 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:26:59,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:26:59,783 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:26:59,784 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:26:59,816 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:26:59,820 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:26:59,827 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:26:59,827 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:00,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:00,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:00,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:00,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:00,587 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:27:00,587 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:27:00,630 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:27:00,634 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:00,634 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:00,635 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:01,291 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:01,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:01,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:01,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 34, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 35, 128])"))
2023-10-16 07:27:01,412 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"))
2023-10-16 07:27:01,412 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:27:01,441 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:01,442 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:01,443 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:01,443 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:01,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:01,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:01,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:01,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:01,446 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:01,446 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:27:01,446 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:01,447 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:27:01,447 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:01,447 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:02,042 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:02,071 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:02,098 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:02,125 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:02,129 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:27:02,129 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:27:02,177 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:27:02,178 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:27:02,179 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:27:02,179 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:02,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,182 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:02,182 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:27:02,182 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:27:02,183 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:27:02,194 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 36])", "<class 'int'>: 35")
2023-10-16 07:27:02,194 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:02,196 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,196 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,197 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,198 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:02,198 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:02,199 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:27:02,202 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:27:02,207 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:27:02,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:02,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:02,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:02,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:02,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:02,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:02,920 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:02,920 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:27:02,944 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:27:02,947 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:27:02,951 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:02,951 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:03,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:03,757 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:03,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:03,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:03,836 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:03,836 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:27:03,860 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:27:03,864 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:27:03,870 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:03,870 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:04,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:04,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:04,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:04,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:04,806 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:04,807 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:27:04,834 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:27:04,838 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:27:04,851 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:04,851 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:05,577 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:05,625 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:05,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:05,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:05,707 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:05,707 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:27:05,732 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:27:05,736 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:27:05,739 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:05,740 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:06,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:06,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:06,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:06,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:06,387 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:06,388 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:27:06,412 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:27:06,416 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:27:06,426 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:06,427 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:07,089 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,200 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,200 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:07,201 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:27:07,234 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:27:07,238 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:27:07,252 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:07,252 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:07,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,562 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:07,562 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:27:07,610 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:27:07,615 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:27:07,631 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:07,631 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:07,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,762 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,805 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:07,845 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:07,846 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:27:07,874 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:27:07,878 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:27:07,884 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:07,885 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:08,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,131 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:08,131 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:27:08,156 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:27:08,160 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:27:08,173 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:08,173 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:08,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,529 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,529 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:08,529 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:27:08,554 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:27:08,557 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:27:08,571 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:08,571 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:08,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,737 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,737 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:08,737 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:27:08,763 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:27:08,766 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:27:08,779 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:08,779 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:08,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:08,952 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:08,952 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:27:08,982 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:27:08,985 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:27:08,998 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:08,999 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:09,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,085 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,164 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:09,164 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:27:09,195 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:27:09,199 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:27:09,208 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:09,209 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:09,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,365 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:09,365 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:27:09,396 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:27:09,399 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:27:09,413 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:09,413 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:09,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,737 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:09,806 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:09,807 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:27:09,829 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:27:09,832 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:27:09,846 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:09,846 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:10,225 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,324 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,324 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:10,325 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:27:10,351 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:27:10,355 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:27:10,365 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:10,366 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:10,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,504 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,504 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:10,504 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:27:10,529 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:27:10,532 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:27:10,546 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:10,546 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:10,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,731 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:10,771 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:10,771 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:27:10,797 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:27:10,800 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:27:10,811 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:10,811 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:11,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,238 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:11,239 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:27:11,263 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:27:11,266 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:27:11,279 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:11,280 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:11,837 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:11,959 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:11,960 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:27:11,988 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:27:11,992 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:27:12,003 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:12,004 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:12,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:12,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:12,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:12,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:12,838 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:12,839 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:27:12,864 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:27:12,867 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:27:12,880 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:12,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:13,632 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:13,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:13,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:13,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:13,728 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:13,728 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:27:13,751 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:27:13,755 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:27:13,765 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:13,765 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:14,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:14,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:14,606 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:14,650 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:14,650 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:14,650 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:27:14,675 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:27:14,678 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:27:14,693 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:14,693 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:15,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,143 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:15,143 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:27:15,199 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:27:15,204 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:27:15,222 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:15,223 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:15,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,764 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,851 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:15,852 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:15,852 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:27:15,876 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:27:15,880 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:27:15,895 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:15,895 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:16,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:16,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:16,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:16,546 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:16,546 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:16,546 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:27:16,577 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:27:16,581 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:27:16,592 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:16,592 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:17,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,135 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,173 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,218 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:17,218 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:27:17,243 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:27:17,247 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:27:17,260 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:17,261 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:17,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,962 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:17,962 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:17,962 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:27:17,990 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:27:17,993 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:27:18,005 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:18,005 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:18,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:18,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:18,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:18,758 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:18,758 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:18,758 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:27:18,782 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:27:18,785 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:27:18,799 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:18,799 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:19,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,433 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,512 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:19,512 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:27:19,544 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:27:19,547 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:27:19,559 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:19,559 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:19,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,835 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:19,835 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:19,835 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:27:19,874 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:27:19,878 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:19,879 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:19,879 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:20,209 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:20,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:20,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:20,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 35, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 36, 128])"))
2023-10-16 07:27:20,321 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"))
2023-10-16 07:27:20,321 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:27:20,349 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:20,350 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:20,350 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:20,350 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:20,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,355 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,355 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:20,355 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:27:20,356 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:20,356 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:27:20,356 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:20,357 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:20,535 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:20,567 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:20,599 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:20,630 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:20,632 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:27:20,633 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:27:20,678 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:27:20,678 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:27:20,679 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:27:20,679 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:20,680 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,680 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,682 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:20,682 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:27:20,682 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:27:20,683 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:27:20,696 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 37])", "<class 'int'>: 36")
2023-10-16 07:27:20,696 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:20,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,699 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,699 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:20,699 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:20,699 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:27:20,703 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:27:20,706 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:27:20,721 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:20,721 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:20,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:20,874 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:20,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:20,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:20,958 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:20,958 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:27:20,981 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:27:20,985 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:27:21,001 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:21,001 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:21,508 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:21,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:21,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:21,686 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:21,686 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:21,686 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:27:21,710 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:27:21,714 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:27:21,728 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:21,728 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:22,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:22,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:22,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:22,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:22,351 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:22,351 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:27:22,377 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:27:22,381 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:27:22,395 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:22,395 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:22,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:22,956 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:22,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:23,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:23,040 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:23,040 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:27:23,065 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:27:23,068 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:27:23,082 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:23,082 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:23,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:23,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:23,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:23,711 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:23,711 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:23,712 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:27:23,742 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:27:23,746 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:27:23,759 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:23,760 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:24,193 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,306 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,307 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:24,307 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:27:24,332 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:27:24,336 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:27:24,350 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:24,350 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:24,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:24,744 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:24,744 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:27:24,769 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:27:24,773 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:27:24,787 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:24,787 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:25,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:25,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:25,336 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:25,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:25,381 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:25,381 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:27:25,406 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:27:25,410 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:27:25,423 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:25,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:25,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:25,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:26,008 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:26,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:26,051 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:26,051 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:27:26,075 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:27:26,078 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:27:26,099 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:26,099 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:26,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:26,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:26,702 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:26,745 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:26,745 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:26,745 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:27:26,768 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:27:26,772 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:27:26,785 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:26,785 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:27,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,398 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,399 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:27,399 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:27:27,423 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:27:27,427 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:27:27,436 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:27,436 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:27,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:27,928 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:27,928 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:27:27,952 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:27:27,955 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:27:27,959 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:27,959 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:28,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:28,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:28,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:28,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:28,698 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:28,699 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:27:28,724 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:27:28,727 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:27:28,738 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:28,738 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:29,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:29,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:29,550 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:29,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:29,586 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:29,586 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:27:29,611 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:27:29,615 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:27:29,625 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:29,625 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:30,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,283 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:30,284 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:27:30,313 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:27:30,316 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:27:30,330 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:30,330 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:30,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,594 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:30,679 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:30,679 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:27:30,729 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:27:30,733 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:27:30,744 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:30,744 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:31,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:31,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:31,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:31,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:31,379 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:31,379 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:27:31,404 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:27:31,408 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:27:31,423 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:31,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:32,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,157 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:32,158 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:27:32,186 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:27:32,189 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:27:32,205 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:32,205 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:32,386 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,499 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:32,499 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:27:32,544 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:27:32,548 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:27:32,562 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:32,562 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:32,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,704 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,745 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:32,746 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:32,746 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:27:32,799 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:27:32,802 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:27:32,814 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:32,814 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:33,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:33,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:33,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:33,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:33,383 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:33,383 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:27:33,408 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:27:33,412 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:27:33,426 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:33,426 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:33,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,084 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,084 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:34,084 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:27:34,114 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:27:34,118 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:27:34,129 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:34,129 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:34,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:34,751 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:34,751 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:27:34,776 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:27:34,780 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:27:34,793 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:34,793 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:34,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,016 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,055 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,098 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,098 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:35,099 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:27:35,126 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:27:35,130 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:27:35,142 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:35,143 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:35,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:35,756 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:35,756 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:27:35,780 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:27:35,783 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:27:35,796 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:35,796 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:36,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:36,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:36,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:36,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:36,439 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:36,439 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:27:36,468 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:27:36,471 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:27:36,476 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:36,476 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:37,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:37,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:37,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:37,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:37,164 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:37,164 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:27:37,188 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:27:37,192 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:27:37,196 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:37,196 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:37,925 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:37,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:38,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:38,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:38,047 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:38,047 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:27:38,076 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:27:38,079 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:27:38,090 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:38,090 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:38,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:38,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:38,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:39,005 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:39,005 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:39,005 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:27:39,030 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:27:39,034 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:27:39,047 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:39,048 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:39,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:39,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:39,651 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:39,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:39,689 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:39,689 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:27:39,729 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:27:39,733 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:27:39,740 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:39,740 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:40,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,099 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,141 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,180 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:40,180 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:27:40,208 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:27:40,213 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:40,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:40,214 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:40,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,915 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 36, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 37, 128])"))
2023-10-16 07:27:40,915 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"))
2023-10-16 07:27:40,915 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:27:40,951 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:40,951 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:40,952 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:40,952 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:40,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:40,956 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:40,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:40,958 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:40,958 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:40,958 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:27:40,958 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:40,959 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:27:40,959 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:40,960 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:41,566 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:41,591 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:41,616 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:41,641 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:41,644 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:27:41,644 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:27:41,692 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:27:41,692 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:27:41,693 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-16 07:27:41,693 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:41,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,695 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,696 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:41,696 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-16 07:27:41,696 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-16 07:27:41,697 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:27:41,701 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 38])", "<class 'int'>: 37")
2023-10-16 07:27:41,701 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:41,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,704 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,704 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:41,705 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:41,706 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-16 07:27:41,709 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-16 07:27:41,713 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:27:41,718 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:41,718 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:42,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:42,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:42,550 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:42,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:42,587 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:42,587 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-16 07:27:42,613 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-16 07:27:42,617 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:27:42,622 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:42,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:42,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:42,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:42,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,001 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:43,001 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-16 07:27:43,049 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-16 07:27:43,052 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:27:43,058 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:43,058 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:43,355 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,465 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:43,465 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-16 07:27:43,551 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-16 07:27:43,557 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:27:43,564 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:43,564 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:43,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,929 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:43,929 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:43,930 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-16 07:27:43,980 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-16 07:27:43,984 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:27:43,988 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:43,988 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:44,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:44,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:44,358 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:44,402 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:44,402 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:44,402 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-16 07:27:44,451 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-16 07:27:44,454 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:27:44,465 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:44,466 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:44,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,102 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:45,102 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-16 07:27:45,141 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-16 07:27:45,145 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:27:45,158 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:45,158 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:45,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,814 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:45,814 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:45,814 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-16 07:27:45,848 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-16 07:27:45,854 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:27:45,870 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:45,870 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:46,398 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:46,432 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:46,469 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:46,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:46,501 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:46,501 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-16 07:27:46,528 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-16 07:27:46,532 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:27:46,545 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:46,545 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:47,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,150 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,187 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,228 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:47,228 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-16 07:27:47,254 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-16 07:27:47,258 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:27:47,272 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:47,273 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:47,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,962 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:47,962 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:47,962 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-16 07:27:47,987 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-16 07:27:47,991 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:27:48,005 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:48,006 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:48,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:48,559 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:48,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:48,637 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:48,637 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:48,638 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-16 07:27:48,662 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-16 07:27:48,665 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:27:48,673 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:48,673 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:49,202 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,310 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,310 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:49,310 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-16 07:27:49,335 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-16 07:27:49,338 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:27:49,352 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:49,352 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:49,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,800 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:49,801 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-16 07:27:49,846 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-16 07:27:49,850 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:27:49,859 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:49,859 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:49,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:49,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,012 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:50,012 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-16 07:27:50,065 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-16 07:27:50,068 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:27:50,080 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:50,080 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:50,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,412 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:50,413 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-16 07:27:50,455 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-16 07:27:50,458 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:27:50,472 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:50,472 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:50,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,687 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:50,767 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:50,767 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-16 07:27:50,794 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-16 07:27:50,797 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:27:50,808 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:50,809 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:51,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,074 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,107 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:51,107 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-16 07:27:51,148 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-16 07:27:51,151 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:27:51,164 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:51,164 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:51,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,672 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,741 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:51,741 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-16 07:27:51,781 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-16 07:27:51,785 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:27:51,798 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:51,798 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:51,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:51,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:52,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:52,074 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:52,075 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:52,075 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-16 07:27:52,100 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-16 07:27:52,104 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:27:52,107 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:52,107 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:52,208 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:52,251 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:52,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:52,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:52,332 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:52,332 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-16 07:27:52,359 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-16 07:27:52,363 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:27:52,369 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:52,369 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:53,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,086 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,165 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,165 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:53,165 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-16 07:27:53,193 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-16 07:27:53,196 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:27:53,204 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:53,204 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:53,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:53,954 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:53,954 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-16 07:27:53,987 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-16 07:27:53,991 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:27:53,996 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:53,996 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:54,534 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:54,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:54,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:54,646 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:54,646 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:54,646 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-16 07:27:54,674 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-16 07:27:54,678 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:27:54,689 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:54,690 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:55,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,488 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:55,488 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-16 07:27:55,530 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-16 07:27:55,533 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:27:55,547 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:55,548 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:55,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,704 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,704 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:55,704 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-16 07:27:55,756 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-16 07:27:55,760 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:27:55,776 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:55,776 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:55,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:55,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:56,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:56,053 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:56,053 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:56,053 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-16 07:27:56,102 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-16 07:27:56,107 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:27:56,117 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:56,118 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:56,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:56,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:56,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:56,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:56,630 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:56,630 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-16 07:27:56,656 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-16 07:27:56,659 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:27:56,671 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:56,671 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:57,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,344 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,382 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:57,382 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-16 07:27:57,411 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-16 07:27:57,415 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:27:57,426 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:57,426 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:57,611 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,654 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,738 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:57,739 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:57,739 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-16 07:27:57,790 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-16 07:27:57,794 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:27:57,802 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:57,803 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:58,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,154 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:58,154 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-16 07:27:58,197 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-16 07:27:58,201 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:27:58,212 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:58,212 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:58,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:58,844 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:58,844 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-16 07:27:58,880 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-16 07:27:58,883 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:58,884 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:58,884 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-16 07:27:59,098 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:59,139 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:59,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:59,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])", "<class 'torch.Tensor'>: torch.Size([1, 32, 37, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 32, 38, 128])"))
2023-10-16 07:27:59,214 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 32, 38, 128])"))
2023-10-16 07:27:59,214 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-16 07:27:59,255 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-16 07:27:59,256 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:59,256 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:59,256 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:59,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:59,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:59,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:59,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 4096])
2023-10-16 07:27:59,260 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])
2023-10-16 07:27:59,260 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-16 07:27:59,261 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-16 07:27:59,261 [model.py:258 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-16 07:27:59,261 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 4096])",)
2023-10-16 07:27:59,262 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-16 07:27:59,807 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:59,836 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:59,863 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:59,888 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 4096])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-16 07:27:59,892 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-16 07:27:59,892 [model.py:268 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-16 07:27:59,980 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-16 07:27:59,980 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-16 07:27:59,980 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I am a human being. I am conscious.
2023-10-16 07:27:59,981 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-16 07:27:59,981 [test.py:40 in test_hf_gen] INFO - Where is Deutschland??
It's the one with the big red cross on it.
2023-10-16 07:27:59,981 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-16 07:27:59,981 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro? to be honest I'm not a fan of Huawei phones but I'm considering getting this one.
I'm not a fan of Huawei either, but
2023-10-16 07:27:59,981 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-16 07:28:00,001 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-16 07:28:00,001 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-16 07:28:00,001 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-16 07:28:00,002 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.12 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.13 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.14 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.15 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.16 from flexgen to old.
2023-10-16 07:28:00,003 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.17 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.18 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.19 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.20 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.21 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.22 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.23 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.24 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.25 from flexgen to old.
2023-10-16 07:28:00,004 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.26 from flexgen to old.
2023-10-16 07:28:00,005 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.27 from flexgen to old.
2023-10-16 07:28:00,005 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.28 from flexgen to old.
2023-10-16 07:28:00,005 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.29 from flexgen to old.
2023-10-16 07:28:00,005 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.30 from flexgen to old.
2023-10-16 07:28:00,005 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.31 from flexgen to old.
2023-10-16 07:28:00,005 [forward.py:23 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-16 07:28:00,005 [forward.py:23 in reset_forward] DEBUG - lm_head from flexgen to old.
